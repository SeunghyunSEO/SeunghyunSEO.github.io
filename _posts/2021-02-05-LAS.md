---
title: Attention Based Seq2Seq for ASR
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---

이 글은 [Listen, Attend and Spell](https://arxiv.org/pdf/1508.01211), [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf) 등 굵직한 Attention Based Seq2Seq Model 논문들과 저의 모자란 지식을 바탕으로 만들어졌습니다.

- <mark style='background-color: #fff5b1'> Sequence Generation(Modeling) Tasks </mark>

우리가 하고싶은것은 입력 x를 받았을때 가장 그럴듯한(likely) 출력값 y를 뽑아내는 것입니다.

그러기 위해서 당연히 likelihood 혹은 log-likelihood $$p(y \mid x)$$를 최대화 하는 방식으로 학습하게 될겁니다.

이런 Sequence-to-Sequence (Seq2Seq) likelihood를 모델링 하는 여러 방법들 중 이번에 다룰것은 Encoder-Decoder Seq2Seq Model 입니다.

- <mark style='background-color: #dcffe4'> Encoder-Decoder Seq2Seq Model </mark>

아래의 그림을 볼까요? (제가 그렸는데 못그려도 이해 부탁드립니다 ㅎㅎ)

![seq2seq](https://user-images.githubusercontent.com/48202736/107010821-35ea2180-67da-11eb-8881-bb9287ea49e7.png)
{: style="width: 80%;" class="center"}
*Fig. 1. Vanilla Encoder-Decoder Seq2Seq Network*

그림이 의미하는 바는 다음과 같습니다.

> 1. 입력값이 Encoder에 들어가고 <br> 
> 2. 어떠한 무슨 정보를 디코더에 넘겨줌 (이는 입력값을 잘 표현한 정보) <br>
> 3. 디코더가 예측한 hypothesis y값을 뱉음 <br>

뭐 당연히 추론한 $$\hat{y}$$랑 진짜 정답 $$y$$를 비교해서 모델 파라메터를 학습하겠죠. 

이제 이 과정을 순차적으로 다시 생각해볼까요?

![seq2seq2](https://user-images.githubusercontent.com/48202736/107010828-37b3e500-67da-11eb-927a-64fd6251d849.png)
{: style="width: 40%;" class="center"}
```
1. 인코더(일단 RNN 이라고 하겠습니다) 에 입력 값을 넣습니다.
```

![seq2seq3](https://user-images.githubusercontent.com/48202736/107010829-384c7b80-67da-11eb-972d-1933a7d0532f.png)
{: style="width: 60%;" class="center"}
```
2. RNN 인코더를 쭉 통과한 어떤 벡터가 나온다, 이를 입력 seqeunce에 대한 문맥 정보가 압축된 표현 벡터(Representation Vector)라고 한다
```

![seq2seq4](https://user-images.githubusercontent.com/48202736/107010831-384c7b80-67da-11eb-8ee8-6fdc9a859c03.png)
{: style="width: 80%;" class="center"}
```
3. 디코더에서 '자 이제 출력 sequence를 뽑아낼거다'라는 사인을 의미하는 <s> 토큰과 representation vector를 받아서 첫번째 토큰을 출력한다.
```

![seq2seq5](https://user-images.githubusercontent.com/48202736/107010836-38e51200-67da-11eb-8665-604fbb91df86.png)
{: style="width: 80%;" class="center"}
```
4. 처음 출려된 값을 다음 디코더 입력값에 넣어주고, representation vector또한 디코더 rnn cell 하나를 통과했으니 정보가 업데이트 된다
```

![seq2seq6](https://user-images.githubusercontent.com/48202736/107010838-397da880-67da-11eb-86b4-9bc97f2909e6.png)
```
5. 이렇게 디코더에서 iterative하게 (Auto-Regressive, AR)하게 출력된 토큰들은 각각 앞토큰에 대한 정보가 conditional하게 반영되면서 뽑힌 토큰들이다.
```

![seq2seq7](https://user-images.githubusercontent.com/48202736/107010840-3a163f00-67da-11eb-9e49-932e09f6bd84.png)
```
6. 이를 다 곱하면 likelihood가 된다.
```

![seq2seq8](https://user-images.githubusercontent.com/48202736/107010842-3a163f00-67da-11eb-825d-7e526ceae0d7.png)
```
7. log-likelihood를 최대화 하는것, 다르게 표현하면 여기서는 각 토큰마다 정답과 일일히 분류 문제를 푸는것이라고 볼 수 있는데, 그렇기 때문에 Cross Entropy Loss를 사용해서 Loss를 구한다
```
```
8. 오차 역전파 (Error Back Propagation)을 통해 파라메터를 업데이트 하여 Loss를 최소화 하는 방향으로 최적화한다.
```


하지만 이게 최선의 방법일까요? 

![seq2seq9](https://user-images.githubusercontent.com/48202736/107010845-3aaed580-67da-11eb-83d9-0718f11cd18c.png)

아닙니다.

이러한 RNN 기반의 Vanilla Encoder Decoder Seq2Seq Model은 몇가지 단점이 있는데,
그 중 하나는 바로 전파되는 Representation Vector의 길이가 입력의 맥락 정보를 다 표현하기에는 부족하다는 것입니다. 
생각해보면 RNN의 Hidden Size가 768 이어도 이 작은 하나의 벡터를 문맥 정보를 전부 다 담은 x를 표현한 벡터라고 하기엔 문제가 있죠.
그리고 RNN이 Hidden Vector에 정보를 업데이트하면서 끝까지 밀어 넘겨주는데, 이 때 입력값의 크기가 너무 크면 정보를 전달하면서 예전 정보를 잃어버리게 되는 문제도 있습니다.


그렇기 때문에 개발된 방법론이 바로 Attention Mechanism을 이용한 Seq2Seq Model입니다.

- <mark style='background-color: #dcffe4'> Attention-based Encoder-Decoder Seq2Seq Model </mark>

이 방법론은 위에서 말한 문제점을 보완하기 위해 제안된 방법입니다.

핵심은 
```
Representation Vector 하나에 모든 정보를 담지 말고, 입력사이즈 만한 Representation Memory를 만들고(사실 RNN cell의 출력값이 두개가있죠, 매 스텝마다 출력된 값들 모아둔겁니다.) 에서 Attention을 이용해 '이번 토큰을 만들어 낼 때는 입력값에서 어느 부분의 정보를 가져오면 될까?(어디를 집중!(Attention)하면 될까?'라는 것을 고려해서 매 번 디코딩 할 때 마다 필요한 정보만 뽑아오자 
```
라는 겁니다.

이것도 순차적으로 살펴볼까요?

![seq2seq11](https://user-images.githubusercontent.com/48202736/107010847-3b476c00-67da-11eb-95ba-b1fcfc9d1b3b.png)
{: style="width: 80%;" class="center"}
```
1. 마찬가지로 인코더는 입력x를 rnn에 통과시켜 Representation Matrix를 뽑아서 킵해둡니다.
```

![seq2seq12](https://user-images.githubusercontent.com/48202736/107010848-3be00280-67da-11eb-9344-eaffb1b2a74d.png)
{: style="width: 80%;" class="center"}
```
2. 하나씩 디코딩을 합니다. 하지만 여기선 더 거대한 정보인 Memory에서 정보를 가져옵니다. 
-> 근데 어떻게요? (답은 Attention 연산을 통해서 어디에 집중할지를 학습을 통해 익히고 가져오는겁니다. 이따가 설명드리도록 하겠습니다.)
```

![seq2seq13](https://user-images.githubusercontent.com/48202736/107010850-3be00280-67da-11eb-9565-b77b6c534da4.png)
{: style="width: 80%;" class="center"}

![seq2seq14](https://user-images.githubusercontent.com/48202736/107010852-3c789900-67da-11eb-8d60-22fcc83e9dd8.png)
{: style="width: 80%;" class="center"}

![seq2seq15](https://user-images.githubusercontent.com/48202736/107010854-3c789900-67da-11eb-8402-c562b4a11a7a.png)
{: style="width: 80%;" class="center"}

![seq2seq16](https://user-images.githubusercontent.com/48202736/107010856-3d112f80-67da-11eb-8682-ff7ace350ed5.png)

![seq2seq17](https://user-images.githubusercontent.com/48202736/107010858-3d112f80-67da-11eb-9263-8abe35a2d6c4.png)

![seq2seq18](https://user-images.githubusercontent.com/48202736/107010860-3da9c600-67da-11eb-836f-93dafe83c135.png)

![seq2seq19](https://user-images.githubusercontent.com/48202736/107010862-3da9c600-67da-11eb-9cca-97b48986eebe.png)
```
3. 최종적으로 likelihood $$p(y|x)$$를 구했습니다. 마찬가지로 토큰 하나 하나에 대해서 분류 문제를 푸는 문제로 바꿀 수 있습니다.
```

![seq2seq20](https://user-images.githubusercontent.com/48202736/107010863-3e425c80-67da-11eb-82fc-afc405194b26.png)
```
4. 각 토큰들에 대해 Cross Entropy Loss를 최소화 하는 방식으로 학습을 하면 됩니다.
```

이러한 Seq2Seq모델에서 중요한건

> 1. Encoder가 입력 x에대한 Representation Memory를 잘 만들어내기만 하면 된다. <br>
> 2. Decoder가 이를 받아서 하나씩 디코딩한다. (Attention연산을 통해 Memory의 일부를 받던지, 아니면 naive하게 벡터 하나로 계속 하던지) <br>

이 두가지입니다.

1번의 'Encoder가 입력 x에대한 Representation Memory를 잘 만들어내기만 하면 된다.'는 정말 어떤 방식으로든 만들어내면 되기 때문에
CNN으로 해도 되고, RNN으로 해도 되고, Transformer 뭘로 정보를 뽑아내도 상관 없습니다. 

하지만 일반적으로 이런 
네트워크가 풀고자 하는 문제가 시간 순서에 따라 정보가 연관되어 있는 시계열 데이터에 대한 문제들이기 때문에 Transformer나 RNN이 더 적합할 수 있습니다.

```
추가로 Encoder에 대해서 음성인식의 경우 Wav2Vec 2.0이라던가, 기계번역의 경우 BERT라던가 하는 사전학습(Pre-trained)된 네트워크를 쓰면 더욱 좋습니다.
걔네가 더 좋은 Representation을 뽑아줄거니까요!
```

- <mark style='background-color: #dcffe4'> Applications </mark>

이렇게 Seq2Seq Model 들은 입력과 출력값이 어떤 pair냐에 따라서 다양한 문제를 풀 수 있습니다. 몇가지 예시를 들어볼까요?

![seq2seq21](https://user-images.githubusercontent.com/48202736/107010865-3e425c80-67da-11eb-88a2-df1d94282b61.png)

위의 예시는 음성 인식(Speech Recognition)의 예시입니다. 입력이 음성의 Spectrogram(2D matrix) 이고 출력이 텍스트면 음성인식이죠.

![seq2seq22](https://user-images.githubusercontent.com/48202736/107010866-3edaf300-67da-11eb-9726-e2b62a1415eb.png)

위의 예시는 음성 합성(Speech Synthesis)의 예시입니다. 입력 출력 바꾸면 되는거죠.

![seq2seq23](https://user-images.githubusercontent.com/48202736/107010867-3f738980-67da-11eb-8531-d2a80479fa73.png)

위의 예시는 기계 번역(Neural Machine Translation)의 예시입니다. 입출력이 얘를들어 '영어-한국어'면 영-한 번역모델이 탄생하는거죠.

우리가 해야할것은 데이터를 부어서 $$p(y \mid x,\theta)$$의 파라메터를 학습하면 되는겁니다.

- <mark style='background-color: #dcffe4'> Auto-Regressive(AR)? Non-Auto-Regressive(NAR)? </mark>

여기서 이번 글의 우리의 목적인 LAS로 들어가기전에 한가지만 더 이야기 해보도록 하겠습니다.

우리는 디코딩을 할 때 매 순간 한 토큰씩만들고, 그 다음 메모리에서 정보를 땡겨와서 또 토큰을 만들고하는 과정을 Auto Regressive (AR) 하게 진행해왔습니다.

과연 이게 optimal일까요?

네 사실 이게 완전 optimal인지는 자세히 모르겠으나, 이렇게 모델링하는게 더 합리적이여 보입니다.
왜냐하면 이전에 우리가 출력한 정보까지 conditional probability로 받아서 출력하는게 훨씬 도움이 되기 때문이죠

이렇게 안하면안되냐구요? 네 가능하긴 합니다. 

![seq2seq24](https://user-images.githubusercontent.com/48202736/107010868-3f738980-67da-11eb-9491-2140c7063bf6.png)
*Fig. Auto-Regressive(AR) Model vs Non-Auto-Regressive(NAR) Model *

위의 그림의 b처럼 하면 되지만, 이는 출력되는 정보들 간의 정보가 전혀 없이 입력값만 가지고 출력을 하는거기 때문에 음성인식이라고 치면 음성인식 퀄리티가 엄청 떨어지게 됩니다.

음성 입력 정보를 받고, 내가 여태 출력한게 '안녕하세요 저'라는 정보까지 받고, 그뒤에 '는'을 예측하는것과
음성 입력 정보만 받고, '안' '녕' ... '는' 같은것들을 다 독립적으로 추론해내기란 엄청 다르기 때문이죠.

이는  수식적으로 보면 다음과 같습니다.

- Auto-Regressive(AR) Model
<center>$$ logp(y \vert x) = \sum_{i=1}^{|y|}logp(y_i \vert y_{<i},x) $$</center>
  
- Non-Auto-Regressive(NAR) Model
<center>$$ logp(y \vert x) = \sum_{i=1}^{|y|}logp(y_i \vert x) $$</center>

NAR이 성능을 잃은 대신 얻는 장점도 있습니다. iterative하게 디코딩 안해도 된다는거죠. conditional prob으로 모델링 되지 않았기 때문에 한번에 값을 전부 출력해버릴 수 있으니까요.

![seq2seq25](https://user-images.githubusercontent.com/48202736/107010869-400c2000-67da-11eb-886c-42a24ef2eabd.png)

이를 극복하기 위해서 latent variable $$z$$ 를 도입한다던가 하는 시도들이 있는데 이는 나중에 다루도록 하고 
글의 나머지 부분에서는 LAS에 대해 얘기해보도록 하겠습니다.

- <mark style='background-color: #fff5b1'> Listen, Attend and Spell (LAS) </mark>

- <mark style='background-color: #dcffe4'> Model Architecture </mark>

자 이제 우리가 LAS에 대해서 얘기해 볼건데 전반적인 얘기는 위에서 하고 넘어왔으므로 바로 모델 아키텍쳐를 보면서 얘기해보도록 하겠습니다.

논문에는 아래의 그림으로 묘사되어있는데요,

<img width="748" alt="las1" src="https://user-images.githubusercontent.com/48202736/107019503-186e8500-67e5-11eb-8438-f38bee1505cf.png">
*Fig. LAS의 Network Architecture 모식도*

그림을 보면 인코더(Listenr)에서 뭔가를 뽑아내고, 그 정보를 통해서 순차적(Auto-Regressive)으로 왼쪽에서 오른쪽 방향으로 'Attend'해가면서 디코딩(Spell) 해나가죠?

이는 제가 위에서 직접 그린 그림과 동일한 의미를 가지고 있습니다. 
(편한대로 보시면 될 거 같습니다.)

![seq2seq19](https://user-images.githubusercontent.com/48202736/107010862-3da9c600-67da-11eb-9cca-97b48986eebe.png)

다만 여기서 Encoder구조에 3층 짜리의 ```Pyramidal Bi-Directional-LSTM``` 을 썼고, 디코더에 2층 짜리 ```Uni-Directional-LSTM```을 썼다는 차이가 있습니다.

왜 논문 제목이 Listen, Attend and Spell (LAS) 일까요?

이는 그저 아까 말한 디코딩 하는 과정을 fancy하게 쓴겁니다.

```
음성을 듣고 (Listen)
음성에서 어느 부분이 이 글자를 예측하는지 고르고 (Attend)
Dictation한다. (쓴다) (Spell)
```

좀더 디테일하게 쓰면

```
음성을 듣고 (Listen) = Encoder, 즉 Listner가 정보를 encoding해서 뽑음
음성에서 어느 부분이 이 글자를 예측하는지 고르고 (Attend) = Attention mechanism으로 계산
Dictation한다. (쓴다) (Spell) = Deocder, 즉 Speller가 하나씩 디코딩
```

이 됩니다.

- <mark style='background-color: #dcffe4'> 수식으로 제대로 보는 LAS </mark>

전반적인 흐름에 대해서 이제는 감이 오셨을 것 같으니, 이번에는 논문의 수식을 따라가면서 LAS를 완벽하게 이해해보도록 하겠습니다.

수식에 쓰일 notation은 다음과 같습니다.

> 1. $$x = (x_1 , . . . , x_T )$$ $$\rightarrow$$ 입력 벡터들(전부 합쳐서 입력 매트릭스 = Mel Spectrogram)이 됩니다. <br>
> 2. $$y = (⟨sos⟩, y_1 , . . . , y_S , ⟨eos⟩)$$ $$\rightarrow$$ 우리가 추론할 정답입니다. <br>
> 3. $$y_i ∈ \{ a, b, c, · · · , z, 0, · · · , 9, ⟨space⟩, ⟨comma⟩, ⟨period⟩, ⟨apostrophe⟩, ⟨unk⟩ \}$$ $$\rightarrow$$ 출력 토큰의 분포 or 단어 사전입니다. <br>
> 4. $$⟨sos⟩$$ $$\rightarrow$$ 단어 사전에 들어있는 특별한 토큰으로 디코딩 시 '자 이제 디코딩 시작할거다, 이걸 받아서 이 음성을 받으면 어떤 값을 처음으로 뱉어야하는가?'를 뱉게 할겁니다. <br>
> 5. $$⟨eos⟩$$ $$\rightarrow$$ 문장의 끝을 알립니다. 이 토큰을 뱉으면 디코딩을 종료하게 할겁니다. <br>

예를 들어 Hello라고 말한 음성에 대해 음성인식을 하는 상황에 대해서, 위의 instance들을 그림에 넣어 표현해봤으니 자세히 보시면 이해하실 수 있을 것 같습니다.

![las_mine1](https://user-images.githubusercontent.com/48202736/107076466-d1a57d00-682e-11eb-81b8-f02ba16c720d.png)



  
<center>$$ P(y|x) = \prod_{i}P(y_i|x,y_{<i}) $$</center>
 
<center>$$ h = Listen(x) $$</center>

<center>$$ P(y|x) = AttendAndSpell(h, y)  $$</center>

<center>$$ h_i^j = BLSTM(h_{i-1}^{j},h_{i}^{j-1}) $$</center>
<center>$$ h_i^j = pBLSTM(h_{i-1}^{j},[h_{2i}^{j-1},h_{2i+1}^{j-1}]) $$</center>
<center>$$ c_i = AttentionContext(s_i, h)   $$</center>
<center>$$ s_i = RNN(s_{i−1}, y_{i−1}, c_{i−1})  $$</center>
<center>$$ P(y_i|x, y_{<i}) = CharacterDistribution(s_i, c_i)  $$</center>
  

- <mark style='background-color: #dcffe4'> Attention Mechanism </mark>

자, 여기서 핵심은 어텐션 매커니즘입니다.

과연 $$context \space vector, c_i$$를 어떻게 구하는가에 대한 문제입니다.

이를 그림으로 나타내기 위해서 직접 그렸기 때문에 조금 수식이나 논문과 미스매치가 있을 순 있으나 큰 틀에서 문제가 없으니 이해하실 수 있을겁니다.

우선 방금 직전 까지 이어받은 맥락 정보가담긴 벡터를 ```MLP```에 태웁니다.

그리고 인코더(Listener)의 출력값들인 메모리들 또한 ```MLP```를 태웁니다.

식으로 나타내면 아래와 같이 됩니다.

<center>$$ \phi(s_i) $$</center>

<center>$$ \psi(h_u) $$</center>

이때 출력 벡터들의 차원은 맞춰지게끔 코딩하면, 같은 hidden size = 512의 ```LSTM```을 쓰더라도 encoder가  ```Bi-LSTM```이기 때문에 1024차원이 되는것을,
디코더 LSTM의 출력 차원인 512차원으로 만들 수 있습니다.

![attn2](https://user-images.githubusercontent.com/48202736/107076875-6f00b100-682f-11eb-8b7e-7e63ac35e450.png)
![attn3](https://user-images.githubusercontent.com/48202736/107076878-6f994780-682f-11eb-934c-82e8d883e8a1.png)
*Fig. 여기서 그림을 조금 잘못 그렸는데요, Encoder Output을 mlp 두번 태우는게 아니고, mlp한번만 태워 다른 벡터들을 만들어냅니다. 그리고 디코더 쪽에서 RNN만 현재 디코딩 스텝의 히든 벡터가, RNN만 통과하고 mlp를 통과하지 않는것처럼 연출됐는데, 제 실수가 맞으니 저기서 mlp 한번 더 태운다고 생각하시면 될 것 같습니다.*

그리고 이 둘을 벡터들 간 내적하는겁니다. 

(현재 decoding step의 입력값 (정확히는 rnn한번 통과한) 벡터와 인코더 메모리 벡터들과 하나씩 내적하는거죠)

<center>$$ e_{i,u} = <\phi(s_i),\psi(h_u)> $$</center>

같은 차원의 벡터간 내적은 하나의 scalar값을 출력해 냅니다. 이것을 인코더 출력의 개수만큼 내적했으니 그 갯수만큼 출력값이 나올겁니다. 그림을 보는게 더 이해가 빠르실 수 있습니다.

![attn4](https://user-images.githubusercontent.com/48202736/107076881-70ca7480-682f-11eb-9936-49156a0a856b.png)
![attn5](https://user-images.githubusercontent.com/48202736/107076883-71630b00-682f-11eb-82c3-e971b9122e84.png)
*Fig. 벡터간 내적을 통해 각각의 벡터가 얼마나 유사한지 (Cosine) Similarity를 구해냅니다.*

바로 이 scalar값, 여기선 energy라고 부르는데, 이 값이 '현재 토큰을 디코딩하는데 필요한 각 memory 벡터의 중요도'가 되는겁니다.

즉 현재 디코딩을 하는데 있어서 필요가 없다고 판단되는 것들은 알아서 죽어버리도록 학습되게 하는게 목표인거죠.

그렇게 구한 scalar값들을 softmax 함수를 통해서 전체 값의 합이 1이 되게 합니다. 

<center>$$ a_{i,u} = \frac{exp(e_{i,u})}{exp(e_{i,u})} $$</center>

![attn6](https://user-images.githubusercontent.com/48202736/107076886-71630b00-682f-11eb-908e-c067a13d7301.png)
*Fig. Softmax함수는 벡터의 요소들의 총합이 1이 되게끔 normalize해줍니다.*

이제 마지막으로 각각의 scalar값들과 그에 해당하는 memory 벡터을 곱한뒤에(scailing 해주는거죠 각각의 벡터들을) 더해서 새로운 $$context \space vector, c_i$$를 만들어내는 겁니다.

<center>$$ c_i = \sum_u a_{i,u}h_{u}  $$</center>

![attn7](https://user-images.githubusercontent.com/48202736/107076887-71fba180-682f-11eb-96e0-eddd990b8743.png)
*Fig. 각각 벡터와 scalar를 곱해서 이를 더하면 디코딩에 필요한 새로운 강력한 맥락 정보 벡터가 만들어집니다.*

이를 디코더에 넘겨줘 디코딩을 하면 끝입니다.

![attn8](https://user-images.githubusercontent.com/48202736/107076891-71fba180-682f-11eb-9dab-2e1e95d9bb32.png)
*Fig. context vecotr를 마지막 rnn 레이어의 입력값으로 넣어 최종적으로 다음 토큰을 디코딩해냅니다. *

그리고 이 $$context \space vector, c_i$$를 다음 디코딩 할 때도 쓰고 이를 반복하면서 디코딩을 하는거죠.

![attn9](https://user-images.githubusercontent.com/48202736/107076894-72943800-682f-11eb-8e4f-854200835daa.png)
*Fig. $$y_2$$를 예측하는거부터 시작했는데 그 시작 부분에는 사실 context vector가 입력단에 들어가는 표현을 못했습니다. 사실 지금처럼 입력 단에서부터 context vector가 들어가는게 맞습니다.*


  
- <mark style='background-color: #dcffe4'> Learning </mark>

<center>$$ max_{\theta} \sum_i log P(y_i | x, y_{<i}^{\ast}; \theta) $$</center>
  
<center>$$ \tilde{y_i} ∼ CharacterDistribution(s_i, c_i)  $$</center>
<center>$$ max_{\theta} \sum_i log P(y_i | x, \tilde{y_{<i}}; \theta) $$</center>

- <mark style='background-color: #dcffe4'> Decoding and Rescoring </mark>

<center>$$ \hat{y} = \arg \max_y log P(y|x)  $$</center>
<center>$$ s(y|x) = \frac{ log P(y|x) }{|y|_c} + λ log P_{LM}(y)  $$</center>

- <mark style='background-color: #dcffe4'> Attention Visualization </mark>

<img width="790" alt="las2" src="https://user-images.githubusercontent.com/48202736/107019521-1c020c00-67e5-11eb-841a-2641dab3d3b5.png">


- <mark style='background-color: #fff5b1'> References </mark>
