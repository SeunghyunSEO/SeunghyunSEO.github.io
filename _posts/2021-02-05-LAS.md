---
title: Attention Based Seq2Seq for ASR
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Sequence Generation(Modeling) Tasks </mark>

우리가 하고싶은것은 입력 x를 받았을때 가장 그럴듯한(likely) 출력값 y를 뽑아내는 것입니다.

그러기 위해서 당연히 likelihood 혹은 log-likelihood $$p(y \mid x)$$를 최대화 하는 방식으로 학습하게 될겁니다.

이런 Sequence-to-Sequence (Seq2Seq) likelihood를 모델링 하는 여러 방법들 중 이번에 다룰것은 Encoder-Decoder Seq2Seq Model 입니다.

- <mark style='background-color: #dcffe4'> Encoder-Decoder Seq2Seq Model </mark>

아래의 그림을 볼까요? (제가 그렸는데 못그려도 이해 부탁드립니다 ㅎㅎ)

![seq2seq](https://user-images.githubusercontent.com/48202736/107010821-35ea2180-67da-11eb-8881-bb9287ea49e7.png)
{: style="width: 80%;" class="center"}
*Fig. 1. Vanilla Encoder-Decoder Seq2Seq Network*

그림이 의미하는 바는 다음과 같습니다.

> 1. 입력값이 Encoder에 들어가고 <br> 
> 2. 어떠한 무슨 정보를 디코더에 넘겨줌 (이는 입력값을 잘 표현한 정보) <br>
> 3. 디코더가 예측한 hypothesis y값을 뱉음 <br>

뭐 당연히 추론한 $$\hat{y}$$랑 진짜 정답 $$y$$를 비교해서 모델 파라메터를 학습하겠죠. 

이제 이 과정을 순차적으로 다시 생각해볼까요?

![seq2seq2](https://user-images.githubusercontent.com/48202736/107010828-37b3e500-67da-11eb-927a-64fd6251d849.png)
{: style="width: 40%;" class="center"}
```
1. 인코더(일단 RNN 이라고 하겠습니다) 에 입력 값을 넣습니다.
```

![seq2seq3](https://user-images.githubusercontent.com/48202736/107010829-384c7b80-67da-11eb-972d-1933a7d0532f.png)
{: style="width: 60%;" class="center"}
```
2. RNN 인코더를 쭉 통과한 어떤 벡터가 나온다, 이를 입력 seqeunce에 대한 문맥 정보가 압축된 표현 벡터(Representation Vector)라고 한다
```

![seq2seq4](https://user-images.githubusercontent.com/48202736/107010831-384c7b80-67da-11eb-8ee8-6fdc9a859c03.png)
{: style="width: 80%;" class="center"}
```
3. 디코더에서 '자 이제 출력 sequence를 뽑아낼거다'라는 사인을 의미하는 <s> 토큰과 representation vector를 받아서 첫번째 토큰을 출력한다.
```

![seq2seq5](https://user-images.githubusercontent.com/48202736/107010836-38e51200-67da-11eb-8665-604fbb91df86.png)
{: style="width: 80%;" class="center"}
```
4. 처음 출려된 값을 다음 디코더 입력값에 넣어주고, representation vector또한 디코더 rnn cell 하나를 통과했으니 정보가 업데이트 된다
```

![seq2seq6](https://user-images.githubusercontent.com/48202736/107010838-397da880-67da-11eb-86b4-9bc97f2909e6.png)
```
5. 이렇게 디코더에서 iterative하게 (Auto-Regressive, AR)하게 출력된 토큰들은 각각 앞토큰에 대한 정보가 conditional하게 반영되면서 뽑힌 토큰들이다.
```

![seq2seq7](https://user-images.githubusercontent.com/48202736/107010840-3a163f00-67da-11eb-9e49-932e09f6bd84.png)
```
6. 이를 다 곱하면 likelihood가 된다.
```

![seq2seq8](https://user-images.githubusercontent.com/48202736/107010842-3a163f00-67da-11eb-825d-7e526ceae0d7.png)
```
7. log-likelihood를 최대화 하는것, 다르게 표현하면 여기서는 각 토큰마다 정답과 일일히 분류 문제를 푸는것이라고 볼 수 있는데, 그렇기 때문에 Cross Entropy Loss를 사용해서 Loss를 구한다
```
```
8. 오차 역전파 (Error Back Propagation)을 통해 파라메터를 업데이트 하여 Loss를 최소화 하는 방향으로 최적화한다.
```


하지만 이게 최선의 방법일까요? 

![seq2seq9](https://user-images.githubusercontent.com/48202736/107010845-3aaed580-67da-11eb-83d9-0718f11cd18c.png)

아닙니다.

이러한 RNN 기반의 Vanilla Encoder Decoder Seq2Seq Model은 몇가지 단점이 있는데,
그 중 하나는 바로 전파되는 Representation Vector의 길이가 입력의 맥락 정보를 다 표현하기에는 부족하다는 것입니다. 
생각해보면 RNN의 Hidden Size가 768 이어도 이 작은 하나의 벡터를 문맥 정보를 전부 다 담은 x를 표현한 벡터라고 하기엔 문제가 있죠.
그리고 RNN이 Hidden Vector에 정보를 업데이트하면서 끝까지 밀어 넘겨주는데, 이 때 입력값의 크기가 너무 크면 정보를 전달하면서 예전 정보를 잃어버리게 되는 문제도 있습니다.


그렇기 때문에 개발된 방법론이 바로 Attention Mechanism을 이용한 Seq2Seq Model입니다.

- <mark style='background-color: #dcffe4'> Attention-based Encoder-Decoder Seq2Seq Model </mark>

이 방법론은 위에서 말한 문제점을 보완하기 위해 제안된 방법입니다.

핵심은 
```
Representation Vector 하나에 모든 정보를 담지 말고, 입력사이즈 만한 Representation Memory를 만들고(사실 RNN cell의 출력값이 두개가있죠, 매 스텝마다 출력된 값들 모아둔겁니다.) 에서 Attention을 이용해 '이번 토큰을 만들어 낼 때는 입력값에서 어느 부분의 정보를 가져오면 될까?(어디를 집중!(Attention)하면 될까?'라는 것을 고려해서 매 번 디코딩 할 때 마다 필요한 정보만 뽑아오자 
```
라는 겁니다.

이것도 순차적으로 살펴볼까요?

![seq2seq11](https://user-images.githubusercontent.com/48202736/107010847-3b476c00-67da-11eb-95ba-b1fcfc9d1b3b.png)
{: style="width: 80%;" class="center"}
```
1. 마찬가지로 인코더는 입력x를 rnn에 통과시켜 Representation Matrix를 뽑아서 킵해둡니다.
```

![seq2seq12](https://user-images.githubusercontent.com/48202736/107010848-3be00280-67da-11eb-9344-eaffb1b2a74d.png)
{: style="width: 80%;" class="center"}
```
2. 하나씩 디코딩을 합니다. 하지만 여기선 더 거대한 정보인 Memory에서 정보를 가져옵니다. 
-> 근데 어떻게요? (답은 Attention 연산을 통해서 어디에 집중할지를 학습을 통해 익히고 가져오는겁니다. 이따가 설명드리도록 하겠습니다.)
```

![seq2seq13](https://user-images.githubusercontent.com/48202736/107010850-3be00280-67da-11eb-9565-b77b6c534da4.png)
{: style="width: 80%;" class="center"}

![seq2seq14](https://user-images.githubusercontent.com/48202736/107010852-3c789900-67da-11eb-8d60-22fcc83e9dd8.png)
{: style="width: 80%;" class="center"}

![seq2seq15](https://user-images.githubusercontent.com/48202736/107010854-3c789900-67da-11eb-8402-c562b4a11a7a.png)
{: style="width: 80%;" class="center"}

![seq2seq16](https://user-images.githubusercontent.com/48202736/107010856-3d112f80-67da-11eb-8682-ff7ace350ed5.png)

![seq2seq17](https://user-images.githubusercontent.com/48202736/107010858-3d112f80-67da-11eb-9263-8abe35a2d6c4.png)

![seq2seq18](https://user-images.githubusercontent.com/48202736/107010860-3da9c600-67da-11eb-836f-93dafe83c135.png)

![seq2seq19](https://user-images.githubusercontent.com/48202736/107010862-3da9c600-67da-11eb-9cca-97b48986eebe.png)
```
3. 최종적으로 likelihood $$p(y|x)$$를 구했습니다. 마찬가지로 토큰 하나 하나에 대해서 분류 문제를 푸는 문제로 바꿀 수 있습니다.
```

![seq2seq20](https://user-images.githubusercontent.com/48202736/107010863-3e425c80-67da-11eb-82fc-afc405194b26.png)
```
4. 각 토큰들에 대해 Cross Entropy Loss를 최소화 하는 방식으로 학습을 하면 됩니다.
```

이러한 Seq2Seq모델에서 중요한건

> 1. Encoder가 입력 x에대한 Representation Memory를 잘 만들어내기만 하면 된다. <br>
> 2. Decoder가 이를 받아서 하나씩 디코딩한다. (Attention연산을 통해 Memory의 일부를 받던지, 아니면 naive하게 벡터 하나로 계속 하던지) <br>

이 두가지입니다.

1번의 'Encoder가 입력 x에대한 Representation Memory를 잘 만들어내기만 하면 된다.'는 정말 어떤 방식으로든 만들어내면 되기 때문에
CNN으로 해도 되고, RNN으로 해도 되고, Transformer 뭘로 정보를 뽑아내도 상관 없습니다. 

하지만 일반적으로 이런 
네트워크가 풀고자 하는 문제가 시간 순서에 따라 정보가 연관되어 있는 시계열 데이터에 대한 문제들이기 때문에 Transformer나 RNN이 더 적합할 수 있습니다.

```
추가로 Encoder에 대해서 음성인식의 경우 Wav2Vec 2.0이라던가, 기계번역의 경우 BERT라던가 하는 사전학습(Pre-trained)된 네트워크를 쓰면 더욱 좋습니다.
걔네가 더 좋은 Representation을 뽑아줄거니까요!
```



- <mark style='background-color: #dcffe4'> Applications </mark>

이렇게 Seq2Seq Model

![seq2seq21](https://user-images.githubusercontent.com/48202736/107010865-3e425c80-67da-11eb-88a2-df1d94282b61.png)
![seq2seq22](https://user-images.githubusercontent.com/48202736/107010866-3edaf300-67da-11eb-9726-e2b62a1415eb.png)
![seq2seq23](https://user-images.githubusercontent.com/48202736/107010867-3f738980-67da-11eb-8531-d2a80479fa73.png)

- <mark style='background-color: #dcffe4'> Auto-Regressive(AR)? Non-Auto-Regressive(NAR)? </mark>

![seq2seq24](https://user-images.githubusercontent.com/48202736/107010868-3f738980-67da-11eb-9491-2140c7063bf6.png)
![seq2seq25](https://user-images.githubusercontent.com/48202736/107010869-400c2000-67da-11eb-886c-42a24ef2eabd.png)

- <mark style='background-color: #fff5b1'> Listen, Attend and Spell (LAS) </mark>
