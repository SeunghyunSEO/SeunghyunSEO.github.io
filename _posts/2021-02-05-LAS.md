---
title: Attention Based Seq2Seq for ASR
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Sequence Generation(Modeling) Tasks </mark>

우리가 하고싶은것은 입력 x를 받았을때 가장 그럴듯한(likely) 출력값 y를 뽑아내는 것입니다.

그러기 위해서 당연히 likelihood 혹은 log-likelihood $$p(y \mid x)$$를 최대화 하는 방식으로 학습하게 될겁니다.

이런 Sequence-to-Sequence (Seq2Seq) likelihood를 모델링 하는 여러 방법들 중 이번에 다룰것은 Encoder-Decoder Seq2Seq Model 입니다.

- <mark style='background-color: #dcffe4'> Encoder-Decoder Seq2Seq </mark>

아래의 그림을 볼까요? (제가 그렸는데 못그려도 이해 부탁드립니다 ㅎㅎ)

![seq2seq](https://user-images.githubusercontent.com/48202736/107010821-35ea2180-67da-11eb-8881-bb9287ea49e7.png)
{: style="width: 80%;" class="center"}
*Fig. 1. Vanilla Encoder-Decoder Seq2Seq Network*

그림이 의미하는 바는 다음과 같습니다.

> 1. 입력값이 Encoder에 들어가고 <br> 
> 2. 어떠한 무슨 정보를 디코더에 넘겨줌 <br>
> 3. 디코더가 예측한 hypothesis y값을 뱉음 <br>

뭐 당연히 추론한 $$\hat{y}$$랑 진짜 정답 $$y$$를 비교해서 모델 파라메터를 학습하겠죠. 

이제 이 과정을 순차적으로 다시 생각해볼까요?

![seq2seq2](https://user-images.githubusercontent.com/48202736/107010828-37b3e500-67da-11eb-927a-64fd6251d849.png)
![seq2seq3](https://user-images.githubusercontent.com/48202736/107010829-384c7b80-67da-11eb-972d-1933a7d0532f.png)
![seq2seq4](https://user-images.githubusercontent.com/48202736/107010831-384c7b80-67da-11eb-8ee8-6fdc9a859c03.png)
![seq2seq5](https://user-images.githubusercontent.com/48202736/107010836-38e51200-67da-11eb-8665-604fbb91df86.png)
![seq2seq6](https://user-images.githubusercontent.com/48202736/107010838-397da880-67da-11eb-86b4-9bc97f2909e6.png)
![seq2seq7](https://user-images.githubusercontent.com/48202736/107010840-3a163f00-67da-11eb-9e49-932e09f6bd84.png)
![seq2seq8](https://user-images.githubusercontent.com/48202736/107010842-3a163f00-67da-11eb-825d-7e526ceae0d7.png)

- <mark style='background-color: #dcffe4'> Applications </mark>


- <mark style='background-color: #fff5b1'> Listen, Attend and Spell (LAS) </mark>
