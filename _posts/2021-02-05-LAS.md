---
title: Attention Based Seq2Seq for ASR
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Sequence Generation(Modeling) Tasks </mark>

우리가 하고싶은것은 입력 x를 받았을때 가장 그럴듯한(likely) 출력값 y를 뽑아내는 것입니다.

그러기 위해서 당연히 likelihood 혹은 log-likelihood $$p(y \mid x)$$를 최대화 하는 방식으로 학습하게 될겁니다.

이런 Sequence-to-Sequence (Seq2Seq) likelihood를 모델링 하는 여러 방법들 중 이번에 다룰것은 Encoder-Decoder Seq2Seq Model 입니다.

- <mark style='background-color: #dcffe4'> Encoder-Decoder Seq2Seq </mark>

아래의 그림을 볼까요? (제가 그렸는데 못그려도 이해 부탁드립니다 ㅎㅎ)

![seq2seq](https://user-images.githubusercontent.com/48202736/107010821-35ea2180-67da-11eb-8881-bb9287ea49e7.png)
{: style="width: 80%;" class="center"}
*Fig. 1. Vanilla Encoder-Decoder Seq2Seq Network*

그림이 의미하는 바는 다음과 같습니다.

> 1. 입력값이 Encoder에 들어가고 <br> 
> 2. 어떠한 무슨 정보를 디코더에 넘겨줌 <br>
> 3. 디코더가 예측한 hypothesis y값을 뱉음 <br>

뭐 당연히 추론한 $$\hat{y}$$랑 진짜 정답 $$y$$를 비교해서 모델 파라메터를 학습하겠죠. 이거는 차차 설명하도록 하겠습니다.



- <mark style='background-color: #dcffe4'> Applications </mark>


- <mark style='background-color: #fff5b1'> Listen, Attend and Spell (LAS) </mark>
