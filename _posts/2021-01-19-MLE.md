---
title: Maximum Likelihood Estimation, MLE
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- 최대우도측정 (Maximum Likelihood Estimation, MLE)란 무엇인가?

머신러닝을 공부하게 되면 가장 처음 Maximum Likelihood Estimation (MLE), Maximum A Posterior (MAP), Bayesian Approach 에 대해 들어보게 될 것입니다.

이를 이해하기 위해서 두 가지에 대해 꼭 알아야 합니다.

1.$$likelihood : p(x\mid\theta)$$ 

2.$$posterior\proptolikelihood\timesprior : p(\theta\midx)\proptop(x\mid\theta)p(\theta)$$

그 중 이번장에서 다룰 MLE란 말 그대로 likelihood를 최대화 하는 것입니다. 

- MLE
사실 머신러닝, 딥러닝을 공부하면서 굉장히 많이 들어본 개념이지만 저는 이 개념이 확실히 와닿지 않았습니다.
그래서 먼저 아래 그림을 보면서 MLE가 무엇인지에 대해서 설명해보도록 하겠습니다.<br>

<img src="https://user-images.githubusercontent.com/48202736/104995401-9868c100-5a69-11eb-959a-6c1742dcee8a.png" title="제목"/>

위의 그림에 나와있는 Likelihood 값은 어떤 데이터가 존재할 때 (지금은 1차원 데이터가 x축 상에 뿌려졌 있음) 이에 대응하는 확률 분포(여기서는 가우시안 분포)의 y값을 전부 곱한 값을 여러 분포의 경우로 나눠 나타냅니다.
각 a,b,c 그림은 가우시안 분포의 평균,분산 값이 어떠냐에 따라서 확률분포가 달라지고 그때 마다의 Likelihood 값을 나타냅니다.<br><br>
최대우도측정 (Maximim likelihood estimation, MLE)는 바로 데이터에 대응하는 확률 값들을 가장 높게 만들어주는 가장 그럴듯한(likely) 확률 분포를 학습을 통해 추정하는(혹은 확률 분포의 파라메터를 찾는) 것이라고 할 수 있습니다. 여기서는 가장 Likelihood가 높은 것이 c가 될 것인데 이 때의 가우시안 분포의 파라메터는 mean=0.836, variance=1.035가 됩니다. <br><br>

우리가 추정하고자 하는 파라메터가 가우시안 분포의 mean, variance이기 때문에 이 값에 따른 likelihood값에 2차원 평면에 나타내면 다음과 같습니다.<br>

<img src="https://user-images.githubusercontent.com/48202736/104995426-9ef73880-5a69-11eb-8662-19d94037b4c6.png" title="제목"/>

ML solution은 위의 그림에서 peak 값을 나타내는 파라메터를 찾는 것인데, 이는 우리가 많이 들어본 gradient descent 방식으로 numerical하게 찾는 방식을 사용할 수도 있으나, 지금의 경우에는 closed-form solution이 존재하기 때문에 미분을 통해 한번에 파라메터 값을 찾아낼 수 있습니다.<br><br>

<img src="https://user-images.githubusercontent.com/48202736/105001430-1b424980-5a73-11eb-8e23-cf7207e5cf47.png" title="제목"/>

MLE를 수식적으로 다시 표현해 보겠습니다.<br>

<img src="https://user-images.githubusercontent.com/48202736/104995366-8b4bd200-5a69-11eb-8b6b-97289e4a25d5.png" title="제목"/>


<center>$$\hat{\Theta}=argmax\left[Pr\left(\x...\right)\right]$$<\center>
 



