---
title: Maximum Likelihood Estimation, MLE
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> 최대 가능도 측정 (Maximum Likelihood Estimation, MLE)란 무엇인가? </mark>

머신러닝을 공부하게 되면 가장 처음 Maximum Likelihood Estimation (MLE), Maximum A Posterior (MAP), Bayesian Approach 에 대해 들어보게 될 것입니다.

이들을 이해하기 위해서 두 가지에 대해서 기억해두면 편하기 때문에 우선 적어보도록 하겠습니다.

> 1. $$likelihood : p(x\mid\theta)$$ <br>
> 2. $$posterior \propto likelihood \times prior : p(\theta \mid x) \propto p(x \mid \theta)p(\theta)$$ <br>

이 두개가 무엇인지는 앞으로 설명하도록 하겠습니다.


이번장에서 다룰 MLE란 말 그대로 likelihood를 최대화 하는 파라메터를 추정하는 것입니다. 

- <mark style='background-color: #fff5b1'> MLE </mark>

사실 머신러닝, 딥러닝을 공부하면서 굉장히 많이 들어본 개념이지만 저는 이 개념이 확실히 와닿지 않았습니다.
그래서 먼저 아래 그림을 보면서 MLE가 무엇인지에 대해서 설명해보도록 하겠습니다.<br>

![likelihood1](https://user-images.githubusercontent.com/48202736/106448953-ccf76680-64c6-11eb-813d-3ce650e0fcc3.png)

위의 그림에 나와있는 Likelihood 값은 어떤 데이터가 존재할 때 (지금은 1차원 데이터가 x축 상에 뿌려졌 있음) 이에 대응하는 확률 분포(여기서는 가우시안 분포)의 y값을 전부 곱한 값을 나타냅니다. 
각 a,b,c 그림은 가우시안 분포의 평균,분산 값이 어떠냐에 따라서 확률분포가 달라지고 그때 마다의 Likelihood 값을 나타냅니다.<br><br>
최대 가능도 측정 (Maximim likelihood estimation, MLE)는 바로 데이터에 대응하는 확률 값들을 가장 높게 만들어주는, 그러니까 데이터 x의 분포를 나타내는 가장 그럴듯한(likely) 확률 분포의 파라메터(여기서는 평균,분산)를 학습을 통해 찾아내는 것이라고 할 수 있습니다. 여기서는 가장 Likelihood가 높은 것이 c가 될 것인데 이 때의 가우시안 분포의 파라메터는 mean=0.836, variance=1.035가 됩니다. <br><br>

우리가 추정하고자 하는 파라메터가 가우시안 분포의 mean, variance이기 때문에 이 값에 따른 likelihood값에 2차원 평면에 나타내면 다음과 같습니다.<br>

![mle1](https://user-images.githubusercontent.com/48202736/106448963-cf59c080-64c6-11eb-9386-4e11666f757f.png)

ML solution은 위의 그림에서 peak 값을 나타내는 파라메터를 찾는 것인데, 이는 우리가 많이 들어본 gradient descent 방식으로 numerical하게 찾는 방식을 사용할 수도 있으나, 지금의 경우에는 closed-form solution이 존재하기 때문에 미분을 통해 한번에 파라메터 값을 찾아낼 수 있습니다.<br><br>

<img src="https://user-images.githubusercontent.com/48202736/105001430-1b424980-5a73-11eb-8e23-cf7207e5cf47.png" title="제목"/>


- <mark style='background-color: #fff5b1'> 수식으로 보는 MLE </mark>

MLE를 수식적으로 다시 표현해 보겠습니다. 
말 그대로 likelihood하나만을  maximize하는 것이기 문에 목적함수는 다음과 같이 쓸 수 있습니다.

<center>$$\hat{\Theta}=argmax_\theta[Pr(x_{1...I}\mid\theta)]$$</center>
 
각각의 데이터 포인트가 독립이라고 가정하면 아래와 같이 모든 데이터포인트의 확률의 곱으로 다시 쓸 수 있습니다.

<center>$$\hat{\Theta}=argmax_\theta[\prod_{i=1}^{I}Pr(x_{i}\mid\theta)]$$</center>
 
우리는 가우시안 분포의 파라메터인 평균,분산을 찾고 싶으므로 다음과 같이 바꿔 쓸 수 있습니다.

<center>$$Pr(x_{1...I}\mid\theta) = Pr(x_{1...I}\mid\mu,\sigma^2)$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,sigma^2}[\prod_{i=1}^{I}Pr(x_{i}\mid\mu,\sigma^2)]$$</center>

가우시안 분포는 

<center>$$Pr(x\mid\mu,\sigma^2)=Norm_x[\mu,\sigma^2]=\frac{1}{\sqrt{2\pi\sigma^2}}exp[-0.5\frac{(x-\mu)^2}{\sigma^2}]$$</center> 

이기 때문에, 

<center>$$Pr(x_{1...I}\mid\theta) = \frac{1}{(2\pi\sigma^2)^{I/2}}exp[-0.5\sum_{i=1}^{I}\frac{(x_i-\mu)^2}{\sigma^2}]$$</center>

로 표현할 수 있습니다.


다시 한번 더 정리하면,

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\prod_{i=1}^{I}Pr(x_{i}\mid\mu,\sigma^2)]$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\prod_{i=1}^{I}Norm_{x_i}[\mu,\sigma^2]]$$</center>

- <mark style='background-color: #dcffe4'> 어떻게 likelihood를 최대화 하는 파라메터를 찾을 것인가? </mark>

자, 이제 우리는 위의 식을 최대화 하는 파라메터인 평균,분산 값만 찾으면 됩니다.<br>

어떻게 찾을까요? 일단은 계산을 쉽게 하기 위해서 log를 취합니다. log를 취한 식을 maximize하는것이 원래의 수식을 maximize 하는 것과 같은 이유는 log가 단조 증가 함수이기 때문입니다.

![logarithm1](https://user-images.githubusercontent.com/48202736/106448972-d08aed80-64c6-11eb-80d3-643aa046d0a3.png)

이제 저희가 최대화 하고자 하는 수식은 다음과 같이 됩니다.

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\sum_{i=1}^{I}log[Norm_{x_i}[\mu,\sigma^2]]]$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[ -0.5Ilog[2\pi] - 0.5Ilog\sigma^2 - 0.5 \sum_{i=1}^{I}\frac{(x_i-\mu)^2}{\sigma^2} ]$$</center>

위의 식을 미분해서 0인 값을 구하면 우리는 구하고자하는 파라메터를 구할 수 있게 됩니다.

근데 두 가지의 파라메터를 모두 구해야 하므로 한번은 평균에 대해 미분하고 한번은 분산에 대해 미분하면 우리는 likelihood를 최대화 하는 추정하고자 하는 가우시안 분포의 두 파라메터를 모두 구할 수 있게 됩니다.

MLE의 솔루션인 주어진 데이터에 대한 최적의 평균, 분산 중 평균은 다음과 같이 계산할 수 있습니다.

<center>$$\frac{\partial L}{\partial \mu} = \sum_{i=1}^{I}\frac{x_i-\mu}{\sigma^2}$$</center>

<center>$$\frac{\sum_{i=1}^{I}x_i}{\sigma^2}-\frac{I\mu}{\sigma^2}=0$$</center>

<center>$$\hat{\mu}=\frac{\sum_{i=1}^{I}x_i}{I}$$</center>

분산에 대해서 유사하게 구하면 다음과 같습니다.

<center>$$\hat{\sigma^2}=\frac{\sum_{i=1}^{I}(x_i-\hat{\mu})^2}{I}$$</center>


- <mark style='background-color: #fff5b1'> Maximum likelihood for the normal distribution... </mark>

사실 위에서 유도한 식은 잘 정리해보면 우리가 잘 알고있는 least squares criterion과 같다는 걸 알 수 . 


<center>$$\hat{\mu}=argmax_{\mu}[ -0.5Ilog[2\pi] - 0.5Ilog\sigma^2 - 0.5 \sum_{i=1}^{I}\frac{(x_i-\mu)^2}{\sigma^2} ]$$</center>

<center>$$=argmax_{\mu}[-\sum_{i=1}^{I}(x_i-\mu)^2 ]$$</center>

<center>$$=argmin_{\mu}[\sum_{i=1}^{I}(x_i-\mu)^2 ]$$</center>

- <mark style='background-color: #fff5b1'> Maximum likelihood의 단점 </mark>

Maximum likelihood의 단점은 아래의 그림을 보면 알 수 있습니다. 예를들어 녹색의 분포에서 데이터를 무작위로 두 개 샘플링했다고 생각해봅시다.
데이터는 (a)(b)(c) 처럼 수평선 위에 뿌려질 수 있습니다. (1차원 데이터)

우리가 추정하고자 하는 분포가 가우시안 분포라고 생각해봅시다. MLE 방법으로 가장 likelihood 값이 높은 경우의 파라메터를 구하고 그 확률분포를 그린게 빨간색 선으로 표현된 분포가 됩니다.
데이터가 어떻게 뿌려지느냐에 따라서 정말 다른 결과가 나타납니다.
이는 (b)를 제외하고는 녹색 선의 진짜 분포와는 굉장히 다른 분포들입니다.

![image](https://user-images.githubusercontent.com/48202736/105208721-584e2f00-5b8c-11eb-9ddd-97d9ca2244d3.png)

이런 MLE의 단점을 보완하기 위해서 내가 구하고자하는 확률 분포의 파라메터들에 대한 prior 분포를 정하고 likelihood와 곱한 posterior를 추정하는 방법이 있습니다.
이럴 경우 데이터가 적을 때는 prior의 영향력이 커서 추정 값이 너무 편향되지 않게 잡아주고, 데이터가 많을 경우 likelihood의 영향력이 커지게 되는 효과가 있습니다.

여기서 posterior값중 가장 큰 파라메터 하나만을 선택하는 것이 MAP이고, 모든 파라메터들에 대해서 적분하여 가능한 경우에 대해서 다 고려하는 것이 Bayesian Approach 입니다.


데이터 개수에 따라 MLE, MAP solution이 어떻게 다른지는 다음과 같습니다.

![mle_vs_map](https://user-images.githubusercontent.com/48202736/106448976-d254b100-64c6-11eb-8216-fc5ffdef5828.png)
{width=70%}


그림에서 보면 데이터 개수가 많을수록 MAP에서도 likelihood의 영향력이 커져 ML 솔루션과 비슷한 평균, 분산 값을 최적 값으로 선택하는 것을 알 수 있습니다.

MAP와 Bayesian에 대해서는 다음에 다루도록 하겠습니다.



- <mark style='background-color: #fff5b1'> References </mark>

1. [Prince, Simon JD. Computer vision: models, learning, and inference. Cambridge University Press, 2012.](http://www.computervisionmodels.com/)

2. [Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)
