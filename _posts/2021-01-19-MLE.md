---
title: MLE & MAP(1) - Maximum Likelihood Estimation, MLE
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---

머신러닝/딥러닝을 공부하게 되면 가장 처음 `Maximum Likelihood Estimation (MLE)`, Maximum A Posterior (MAP), Bayesian Approach 에 대해 들어보게 될 것입니다.
특히 대부분의 딥러닝 알고리즘들은 이 MLE 문제를 푸는 알고리즘들이기 때문에 `Likelihood`가 무엇인지 이해하는 것은 매우 중요합니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> 최대 가능도 측정 (Maximum Likelihood Estimation, MLE)란 무엇인가? </mark>

```
In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters.
```
(출처 : [link](https://en.wikipedia.org/wiki/Likelihood_function))

위에서 말한 것 처럼, 통계학적으로 likelihood란 어떠한 통계적 모형(분포)가 주어진 샘플 데이터에 대해 얼마나 잘 맞는가? 를 나타내는 거라고 생각할 수 있습니다.


이번장에서 다룰 Maximum Likelihood Estimation(MLE)이란 말 그대로 Likelihood를 최대화 하는 파라메터를 찾아내는 것입니다. 
(즉 주어진 샘플 데이터를 가장 잘 설명하는 통계적 모형의 파라메터를 찾는 과정인거죠)




## <mark style='background-color: #fff5b1'> Maximum Likelihood Estimation (MLE) </mark>

사실 위에서 한 번 언급했지만 잘 감이 오지 않습니다.
그래서 먼저 아래 그림을 보면서 MLE가 무엇인지에 대해서 설명해보도록 하겠습니다.

![mle1](/assets/images/MLE/mle1.png)
*Fig. 데이터를 가장 잘 설명하는 분포는 과연 뭘까?*

위의 그림에 나와있는 `Likelihood 값`은 어떤 데이터가 존재할 때 (지금은 1차원 데이터가 x축 상에 뿌려졌 있음) 이에 대응하는 확률 분포(여기서는 가우시안 분포)의 y값을 전부 곱한 값을 나타냅니다. 
각 a,b,c 그림은 `가우시안 분포`의 평균,분산 값이 어떠냐에 따라서 확률분포가 달라지고 그때 마다의 Likelihood 값을 나타냅니다.<br><br>
최대 가능도 측정 (Maximim likelihood estimation, MLE)는 바로 데이터에 대응하는 확률 값들을 가장 높게 만들어주는, 그러니까 `데이터 x의 분포를 가장 잘 표현하는, 그러니까 가장 그럴듯한(likely) 확률 분포`의 파라메터(여기서는 평균,분산)를 학습을 통해 찾아내는 것이라고 할 수 있습니다. 여기서는 가장 Likelihood가 높은 것이 c가 될 것인데 이 때의 가우시안 분포의 파라메터는 mean=0.836, variance=1.035가 됩니다. 


우리가 추정하고자 하는 파라메터가 가우시안 분포의 mean, variance이기 때문에 이 값에 따른 likelihood값에 2차원 평면에 나타내면 다음과 같습니다.


![mle2](/assets/images/MLE/mle2.png){: width="60%"}
*Fig. 데이터를 표현하는 분포는 가우시안 분포일거라고 가정했으므로, likelihood는 $$\sigma$$,$$\mu$$로 이루어진 2차원이다.*

위의 그림에서 peak 값을 나타내는 파라메터를 찾는 것인데, 이것이 바로 `최대 우도 측정 (Maximum Likelihood Estimation, MLE)` 방법입니다.  


이는 우리가 많이 들어본 `gradient descent` 방식으로 numerical하게 찾는 방식을 사용할 수도 있으나 (최적화), 지금의 경우에는 비선형 요소가 존재하지 않기 때문에 closed-form solution이 존재하기 대문에 단박에 데이터를 가장 잘 나타내는, 그러니까 Likelihood 값을 가장 크게 하는 파라메터를 찾아낼 수 있습니다.

![mle3](/assets/images/MLE/mle3.png)
*Fig. 최적화를 통해 점진적으로 최적의 파라메터를 찾아낼 수도 있다.*


![ian1](/assets/images/MLE/ian1.png)
*Fig. 점점 초기의 분포는 Likelihood가 최대가 되는 분포로 변하게 된다.*






## <mark style='background-color: #fff5b1'> 수식으로 보는 MLE </mark>

MLE를 수식적으로 다시 표현해 보겠습니다. 
말 그대로 likelihood하나만을  maximize하는 것이기 문에 목적함수는 다음과 같이 쓸 수 있습니다.

<center>$$\hat{\Theta}=argmax_\theta[Pr(x_{1...I}\mid\theta)]$$</center>
 
각각의 데이터 포인트가 독립이라고 가정하면 아래와 같이 모든 데이터포인트의 확률의 곱으로 다시 쓸 수 있습니다.

<center>$$\hat{\Theta}=argmax_\theta[\prod_{i=1}^{I}Pr(x_{i}\mid\theta)]$$</center>
 
우리는 가우시안 분포의 파라메터인 평균,분산을 찾고 싶으므로 다음과 같이 바꿔 쓸 수 있습니다.

<center>$$Pr(x_{1...I}\mid\theta) = Pr(x_{1...I}\mid\mu,\sigma^2)$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,sigma^2}[\prod_{i=1}^{I}Pr(x_{i}\mid\mu,\sigma^2)]$$</center>

가우시안 분포는 

<center>$$Pr(x\mid\mu,\sigma^2)=Norm_x[\mu,\sigma^2]=\frac{1}{\sqrt{2\pi\sigma^2}}exp[-0.5\frac{(x-\mu)^2}{\sigma^2}]$$</center> 

이기 때문에, 

<center>$$Pr(x_{1...I}\mid\theta) = \frac{1}{(2\pi\sigma^2)^{I/2}}exp[-0.5\sum_{i=1}^{I}\frac{(x_i-\mu)^2}{\sigma^2}]$$</center>

로 표현할 수 있습니다.


다시 한번 더 정리하면, 아래의 식을 얻을 수 있습니다.

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\prod_{i=1}^{I}Pr(x_{i}\mid\mu,\sigma^2)]$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\prod_{i=1}^{I}Norm_{x_i}[\mu,\sigma^2]]$$</center>







### <mark style='background-color: #dcffe4'> 어떻게 likelihood를 최대화 하는 파라메터를 찾을 것인가? </mark>

자, 이제 우리는 위의 식을 최대화 하는 파라메터인 평균,분산 값만 찾으면 됩니다.<br>

어떻게 찾을까요? 일단은 계산을 쉽게 하기 위해서 `log`를 취합니다. log를 취한 식을 maximize하는것이 원래의 수식을 maximize 하는 것과 같은 이유는 log가 단조 증가 함수이기 때문입니다.

![mle4](/assets/images/MLE/mle4.png){: width="70%"}
*Fig. Logarithm을 취해도 문제를 푸는데 문제가 없다.*

이제 저희가 최대화 하고자 하는 수식은 다음과 같이 됩니다.

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\sum_{i=1}^{I}log[Norm_{x_i}[\mu,\sigma^2]]]$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[ -0.5Ilog[2\pi] - 0.5Ilog\sigma^2 - 0.5 \sum_{i=1}^{I}\frac{(x_i-\mu)^2}{\sigma^2} ]$$</center>

위의 식을 미분해서 0인 값을 구하면 우리는 구하고자하는 파라메터를 구할 수 있게 됩니다.

근데 두 가지의 파라메터를 모두 구해야 하므로 한번은 평균에 대해 미분하고 한번은 분산에 대해 미분하면 우리는 likelihood를 최대화 하는 추정하고자 하는 가우시안 분포의 두 파라메터를 모두 구할 수 있게 됩니다.

MLE의 솔루션인 주어진 데이터에 대한 최적의 평균, 분산 중 평균은 다음과 같이 계산할 수 있습니다.

<center>$$\frac{\partial L}{\partial \mu} = \sum_{i=1}^{I}\frac{x_i-\mu}{\sigma^2}$$</center>

<center>$$\frac{\sum_{i=1}^{I}x_i}{\sigma^2}-\frac{I\mu}{\sigma^2}=0$$</center>

<center>$$\hat{\mu}=\frac{\sum_{i=1}^{I}x_i}{I}$$</center>

분산에 대해서 유사하게 구하면 다음과 같습니다.

<center>$$\hat{\sigma^2}=\frac{\sum_{i=1}^{I}(x_i-\hat{\mu})^2}{I}$$</center>

하지만 앞서 말했듯 이런식으로 미분을 해서 값을 구할 수 없는 경우가 머신러닝/딥러닝에선 대부분입니다. 
이 부분은 다른 알고리즘을 다루면서 나중에 설명하도록 하겠습니다.






## <mark style='background-color: #fff5b1'> Maximum likelihood solution for the normal distribution... </mark>

사실 위에서 유도한 식은 잘 정리해보면 우리가 잘 알고있는 `least squares solution`과 같다는 걸 알 수 있습니다. 


<center>$$\hat{\mu}=argmax_{\mu}[ -0.5Ilog[2\pi] - 0.5Ilog\sigma^2 - 0.5 \sum_{i=1}^{I}\frac{(x_i-\mu)^2}{\sigma^2} ]$$</center>

<center>$$=argmax_{\mu}[-\sum_{i=1}^{I}(x_i-\mu)^2 ]$$</center>

<center>$$=argmin_{\mu}[\sum_{i=1}^{I}(x_i-\mu)^2 ]$$</center>






## <mark style='background-color: #fff5b1'> Maximum likelihood의 단점 </mark>

Maximum likelihood의 단점은 아래의 그림을 보면 알 수 있습니다. 


예를들어 녹색의 분포에서 데이터를 무작위로 두 개 샘플링했다고 생각해봅시다.
데이터는 (a)(b)(c) 처럼 수평선 위에 뿌려질 수 있습니다. (1차원 데이터)

우리가 추정하고자 하는 분포가 가우시안 분포라고 생각해봅시다. MLE 방법으로 가장 likelihood 값이 높은 경우의 파라메터를 구하고 그 확률분포를 그린게 빨간색 선으로 표현된 분포가 됩니다.
데이터가 어떻게 뿌려지느냐에 따라서 정말 다른 결과가 나타납니다.
이는 (b)를 제외하고는 녹색 선의 진짜 분포와는 굉장히 다른 분포들입니다.

![mle5](/assets/images/MLE/mle5.png){: width="60%"}
*Fig. 실제 분포인 녹색 분포에서 샘플링 된 2개의 데이터 포인트를 기준으로 MLE를 할 경우, 머신러닝의 궁극적인 목적인 실제 분포를 찾는 것과는 다른 솔루션을 얻을 수 밖에 없다. (데이터가 좋지않으면, 적으면)*



이런 MLE의 단점을 보완하기 위해서 내가 구하고자하는 확률 분포의 파라메터들에 대한 `prior 분포`를 정하고 `likelihood`와 곱한 `posterior`를 추정하는 방법이 있습니다.

```
prior란 학습이 실제로 되는 파라메터에 대한 사전 지식으로, "음 mean,variance가 어떤 값을 가질지는 모르겠지만 대충 0~0.5 사이 값이던데?" 라는 선입견을 주는 겁니다.
```


이럴 경우 데이터가 적을 때는 prior의 영향력이 커서 추정 값이 너무 편향되지 않게 잡아주고, 데이터가 많을 경우 likelihood의 영향력이 커지게 되는 효과가 있습니다.

여기서 `posterior값중 가장 큰 파라메터 하나만을 선택하는 것이 MAP`이고, `모든 파라메터들을 고려해서 weighted sum한 결과를 최종 결과로 취하는 것이 Bayesian Approach` 입니다.


데이터 개수에 따라 MLE, MAP solution이 어떻게 다른지는 다음과 같습니다.


![mle6](/assets/images/MLE/mle6.png)
*Fig. 데이터 갯수에 따른 MLE와 MAP 방법론의 결과 차이. 그림에서 분홍색 cross가 나타내는 것은 사전 분포로, "mean=0, variance=0 일 확률이 높더라" 라는 정보를 주고 있으며, 이에 영향받은 MAP와 영향을 받지 않는 MLE는 데이터가 적을때는 값이 차이가 많이 나지만 데이터가 많을 때는 차이가 별로 없어진다.*


MAP와 Bayesian에 대해서는 다음에 다루도록 하겠습니다.

+ p.s) 본문에서 다루지는 않았지만 우리가 추정하고자 하는 밀도 함수가 다른 분포여도 됩니다, 본문에서는 MLE를 설명하기 위해서 가장 간단하고 일반적으로 쓰이는 연속적인 분포 함수인 가우시안 분포만을 사용했습니다. 


## <mark style='background-color: #fff5b1'> References </mark>

1. [Prince, Simon JD. Computer vision: models, learning, and inference. Cambridge University Press, 2012.](http://www.computervisionmodels.com/)

2. [Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)
