---
title: Maximum Likelihood Estimation, MLE
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- 최대우도측정 (Maximum Likelihood Estimation, MLE)란 무엇인가?

머신러닝을 공부하게 되면 가장 처음 Maximum Likelihood Estimation (MLE), Maximum A Posterior (MAP), Bayesian Approach 에 대해 들어보게 될 것입니다.

이를 이해하기 위해서 두 가지에 대해 꼭 알아야 합니다.

1. $$likelihood : p(x\mid\theta)$$ 

2. $$posterior \propto likelihood \times prior : p(\theta \mid x) \propto p(x \mid \theta)p(\theta)$$

그 중 이번장에서 다룰 MLE란 말 그대로 likelihood를 최대화 하는 파라메터를 추정하는 것입니다. 

- MLE
사실 머신러닝, 딥러닝을 공부하면서 굉장히 많이 들어본 개념이지만 저는 이 개념이 확실히 와닿지 않았습니다.
그래서 먼저 아래 그림을 보면서 MLE가 무엇인지에 대해서 설명해보도록 하겠습니다.<br>

<img src="https://user-images.githubusercontent.com/48202736/104995401-9868c100-5a69-11eb-959a-6c1742dcee8a.png" title="제목"/>

위의 그림에 나와있는 Likelihood 값은 어떤 데이터가 존재할 때 (지금은 1차원 데이터가 x축 상에 뿌려졌 있음) 이에 대응하는 확률 분포(여기서는 가우시안 분포)의 y값을 전부 곱한 값을 나타냅니다. 
각 a,b,c 그림은 가우시안 분포의 평균,분산 값이 어떠냐에 따라서 확률분포가 달라지고 그때 마다의 Likelihood 값을 나타냅니다.<br><br>
최대우도측정 (Maximim likelihood estimation, MLE)는 바로 데이터에 대응하는 확률 값들을 가장 높게 만들어주는, 그러니까 데이터 x의 분포를 나타내는 가장 그럴듯한(likely) 확률 분포의 파라메터(여기서는 평균,분산)를 학습을 통해 찾아내는 것이라고 할 수 있습니다. 여기서는 가장 Likelihood가 높은 것이 c가 될 것인데 이 때의 가우시안 분포의 파라메터는 mean=0.836, variance=1.035가 됩니다. <br><br>

우리가 추정하고자 하는 파라메터가 가우시안 분포의 mean, variance이기 때문에 이 값에 따른 likelihood값에 2차원 평면에 나타내면 다음과 같습니다.<br>

<img src="https://user-images.githubusercontent.com/48202736/104995426-9ef73880-5a69-11eb-8662-19d94037b4c6.png" title="제목"/>

ML solution은 위의 그림에서 peak 값을 나타내는 파라메터를 찾는 것인데, 이는 우리가 많이 들어본 gradient descent 방식으로 numerical하게 찾는 방식을 사용할 수도 있으나, 지금의 경우에는 closed-form solution이 존재하기 때문에 미분을 통해 한번에 파라메터 값을 찾아낼 수 있습니다.<br><br>

<img src="https://user-images.githubusercontent.com/48202736/105001430-1b424980-5a73-11eb-8e23-cf7207e5cf47.png" title="제목"/>


- 수식으로 보는 MLE

MLE를 수식적으로 다시 표현해 보겠습니다. 
말 그대로 likelihood하나만을  maximize하는 것이기 떄문에 목적함수는 다음과 같이 쓸 수 있습니다.

<center>$$\hat{\Theta}=argmax_\theta[Pr(x_{1...I}\mid\theta)]$$</center>
 
각각의 데이터 포인트가 독립이라고 가정하면 아래와 같이 모든 데이터포인트의 확률의 곱으로 다시 쓸 수 있습니다.

<center>$$\hat{\Theta}=argmax_\theta[\prod_{i=1}^{I}Pr(x_{i}\mid\theta)]$$</center>
 
우리는 가우시안 분포의 파라메터인 평균,분산을 찾고 싶으므로 다음과 같이 바꿔 쓸 수 있습니다.

<center>$$Pr(x_{1...I}\mid\theta) = Pr(x_{1...I}\mid\mu,\sigma^2)$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,sigma^2}[\prod_{i=1}^{I}Pr(x_{i}\mid\mu,\sigma^2)]$$</center>

가우시안 분포는 

<center>$$Pr(x\mid\mu,\sigma^2)=Norm_x[\mu,\sigma^2]=\frac{1}{\sqrt{2\pi\sigma^2}}exp[-0.5\frac{(x-\mu)^2}{\sigma^2}]$$</center> 

이기 때문에, 

<center>$$Pr(x_{1...I}\mid\theta) = \frac{1}{(2\pi\sigma^2)^{I/2}}exp[-0.5\sum_{i=1}^{I}\frac{(x_i-\mu)^2}{\sigma^2}]$$</center>

로 표현할 수 있습니다.


다시 한번 더 정리하면,

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\prod_{i=1}^{I}Pr(x_{i}\mid\mu,\sigma^2)]$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\prod_{i=1}^{I}Norm_{x_i}[\mu,\sigma^2]]$$</center>

자, 이제 우리는 위의 식을 최대화 하는 파라메터인 평균,분산 값만 찾으면 됩니다.<br>

어떻게 찾을까요? 일단은 계산을 쉽게 하기 위해서 log를 취합니다. log를 취한 식을 maximize하는것이 원래의 수식을 maximize 하는 것과 같은 이유는 log가 단조 증가 함수이기 때문입니다.

<img src="https://user-images.githubusercontent.com/48202736/105206044-3ef7b380-5b89-11eb-93f2-6f81f6c5cc91.png" title="제목"/>

이제 저희가 최대화 하고자 하는 수식은 다음과 같이 됩니다.

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[\sum_{i=1}^{I}log[Norm_{x_i}[\mu,\sigma^2]]]$$</center>

<center>$$\hat{\mu},\hat{\sigma^2}=argmax_{\mu,\sigma^2}[ -0.5Ilog[2\pi] - 0.5Ilog\sigma^2 - 0.5 \sum_{i=1}^{I}\frac{(x_i-\mu)^2}{sigma^2} ]$$</center>

위의 식을 미분해서 0인 값을 구하면 우리는 구하고자하는 파라메터를 구할 수 있게 됩니다.

근데 두 가지의 파라메터를 모두 구해야 하므로 한번은 평균에 대해 미분하고 한번은 분산에 대해 미분하면 우리는 likelihood를 최대화 하는 추정하고자 하는 가우시안 분포의 두 파라메터를 모두 구할 수 있게 됩니다.

MLE의 솔루션인 평균의 경우 다음과 같이 계산할 수 있습니다.

<center>$$\frac{\partial L}{\partial \mu} = \sum_{i=1}^{I}\frac{x_i-\mu}{\sigma^2}$$</center>

<center>$$\frac{\sum_{i=1}^{I}x_i}{\sigma^2}-\frac{I\mu}{\sigma^2}=0$$</center>

<center>$$\hat{\mu}=\frac{\sum_{i=1}^{I}x_i}{I}$$</center>

분산에 대해서 똑같이 구하면 다음과 같습니다.

<center>$$\hat{\sigma^2}=\frac{\sum_{i=1}^{I}(x_i-\hat{\mu})^2}{I}$$</center>


