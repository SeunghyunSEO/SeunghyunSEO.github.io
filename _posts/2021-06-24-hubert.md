---
title: (미완) (Paper) Hubert, How Much Can a Bad Teacher Benefit ASR Pre-Training?
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true

comments: true
---

이번에 report 하려고 하는 논문은 2021 ICASSP에 발표된 Wav2Vec 시리즈에 이은 (후속작?) Facebook AI Research (FAIR)의 Self-Supervised Learning for Speech 알고리즘 입니다. 

- Paper : [Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?](https://arxiv.org/pdf/2106.07447)
- Code : [HuBERT from pytorch/fairseq/examples/hubert](https://github.com/pytorch/fairseq/tree/master/examples/hubert?fbclid=IwAR3TsIvqvUuFcoenNbp6yqt6luNypmwpSLLYQp9uvTNSJYDRcDeZmYTo2EM)
- Blog : [HuBERT: Self-supervised representation learning for speech recognition, generation, and compression](https://ai.facebook.com/blog/hubert-self-supervised-representation-learning-for-speech-recognition-generation-and-compression/)




---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> Problem Definition and Contibution Points </mark>



## <mark style='background-color: #fff5b1'> Proposed Model Architecture </mark>

![hubert](/assets/images/hubert/hubert_architecture.jpeg){: width="80%"}
*Fig.*


## <mark style='background-color: #fff5b1'> Preliminaries </mark>

### <mark style='background-color: #dcffe4'> Pseudo-Labeling </mark>

`Self-Training`이라고도 알려진 Pseudo-Lbaeling은 unlabeld speech 데이터를 사용하기 위해 적은량의 labeld speech-text pair data $$D_l = \{ (X_j,Y_j) \}_{j=1}^{N_l}$$를 사용해 학습한 Teacher ASR model, $$g$$를 우선 학습하고, 이를 unlabeld data의 label을 만들어내는 용도로 사용하는 겁니다.

![pseudo-labeling](/assets/images/hubert/pseudo-labeling.png){: width="70%"}
*Fig.*

(이미지 출처 : [Pseudo-labeling a simple semi-supervised learning method](https://datawhatnow.com/pseudo-labeling-semi-supervised-learning/))


![pseudo](/assets/images/hubert/pseudo.png){: width="80%"}
*Fig.*

(이미지 출처 : [Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks](https://www.semanticscholar.org/paper/Pseudo-Label-%3A-The-Simple-and-Efficient-Learning-Lee/798d9840d2439a0e5d47bcf5d164aa46d5e7dc26))

### <mark style='background-color: #dcffe4'> Masked Prediction </mark>

![bert](/assets/images/hubert/bert.png)
*Fig.*

(이미지 출처 : [DeepMind x UCL, Deep Learning Lectures 7/12 Deep Learning for Natural Language Processing](https://www.youtube.com/watch?v=8zAP2qWAsKg))


## <mark style='background-color: #fff5b1'> Clustering for Unsupervised Pseudo Lebeling </mark>

## <mark style='background-color: #fff5b1'> Pre-Training via Masked Pseudo Label Prediction </mark>


## <mark style='background-color: #fff5b1'> Teacher Ensembling and Iterative Refinement </mark>

## <mark style='background-color: #fff5b1'> Implementation </mark>

## <mark style='background-color: #fff5b1'> Experiments and Results </mark>





## <mark style='background-color: #fff5b1'> Reference </mark>

