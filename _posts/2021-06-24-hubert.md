---
title: (미완) (Paper) Hubert, How Much Can a Bad Teacher Benefit ASR Pre-Training?
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true

comments: true
---

이번에 report 하려고 하는 논문은 2021 ICASSP에 발표된 Wav2Vec 시리즈에 이은 (후속작?) Facebook AI Research (FAIR)의 Speech Representation을 위한 Self-Supervised Learning 알고리즘 입니다. 

- Paper : [Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?](https://arxiv.org/pdf/2106.07447)
- Code : [HuBERT from pytorch/fairseq/examples/hubert](https://github.com/pytorch/fairseq/tree/master/examples/hubert?fbclid=IwAR3TsIvqvUuFcoenNbp6yqt6luNypmwpSLLYQp9uvTNSJYDRcDeZmYTo2EM)
- Blog : [HuBERT: Self-supervised representation learning for speech recognition, generation, and compression](https://ai.facebook.com/blog/hubert-self-supervised-representation-learning-for-speech-recognition-generation-and-compression/)




---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> Problem Definition and Contibution Points </mark>

- Natural Language Processing (NLP) 와 Compute Vision (CV) 에서와 다르게 음성 도메인에서 Representation을 학습하기란 어려움.
  - NLP : NLP의 특징은 입력 벡터들이 일단 discrete 하다는 것 (one-hot vector -> embedding look-up)과 이런 단어 벡터들이 sequence로 되어있다는 것임. 그래서 이에 잘 맞는 BERT, GPT와 같은 학습 방법이 제안되었고 엄청난 성공을 거둠
  - CV : CV의 특징은 입력 벡터들이 continuous 하다는 것 (이는 speech와 유사함)이며, 그렇기 때매 MoCo나 SimCLR 같이 입력 이미지를 Augmentation 한 뒤 instance level contrastive learning 을 하거나 pixel level에서 contrastive learning을 함. 
- Speech Representation을 학습하기 어려운 이유로 논문에서는 3가지를 언급함.
  - 일단 continuous-valued sequence임.
  - 1.입력 utterance에 여러가지 다양한 사운드가 들어있음 (실제 문장을 말하는 거 + 웃음소리 + 주변 소음 등)
  - 2.NLP는 BERT같은 모델을 pre-training할 때 word나 word-piece를 사용하는 등의 방법을 사용하지만, 음성에서는 이를 적용하기 어렵고, 따라서 predictive loss를 쓰기 어려움.  
  - 3.어디서부터 어디까지의 벡터들이 어떤 음성을 의미하는지 boundary가 없음. (즉 "안녕하세요"라고 말했는데 0.5초~0.7초는 "안"이라고 할 만한 정보가 없다는 것)
- 사실 Wav2Vec 2.0의 결함을 지적하면서 새로운 방법론의 필요성을 어필한 논문은 아니고, 더 간단하고 좋은 Representation을 학습하는 방법을 제시해 음성 10분, 1시간, 10시간 등의 low-resource 상황에서 W2V 2.0보다 나은 성능을 보였다가 Key point 인 것 같음.

![blog_figure1](/assets/images/hubert/blog_figure1.png)
*Fig.*





## <mark style='background-color: #fff5b1'> Proposed Model Architecture </mark>

![hubert](/assets/images/hubert/hubert_architecture.jpeg){: width="80%"}
*Fig.*






## <mark style='background-color: #fff5b1'> Preliminaries </mark>

### <mark style='background-color: #dcffe4'> Pseudo-Labeling </mark>

`Self-Training`이라고도 알려진 Pseudo-Lbaeling은 unlabeld speech 데이터를 사용하기 위해 적은량의 labeld speech-text pair data $$D_l = \{ (X_j,Y_j) \}_{j=1}^{N_l}$$를 사용해 학습한 Teacher ASR model, $$g$$를 우선 학습하고, 이를 unlabeld data의 label을 만들어내는 용도로 사용하는 겁니다.

![pseudo-labeling](/assets/images/hubert/pseudo-labeling.png){: width="60%"}
*Fig.*

(이미지 출처 : [Pseudo-labeling a simple semi-supervised learning method](https://datawhatnow.com/pseudo-labeling-semi-supervised-learning/))


![pseudo](/assets/images/hubert/pseudo.png){: width="80%"}
*Fig.*

(이미지 출처 : [Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks](https://www.semanticscholar.org/paper/Pseudo-Label-%3A-The-Simple-and-Efficient-Learning-Lee/798d9840d2439a0e5d47bcf5d164aa46d5e7dc26))




### <mark style='background-color: #dcffe4'> Masked Prediction </mark>

![bert](/assets/images/hubert/bert.png)
*Fig.*

(이미지 출처 : [DeepMind x UCL, Deep Learning Lectures 7/12 Deep Learning for Natural Language Processing](https://www.youtube.com/watch?v=8zAP2qWAsKg))




### <mark style='background-color: #dcffe4'> K-means Clustering </mark>


![kmeans](/assets/images/hubert/kmeans.gif)
*Fig.*







## <mark style='background-color: #fff5b1'> Method </mark>

### <mark style='background-color: #dcffe4'> Clustering for Unsupervised Pseudo Lebeling </mark>

### <mark style='background-color: #dcffe4'> Pre-Training via Masked Pseudo Label Prediction </mark>


### <mark style='background-color: #dcffe4'> Teacher Ensembling and Iterative Refinement </mark>

### <mark style='background-color: #dcffe4'> Implementation </mark>









## <mark style='background-color: #fff5b1'> Experiments and Results </mark>

![paper_figure1](/assets/images/hubert/paper_figure1.png)
*Fig.*

![paper_figure2](/assets/images/hubert/paper_figure2.png)
*Fig.*

![paper_figure3](/assets/images/hubert/paper_figure3.png)
*Fig.*

![paper_figure4](/assets/images/hubert/paper_figure4.png)
*Fig.*

![paper_figure5](/assets/images/hubert/paper_figure5.png)
*Fig.*

![paper_figure6](/assets/images/hubert/paper_figure6.png)
*Fig.*

![paper_figure7](/assets/images/hubert/paper_figure7.png)
*Fig.*





### <mark style='background-color: #dcffe4'> tmp </mark>

![blog_figure1](/assets/images/hubert/blog_figure1.png)
*Fig.*

![blog_figure2](/assets/images/hubert/blog_figure2.png)
*Fig.*

![blog_figure3](/assets/images/hubert/blog_figure3.png)
*Fig.*




## <mark style='background-color: #fff5b1'> Reference </mark>

