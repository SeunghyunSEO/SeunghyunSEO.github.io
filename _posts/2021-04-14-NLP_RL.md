---
title: Natural Language Generation with Reinforcement Learning
categories: Reinforcement_Learning_and_Deep_Reinforcement_Learning
tag: [RL]

toc: true
toc_sticky: true
---

이 글은 마키나락스에서 자연어 처리를 연구중이신 `김기현`님의 `자연어 처리 딥러닝 캠프, 파이토치 편`의 12장을 읽고 임시로 정리한 글 입니다. 

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> Natural Language Generation with Reinforcement Learning </mark>

강화학습을 이용해 자연어를 생성한다는 것은 시퀀스를 만들어내는 타겟 모델을 학습하는 방법으로 Maximum Likelihood Estimation, MLE를 사용하지 않고 
강화학습의 학습 방법인 정책 경사 알고리즘 (Policy Gradient)를 사용해 학습하는 것을 말합니다.


이번 글에서는 이것이 어떤 의미를 가지게 되고 (MLE와 어떻게 다른지), 어디서부터 기인?했는지에 대해서 짧게 이야기 해보도록 하겠습니다.

### <mark style='background-color: #dcffe4'> Ganerative Adversarial Network (GAN) </mark>

`생성적 적대 신경망 (Generative Adversarial Network, GAN)` 이라는 모델은 Ian Goodfellow가 제안한 기법으로 과거 부터 제안되어온 머신 러닝의 생성 모델들 중 하나로,
변분 오토 인코더 (Variational Auto Encoder, VAE)와 함께 현대 딥 러닝 방법론들 중의 대표적인 생성 모델 중 하나 입니다.

생성 모델은 데이터가 샘플링 됐을 법한 실제 데이터 분포를 추정하는 방법으로 판별 모델(Discriminative Model)과는 약간 다른데, 이렇게 실제 훈련 데이터셋이 샘플링 된 데이터 분포를 추정하게 되면 (이정도 분포를 추정하려면 데이터가 정교하고, 학습하는 방법이 고도화 돼야 합니다. 물로 계산량도 많이 들고) 얻은 분포를 통해서 훈련 집합에는 존재하지 않는 샘플들을 뽑아낼 수 있습니다.


GAN은 이러한 생성 모델들의 어려운 점을 잘 해결하기 위해 디자인 된 모델이며, 이런식으로 데이터 분포에서 훈련 데이터셋을 더 뽑아 훈련하는데 사용하게 되면 모델의 퍼포먼스를 올릴 수 있게 됩니다.  


이러한 아이디어를 Sequence Genration 에도 적용해 생각해 보도록 하겠습니다. 
우리는 `seq2seq(G)`에서 생성한 문장과 실제 데이터 셋에서 나온 문장을 판별하는 `Disctiminator(D)`를 두어서 GAN과 같은 방법론으로 `seq2seq Generator(G)`를 학습할 수 있을 것입니다.


하지만 우리는 이러한 아이디어로 네트워크를 단박에 학습할 수 없는데, 그 이유는 seq2seq의 결과는 각 생성된 토큰들(마지막에 logit값들에 softmax를 적용한 character distribution 토큰)에 샘플링, 혹은 argmax operation을 적용한 이산적인(discrete) 토큰이기 때문입니다. 여기서 argmax operation을 취하는 것은 미분이 불가능한 연산이기 때문에 오차 역전파 알고리즘에 따라 네트워크를 학습할 수 없게 되는 것이죠.  


이처럼 GAN의 아이디어를 자연어 생성에 적용하는 것은 쉽지 않은데(미분 불가능한 문제를 해결 하기 위해 Gumbel Softmax를 사용한다던가, 후에 기술할 REINFORCE 등을 사용하면 가능 할 수 있음) 
여기에 강화학습을 적용하면 이야기는 달라질 수 있습니다.

## <mark style='background-color: #fff5b1'> Basic Reinforcement Learning </mark>

강화학습을 사용한다는 것은 우리가 더이상 자연어 생성 모델을 학습하는 데 생성 레이블과 정답 레이블간를 cross entropy loss를 사용해 점점 모델이 정답 레이블을 뱉게 끔 학습하는 것이 아니라,
어떠한 `보상(reward)`을 정의하고 이를 극대화 하는 방법을 사용하겠다는 것입니다.


### <mark style='background-color: #dcffe4'> Policy based Reinforcement Learning </mark>

### <mark style='background-color: #dcffe4'> MLE vs Policy Gradient </mark>

### <mark style='background-color: #dcffe4'> REINFORCE Algorithm </mark>






## <mark style='background-color: #fff5b1'> Natural Language Generation and Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Supervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Unsupervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> References </mark>

