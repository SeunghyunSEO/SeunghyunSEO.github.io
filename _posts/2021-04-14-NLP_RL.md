---
title: Natural Language Generation with Reinforcement Learning
categories: Reinforcement_Learning_and_Deep_Reinforcement_Learning
tag: [RL]

toc: true
toc_sticky: true
---

이 글은 마키나락스에서 자연어 처리를 연구중이신 `김기현`님의 `자연어 처리 딥러닝 캠프, 파이토치 편`의 12장을 읽고 임시로 정리한 글 입니다. 

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> Natural Language Generation with Reinforcement Learning </mark>

강화학습을 이용해 자연어를 생성한다는 것은 시퀀스를 만들어내는 타겟 모델을 학습하는 방법으로 Maximum Likelihood Estimation, MLE를 사용하지 않고 
강화학습의 학습 방법인 정책 경사 알고리즘 (Policy Gradient)를 사용해 학습하는 것을 말합니다.


이번 글에서는 이것이 어떤 의미를 가지게 되고 (MLE와 어떻게 다른지), 어디서부터 기인?했는지에 대해서 짧게 이야기 해보도록 하겠습니다.

### <mark style='background-color: #dcffe4'> Ganerative Adversarial Network (GAN) </mark>

`생성적 적대 신경망 (Generative Adversarial Network, GAN)` 이라는 모델은 Ian Goodfellow가 제안한 기법으로 과거 부터 제안되어온 머신 러닝의 생성 모델들 중 하나로,
변분 오토 인코더 (Variational Auto Encoder, VAE)와 함께 현대 딥 러닝 방법론들 중의 대표적인 생성 모델 중 하나 입니다.

생성 모델은 데이터가 샘플링 됐을 법한 실제 데이터 분포를 추정하는 방법으로 판별 모델(Discriminative Model)과는 약간 다른데, 이렇게 실제 훈련 데이터셋이 샘플링 된 데이터 분포를 추정하게 되면 (이정도 분포를 추정하려면 데이터가 정교하고, 학습하는 방법이 고도화 돼야 합니다. 물로 계산량도 많이 들고) 얻은 분포를 통해서 훈련 집합에는 존재하지 않는 샘플들을 뽑아낼 수 있습니다.


GAN은 이러한 생성 모델들의 어려운 점을 잘 해결하기 위해 디자인 된 모델이며, 이런식으로 데이터 분포에서 훈련 데이터셋을 더 뽑아 훈련하는데 사용하게 되면 모델의 퍼포먼스를 올릴 수 있게 됩니다.  


이러한 아이디어를 Sequence Genration 에도 적용해 생각해 보도록 하겠습니다. 
우리는 `seq2seq(G)`에서 생성한 문장과 실제 데이터 셋에서 나온 문장을 판별하는 `Disctiminator(D)`를 두어서 GAN과 같은 방법론으로 `seq2seq Generator(G)`를 학습할 수 있을 것입니다.


하지만 우리는 이러한 아이디어로 네트워크를 단박에 학습할 수 없는데, 그 이유는 seq2seq의 결과는 각 생성된 토큰들(마지막에 logit값들에 softmax를 적용한 character distribution 토큰)에 샘플링, 혹은 argmax operation을 적용한 이산적인(discrete) 토큰이기 때문입니다. 여기서 argmax operation을 취하는 것은 미분이 불가능한 연산이기 때문에 오차 역전파 알고리즘에 따라 네트워크를 학습할 수 없게 되는 것이죠.  


이처럼 GAN의 아이디어를 자연어 생성에 적용하는 것은 쉽지 않은데(미분 불가능한 문제를 해결 하기 위해 Gumbel Softmax를 사용한다던가, 후에 기술할 REINFORCE 등을 사용하면 가능 할 수 있음) 
여기에 강화학습을 적용하면 이야기는 달라질 수 있습니다.

## <mark style='background-color: #fff5b1'> Basic Reinforcement Learning </mark>

강화학습을 자연어 생성에 적용한다는 것은 우리가 더이상 자연어 생성 모델을 학습하는 데 생성 레이블과 정답 레이블간 `Cross Entropy Loss (CE)`를 사용해 
점점 모델이 정답 레이블을 뱉게 끔 학습하는 것이 아니라, 어떠한 `보상(reward)`을 정의하고 이를 극대화 하는 방법을 사용하겠다는 것입니다. 
우리가 이러한 보상을 잘 정의한다면 CE를 사용해 어떤 입력이 주어졌을때 (given x) 단순히 패턴을 인식해 정답 레이블을 뱉는 것 이상의 무언가를 해낼 수 있다는 믿음이 있고, 
이는 대표적으로 자연어 생성 모델에서 사용되는 평가 지표중 하나인 `BLEU Score` (얼마나 자연어를 그럴싸하게 생성했는가?)가 될 수도 있습니다.


우리는 모델에 강화학습을 적용하기 위해서 궁극적으로 정책 경사 (Policy Gradient) 방법을 사용할 것이지만 여기까지 도달하기 위해 몇가지 강화학습의 기초에 대해서 짚고 넘어가 보도록 하겠습니다.


강화학습의 기본적인 매커니즘은, 어떠한 `에이전트(Agent)`가 `정책(Policy)`에 따라서 `행동(Action)`을 하면 `환경(Environment)`는 에이전트가 한 행동에 따라 즉각적인 `보상(Reward)`을 리턴하게 되고, 이에 따라 새롭게 바뀐 에이전트가 있는 `상태(State)`가 변하게 되는 것 입니다.

<center>
$$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...  $$
</center>

여기서 특정 조건이 만족되면 환경은 위의 일련의 시퀀스를 종료하게 되는데 이를 하나의`에피소드(episode)`라고 합니다.

우리의 목적은 이렇게 반복되는 여러번의 에피소드에서 에이전트가 보상을 최대화 하기 위한 `정책(Policy)`을 찾는 것 입니다.

(마르코프 체인 과 마르코프 결정 과정(MDP)에 대한 자세한 설명은 생략하겠습니다.)


강화학습에서는 우리가 어떤 상태에서 어떤 행동을 하는지를 현재의 정책을 따르며 가장 얻는 보상을 극대화 하려고 합니다.
위의 일련의 에피소드에서 보상은 $$R_1,R_2,R_3,...$$ 등 여러번 받을 수 있지만 우리는 현재 상태를 $$S_0$$ 라고 할 때 과연 어떤 장단에 맞춰 (어떤 상태의 보상을 최대화 하는 행동을 해야할 지)를 잘 모르겠죠.
$$S_0$$ 일 때는 과연 $$R_1$$ 만을 최대화 하려고 행동하면 될까요? 아니죠. 우리는 장기적인 관점에서 얻을 수 있는 모든 보상을 최대화 하기 위해 행동해야 합니다.
비유하자면 고등학생의 상태인 에이전트가 인내심을 가지고 공부를 하는 것은 현재 상태의 reward를 별로 잘 가져갈 수 없겠지만 장기적으로 봤을 때는 자수성가를 하게되어 큰 reward를 얻을 수 있다는 거죠.

<center>
$$ G_t = R_{t+1} + R_{t+2} + R_{t+3} ... + R_{T}  $$
</center>

즉 우리가 최종적으로 받을 `누적 보상`($$G_t$$)을 최대화 하는 방향으로 행동을 하게 될 것이라는것인데, 여기서 미래의 보상보다는 그래도 가까운 보상을 추구하자는 의미를 내포하기 위해서 
`감소율 (discount factor)`을 곱해줍니다.

<center>
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ...  $$
</center>

자 이제 우리는 누적 보상을 최대화 하기 위해 각 상태마다 행동을 하면 되는데 이 행동은 각 선택할 수 있는 행동 옵션들에 대한 확률을 가지고 있는 아래의 `정책(Policy)`을 따르게 될 것이고

<center>
$$ \pi(a|s) = P(A_t=a|S_t=s) $$
</center>




### <mark style='background-color: #dcffe4'> Policy based Reinforcement Learning </mark>

### <mark style='background-color: #dcffe4'> MLE vs Policy Gradient </mark>

### <mark style='background-color: #dcffe4'> REINFORCE Algorithm </mark>






## <mark style='background-color: #fff5b1'> Natural Language Generation and Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Supervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Unsupervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> References </mark>

