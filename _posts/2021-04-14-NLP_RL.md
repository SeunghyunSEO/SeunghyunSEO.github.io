---
title: Natural Language Generation with Reinforcement Learning
categories: Reinforcement_Learning_and_Deep_Reinforcement_Learning
tag: [RL]

toc: true
toc_sticky: true
---

이 글은 `마키나락스`에서 자연어 처리를 연구중이신 `김기현`님의 `자연어 처리 딥러닝 캠프, 파이토치 편`의 12장을 읽고 임시로 정리한 글 입니다. 

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> Natural Language Generation with Reinforcement Learning </mark>

강화학습을 이용해 자연어를 생성한다는 것은 시퀀스를 만들어내는 타겟 모델을 학습하는 방법으로 Maximum Likelihood Estimation, MLE를 사용하지 않고 
강화학습의 학습 방법인 정책 경사 알고리즘 (Policy Gradient)를 사용해 학습하는 것을 말합니다.


이번 글에서는 이것이 어떤 의미를 가지게 되고 (MLE와 어떻게 다른지), 어디서부터 기인?했는지에 대해서 짧게 이야기 해보도록 하겠습니다.

### <mark style='background-color: #dcffe4'> Ganerative Adversarial Network (GAN) </mark>

`생성적 적대 신경망 (Generative Adversarial Network, GAN)` 이라는 모델은 Ian Goodfellow가 제안한 기법으로 과거 부터 제안되어온 머신 러닝의 생성 모델들 중 하나로,
변분 오토 인코더 (Variational Auto Encoder, VAE)와 함께 현대 딥 러닝 방법론들 중의 대표적인 생성 모델 중 하나 입니다.

생성 모델은 데이터가 샘플링 됐을 법한 실제 데이터 분포를 추정하는 방법으로 판별 모델(Discriminative Model)과는 약간 다른데, 이렇게 실제 훈련 데이터셋이 샘플링 된 데이터 분포를 추정하게 되면 (이정도 분포를 추정하려면 데이터가 정교하고, 학습하는 방법이 고도화 돼야 합니다. 물로 계산량도 많이 들고) 얻은 분포를 통해서 훈련 집합에는 존재하지 않는 샘플들을 뽑아낼 수 있습니다.


GAN은 이러한 생성 모델들의 어려운 점을 잘 해결하기 위해 디자인 된 모델이며, 이런식으로 데이터 분포에서 훈련 데이터셋을 더 뽑아 훈련하는데 사용하게 되면 모델의 퍼포먼스를 올릴 수 있게 됩니다.  


이러한 아이디어를 Sequence Genration 에도 적용해 생각해 보도록 하겠습니다. 
우리는 `seq2seq(G)`에서 생성한 문장과 실제 데이터 셋에서 나온 문장을 판별하는 `Disctiminator(D)`를 두어서 GAN과 같은 방법론으로 `seq2seq Generator(G)`를 학습할 수 있을 것입니다.


하지만 우리는 이러한 아이디어로 네트워크를 단박에 학습할 수 없는데, 그 이유는 seq2seq의 결과는 각 생성된 토큰들(마지막에 logit값들에 softmax를 적용한 character distribution 토큰)에 샘플링, 혹은 argmax operation을 적용한 이산적인(discrete) 토큰이기 때문입니다. 여기서 argmax operation을 취하는 것은 미분이 불가능한 연산이기 때문에 오차 역전파 알고리즘에 따라 네트워크를 학습할 수 없게 되는 것이죠.  


이처럼 GAN의 아이디어를 자연어 생성에 적용하는 것은 쉽지 않은데(미분 불가능한 문제를 해결 하기 위해 Gumbel Softmax를 사용한다던가, 후에 기술할 REINFORCE 등을 사용하면 가능 할 수 있음) 
여기에 강화학습을 적용하면 이야기는 달라질 수 있습니다.

## <mark style='background-color: #fff5b1'> Basic Reinforcement Learning </mark>

강화학습을 자연어 생성에 적용한다는 것은 우리가 더이상 자연어 생성 모델을 학습하는 데 생성 레이블과 정답 레이블간 `Cross Entropy Loss (CE)`를 사용해 
점점 모델이 정답 레이블을 뱉게 끔 학습하는 것이 아니라, 어떠한 `보상(reward)`을 정의하고 이를 극대화 하는 방법을 사용하겠다는 것입니다. 
우리가 이러한 보상을 잘 정의한다면 CE를 사용해 어떤 입력이 주어졌을때 (given x) 단순히 패턴을 인식해 정답 레이블을 뱉는 것 이상의 무언가를 해낼 수 있다는 믿음이 있고, 
이는 대표적으로 자연어 생성 모델에서 사용되는 평가 지표중 하나인 `BLEU Score` (얼마나 자연어를 그럴싸하게 생성했는가?)가 될 수도 있습니다.


우리는 모델에 강화학습을 적용하기 위해서 궁극적으로 정책 경사 (Policy Gradient) 방법을 사용할 것이지만 여기까지 도달하기 위해 몇가지 강화학습의 기초에 대해서 짚고 넘어가 보도록 하겠습니다.


강화학습의 기본적인 매커니즘은, 어떠한 `에이전트(Agent)`가 `정책(Policy)`에 따라서 `행동(Action)`을 하면 `환경(Environment)`는 에이전트가 한 행동에 따라 즉각적인 `보상(Reward)`을 리턴하게 되고, 이에 따라 새롭게 바뀐 에이전트가 있는 `상태(State)`가 변하게 되는 것 입니다.

<center>
$$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...  $$
</center>

여기서 특정 조건이 만족되면 환경은 위의 일련의 시퀀스를 종료하게 되는데 이를 하나의`에피소드(episode)`라고 합니다.

우리의 목적은 이렇게 반복되는 여러번의 에피소드에서 에이전트가 보상을 최대화 하기 위한 `정책(Policy)`을 찾는 것 입니다.

(마르코프 체인 과 마르코프 결정 과정(MDP)에 대한 자세한 설명은 생략하겠습니다.)


강화학습에서는 우리가 어떤 상태에서 어떤 행동을 하는지를 현재의 정책을 따르며 가장 얻는 보상을 극대화 하려고 합니다.
위의 일련의 에피소드에서 보상은 $$R_1,R_2,R_3,...$$ 등 여러번 받을 수 있지만 우리는 현재 상태를 $$S_0$$ 라고 할 때 과연 어떤 장단에 맞춰 (어떤 상태의 보상을 최대화 하는 행동을 해야할 지)를 잘 모르겠죠.
$$S_0$$ 일 때는 과연 $$R_1$$ 만을 최대화 하려고 행동하면 될까요? 아니죠. 우리는 장기적인 관점에서 얻을 수 있는 모든 보상을 최대화 하기 위해 행동해야 합니다.
비유하자면 고등학생의 상태인 에이전트가 인내심을 가지고 공부를 하는 것은 현재 상태의 reward를 별로 잘 가져갈 수 없겠지만 장기적으로 봤을 때는 자수성가를 하게되어 큰 reward를 얻을 수 있다는 거죠.

<center>
$$ G_t = R_{t+1} + R_{t+2} + R_{t+3} ... + R_{T}  $$
</center>

즉 우리가 최종적으로 받을 `누적 보상`($$G_t$$)을 최대화 하는 방향으로 행동을 하게 될 것이라는것인데, 여기서 미래의 보상보다는 그래도 가까운 보상을 추구하자는 의미를 내포하기 위해서 
`감소율 (discount factor)`을 곱해줍니다.

<center>
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ...  $$
</center>

자 이제 우리는 누적 보상을 최대화 하기 위해 각 상태마다 행동을 하면 되는데 이 행동은 각 선택할 수 있는 행동 옵션들에 대한 확률을 가지고 있는 아래의 `정책(Policy)`을 따르게 될 것이고

<center>
$$ \pi(a|s) = P(A_t=a|S_t=s) $$
</center>

어떤 상태에서 '어떤 행동을 해야 내가 얻을 보상이 최대가 될까?' 를 결정하는 `좋은 정책`을 얻어내는게 앞서 말했듯 강화학습의 최종 목표입니다. 

이 목표를 달성하기 위해서 일반적으로 강화학습에서는 '현재 상태의 가치가 얼마나 좋은가?' 를 의미하는 `가치 함수(Value Function)` 혹은 
`행동-가치 함수(Action-Value Function)`을 따로 정의해서 이를 최적화 하던가 아니면 `정책 함수(Policy Function)` 그 자체를 아예 최적화 하는 방법을 사용하곤 합니다.

### <mark style='background-color: #dcffe4'> Value-based Methods </mark>

앞서 말한`가치 함수(Value Function)`은 현재의 policy $$\pi$$ 아래에서 특정 상태(state) $$s$$에서부터 앞으로 얻을 수 있는 보상의 누적 총합의 기대값을 나타내는 함수입니다.

<center>
$$ v_{\pi}(s) = \mathbb{E}_{\pi} [G_t | S_t = s] = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t = s], \vee s \in S $$
</center>

위의 수식은 가치 함수의 수식으로 현재 주어진 시점, $$t$$ 와 상태, $$s$$에서 부터 얻을 수 있는 모든 보상들에 대한 기대값을 나타냅니다.

그렇다면 `행동-가치 함수(Action-Value Function)`의 수식은 어떻게 될 까요?

<center>
$$ q_{\pi}(s,a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t = s, A_t = a] $$
</center>

가치 함수에서 상태 a를 추가하면 그게 곧 `행동-가치 함수(Action-Value Function)` 혹은 `큐 함수(Q-Function)`이 됩니다.

가치 함수와 행동-가치함수의 차이는 뭘까요?

가치 함수가 어떠한 주어진 상황($$s$$)에서 어떤 행동을 선택할지와 관계 없이 얻을 수 있는 누적 보상의 기대값이라고 한다면, 
행동-가치 함수는 어떤 상태($$s$$) 에서 어떤 행동($$a$$)을 선택하는 것이 최선의 선택인지에 대한 개념이 추가됐을 때의 누적 보상 기대값을 나타내는 것이라는 것입니다. 


우리는 앞서 정의한 가치 함수(V-Function, Q-Function)들을 모두 어떠한 형태로 나타낼 수 있는데, 

<center>
$$ v_{\ast}(s) = max_a \mathbb{E} [R_{t+1} + \gamma v_{\ast}(S_{t+1}) | S_t=s, A_t = a] $$
$$ = max_a \sum_{s',r} P(s',r | s,a) [ r+\gamma v_{\ast}(s') ] $$
</center>

<center>
$$ q_{\ast}(s,a) = max_a \mathbb{E} [R_{t+1} + \gamma max_{a'} q_{\ast}(S_{t+1},a') | S_t=s, A_t = a] $$
$$ = \sum_{s',r} P(s',r | s,a) [ r+\gamma max_a q_{\ast}(s',a') ] $$
</center>

이를 우리는 `벨만 방정식(Bellman equation)`의 형태로 나타냈다고 표현합니다.

벨만 방정식의 형태로 표현이 가능하다는 것은 다시 말해서 `동적 프로그래밍 (Dynamic Programming)` 알고리즘 문제로 접근하여 계산을 더 효율적이고 빠르게 할 수 있다는 뜻입니다.


하지만 이렇게 까지 수식을 정리하는 것 만으로도 강화학습을 하기엔 충분하지 않은데요. 이는 앞선 수식의 $$P(s',r|s,a)$$ 부분을 우리가 대부분의 경우 모르기 때문입니다.
즉 우리가 어떤 상태에서 어떤 액션을 취할 때 얻을 확률과 그 다음상태가 어떻게 될지를 알 수 없다는건데요, 이는 게임의 경우를 생각해보면 우리가 시스템에 대한 전체 정보를 모르기 때문에 몬스터를 공격했을때 어떤 보상을 얻게 될 지, 공격하면 미스가뜰지 데미지가 들어갈지 등의 확률을 자세히 모른다는 겁니다. 


즉 우리는 이러한 상태 변이 확률, 또는 `상태 천이 확률 (State Transition Probability)`를 알 수 없기 때문에 이를 직접 해봐야 한 다는 겁니다.

다시 한 번 말하자면, 이는 우리가 주사위가 육면체의 형태이기 떄문에 모든 면이 1/6확률로 나온다는 사실을 접어둔다면, 주사위가 어떻게 생겼는지 모르기 때문에 일일히 직접 여러번 주사위를 던져서 '숫자 1이 나올 확률이 몇이며 ... 숫자 6이 나올 확률이 나올 확률이 몇이다'를 알아내야 한다는 것입니다.

즉 시뮬레이션을 통해서 몇 수를 진행해 보고 이를 토대로 에이전트를 학습해야 한다는 것이죠.

<center>
$$ V(S_t) \leftarrow V(S_t) + \alpha[G_t-V(S_t)] $$
</center>


### <mark style='background-color: #dcffe4'> Monte Carlo Method </mark>

몬테카를로 방법을 이용해 위의 벨만 기대 방정식 (Bellman Expectation Equation)을 해결할 수 있는데, 이는 하나의 에피소드(한 게임)가 끝날 때 까지 기다렸다가 $$G_t$$를 얻은 뒤 이를 사용하는 것입니다.


### <mark style='background-color: #dcffe4'> Temporal Difference Learning </mark>

시간 차 학습 (Temporal Difference, TD Learning) 방법은 에피소드가 끝날 때 까지 기다렸다가 가치 함수를 업데이트 하는 앞선 방법이 너무 비효율적이라고 생각하여 (왜냐하면 에피소드가 굉장히 오래 걸릴 경우 좋지 않은 가치 함수를 가지고 계속 에피소드를 진행해 나가야 하기 떄문) 제안된 방법으로 아래의 식을 이용하여 학습합니다.

<center>
$$ V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] $$
</center>

만약 우리가 올바른 행동 가치 함수를 알고 있다면, 어떤 상황에서도 (어떤 상태에서도) 항상 기대 누적 보상을 최대화 하는 방향으로 선택을 해 나가 결국 얻을 수 있는 최고의 보상을 얻게 될 겁니다.
이 때 행동 가치 함수를 잘 학습하는 것을 `큐 러닝(Q-Learning)` 이라고 하며 위의 수식에 `행동(Action)`을 추가한 수식

<center>
$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma max_a Q(S_{t+1},a) - Q(S_t,A_t)] $$
</center>

을 사용하게 됩니다.

여기서 우변의 $$\alpha$$ 뒤의 수식 중 첫째 항인 ($$R_{t+1} + \gamma max_a Q(S_{t+1},a)$$)는 타겟 가치 함수(Target Value Function) 이 되고, 두번째 항인 ($$Q(S_t,A_t)$$)는 현재의 가치 함수(Current Value Function) 이 되어 이 두 가치 함수의 차이를 줄이면서 학습하게 되면 우리는 결국 올바른 가치 함수를 얻게 된다는 것이 바로 큐러닝 입니다.


하지만 우리가 강화학습을 적용하고 싶은 분야는 대게 미로찾기처럼 간단한 상태(State)의 크기와 행동(Action)의 크기를 가지는 것이 아니라, 무수히 많은 크기를 가지는 상태와 행동을 가지고 있을 겁니다.

이럴 때는 훈련 과정에서 희소성(Sparsity)이 너무 크기 때문에 학습을 하기 어려운데, 이를 해결 하기 위해서 무수히 많은 상태와 행동으로 이루어진 공간을 작은 공간으로 근사 하여(mapping 하여) 학습 하는데 사용하는데 이를 `딥-큐 러닝 (Deep Q-Learning)` 이라고 합니다. 


```
책에서는 비가 오는 상황에서 짬뽕을 시킬것이냐 짜장면을 시킬 것이냐의 행동을 고르는 상황은, 비가 5mm 내리거나 10mm내리거나 100mm내리거나 비슷한 상황으로 볼 수 있기때매 이렇게 무수히 많은 상태의 경우를 줄이는 것을 딥러닝이 해낸다고 표현하고 있습니다.
```

즉 우리는 아래의 수식에서 좌변의 첫번째 항인 $$Q(S_t,A_t)$$ 를 신경망으로 근사하겠다는 것입니다.

<center>
$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma max_a Q(S_{t+1},a) - Q(S_t,A_t)] $$
</center>

### <mark style='background-color: #dcffe4'> Policy-based Methods </mark>

앞선 설명이 길었지만 우리가 눈여겨 봐야 할 것은 정책 기반의 강화학습 알고리즘 입니다. 

(여담이지만, 어떤 회사는 가치 기반 알고리즘을 주로 사용하고, 어떤 회사는 정책 기반 알고리즘을 미는 것 같습니다. 즉 어떤 게 더 우월하다는 데 있어 정답은 없는 것 같다는 것이 저의 개인적인 의견입니다.) 


`정책 경사(Policy Gradient)` 알고리즘은 앞서 이야기한 `딥-큐 러닝`과 다릅니다.


두 방식을 사용할 때의 가장 큰 차이점은, 가치 기반 학습 방식은 뉴럴 네트워크를 통해서 어떤 행동(Action)을 골랐을 때 얻을 수 있는 보상을 예측하도록 훈련하여 이 과정속에서 올바른 정책 구하는 것이나 다름 없게 학습하는? 것이고, 정책 기반 학습 방식은 뉴럴 네트워크의 행동에 대한 보상을 역전파 알고리즘을 통해 전달하여 바로 `정책(Policy)`을 학습한다는 것입니다.

즉, `딥-큐 러닝`의 경우에는 행동의 선택이 확률적(Stochastic)으로 나오지 않는 것에 비해, 정책 경사 알고리즘들은 행동을 선택할 때 확률적인 과정을 거친다는 겁니다.

정책 경사 알고리즘의 수식은 아래와 같습니다.

<center>
$$ \pi_{\theta}(a|s) = P_{\theta}(a|s) = P(a|s;\theta) $$
</center>

우리의 목표는 기대 누적 보상을 최대로 하는 정책 $$\theta$$로 파라메터화 된 $$\pi$$를 찾는 것입니다.

<center>
$$ J(\theta) = \mathbb{E}_{\pi_{\theta}}[r] = v_{\theta}(s_0) $$
$$ = \sum_{s \in S} d(s) \sum_{a \in A} \pi_{\theta} (s,a) R_{s,a} $$
</center>

즉 위의 최초 상태(initial state)인 $$\theta$$를 $$J$$를 최대화 해야 하게 끔 업데이트 해 나가야 한다는 것인데(딥러닝에서 손실을 줄이는 방식으로 파라메터를 업데이트 하듯), 
우리는 딥러닝에서 처럼 손실을 정의하고 이를 최소화 하는것이 아니라, 기대 보상을 정의하고 이를 최대화 하는 것이 목적이기 떄문에 `경사 상승법(gradient ascent)` 이라는 방법을 사용해서 파라메터를 최적화 하게 됩니다.

<center>
$$ \theta_{t+1} = \theta_t + \gamma \bigtriangledown_{\theta} J(\theta) $$
</center>

여기서 $$d(s)$$는 마르코프 연쇄(Markov Chain)의 정적 분포(Stationary Distribution)으로써 시작점에 상관 없이 전체 경로에서 어떤 상태 $$s$$에 머무르는 시간의 비율을 의미합니다.

자 이제 경사 상승법을 위해서 $$J$$식을 한 번 미분해보도록 하겠습니다.

우선 정책을 미분한 식은 간단히 아래와 같이 나타낼 수 있고

<center>
$$ \bigtriangledown_{\theta} \pi_{\theta}(s,a) = \pi_{\theta}(s,a) \frac{ \bigtriangledown_{\theta} \pi_{\theta}(s,a) }{ \pi_{\theta}(s,a) }  $$
$$ = \pi_{\theta}(s,a) \bigtriangledown_{\theta} log \pi_{\theta} (s,a) $$
</center>

이를 이용해서 누적 기대 보상 $$J$$의 편미분 식을 구하면 아래와 같이 됩니다.

<center>
$$ \bigtriangledown_{\theta} J(\theta) = \sum_{s \in S} d(s) \sum_{a \in A} \bigtriangledown_{\theta} \pi_{\theta} (s,a) R_{s,a} $$
$$ = \sum_{s \in S} d(s) \sum_{a \in A} \pi_{\theta} (s,a) \bigtriangledown_{\theta} log \pi_{\theta} (s,a) R_{s,a} $$
$$ = \mathbb{E}_{\pi_{\theta}} [ \bigtriangledown_{\theta} log \pi_{\theta} (a|s) r ] $$
</center>

이 수식의 의미는 매 time-step별 상황 $$s$$가 주어졌을 때 $$a$$ 를 선택할 로그 확률의 기울기와 그에 따른 보상을 곱한 값의 기대값을 나타냅니다.

여기서 해당 time-step에 대한 `즉각적인 보상, r` 대신에 에피소드 종료까지의 `누적 보상`을 사용하게되면 식은 아래와 같아집니다.

<center>
$$ \bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}} [ \bigtriangledown_{\theta} log \pi_{\theta} (a|s) Q^{\pi_{\theta}} (s,a) ] $$
</center>

즉 큐 함수를 사용해 다시 식을 나타낼 수 있다는 것이죠. 하지만 우리는 정책 경사 방법의 신경망에 대해서는 미분을 계산해야 하지만, 큐 함수에 대해서는 미분을 계산할 필요가 없는데요, 
이는 즉, 미분의 가능 여부를 떠나서 임의의 어떠한 함수라도 보상 함수로 사용 해 최적의 정책을 찾는 방향으로 업데이트를 할 수 있게 된다는 것입니다.


이제 우리는 CE Loss나 MSE Loss를 사용하지 않고 자연어 생성 모델의`BLEU`같은 네트워크의 실제 평가 `metric`이나 음성 인식의 `WER`등을 이용해서도 파라메터를 학습할 수 있게 된 것이죠.


마지막으로 우리가 위의 수식에서 기대값을 몬테카를로 샘플링으로 대체하게 된다면 우리는 아래의 수식을 얻을 수 있습니다.


<center>
$$ \theta \leftarrow \theta + \gamma \bigtriangledown_{\theta} log \pi_{\theta} (a|s) Q^{\pi_{\theta}} (s,a) $$
</center>

이 수식에서 $$log \pi_{\theta} (a \vert s)$$가 의미하는 것은 어떤 상태 $$s_t$$가 주어졌을 때, 정책 파라메터 $$\theta$$ 상에서의 확률 분포에서 샘플링되어 선택된 행동이 $$a_t$$일 확률 값입니다.
따라서 위의 경사 상승법은 $$log \pi_{\theta} (a \vert s)$$를 최대화 하는 것으로, 높은 보상을 리턴하는 $$a_t$$의 확률을 높이도록 함으로써(즉 해봤더니 반응이 더 좋은 행동을 더 선택하게 확률을 높히게 끔 파라메터를 업데이트 함) 학습을 하는 것입니다.


여기서 이 로그 확률값 뒤에 $$r$$이 곱해졌다는 것의 의미는 만약 이 값이 양수라면, 그것도 큰 값이라면 샘플링된 해당 행동들이 큰 보상을 받았으므로 더욱 그 행동을 독려하는 것이고(더 큰스텝으로), 아니라면 적당히,
반대로 마이너스라면 경사의 반대방향으로 스텝을 갖도록 방향을 바꿀 것입니다(했던 행동을 앞으로는 잘 하지 못하도록).


정책 경사 알고리즘은 하지만 기존의 경사도의 방향에 크기 값을 곱해주므로 실제 보상을 최대화하는 직접적인 방향을 지정해 줄 수는 없기 때문에 보상을 최대화하는 최적의 방향을 스스로 찾아갈 수 없으므로, 사실상 훈련이 어렵고 비효율적이라는 단점이 있습니다.

### <mark style='background-color: #dcffe4'> MLE vs Policy Gradient </mark>

이제 우리가 일반적으로 딥러닝에서 사용하는 최대 가능도 추정 (Maximum Likelihood Estimation, MLE)과 정책 경사 (Policy Gradient) 알고리즘을 비교해보도록 하겠습니다.

<center>
$$ B = \{ (x_{1:n}^i, y_{1:m}^i) \}_{i=1}^N $$
x_{1:n} = \{ x_1,x_2, ..., x_n \}
y_{1:m} = \{ y_1,y_2, ..., y_m \}
</center>

우리가 어떤 길이 n의 시퀀스 입력을 받아 길이 m의 타겟 시퀀스를 만들어내는 함수를 찾고 싶다고 가정해보도록 하겠습니다.
우리의 목표는 $$f : x \rightarrow y$$ 를 근사하는 네트워크 파라메터 $$\theta$$를 최적화를 통해 찾아내는 거죠.

<center>
$$ \hat{y_{1:m}} = argmax_{y \in Y} P(y|x_{1:n},\theta) $$
</center>

그러면 우리는 입력 시퀀스 $$x$$와 파라메터 $$\theta$$와의 연산을 통해서 가장 그럴듯한 출력값 $$y$$를 리턴할 수 있습니다.
여기서 파라메터는 가능도 함수를 최대화 하는, 즉 음의 로그 가능도를 최소화 하는 파라메터로 아래의 수식(MLE)를 통해 얻어냅니다. 

<center>
$$ \hat{\theta} = argmax_{\theta \in \Theta} P(y|x,\theta) $$
</center>

n개의 토큰들로 이루어진 입력 시퀀스를 받아 m개의 토큰으로 이루어진 출력 시퀀스를 예측하는 태스크에서 우리는 일반적으로 아래의 손실함수를 사용합니다.

![seq2seq](/assets/images/NLP_RL/seq2seq.png)
*Fig. Seq2Seq Model for Neural Machine Translation. 일반적으로 기계 번역 태스크에 사용되는 네트워크의 모식도*
(이미지 출처 : [Neural Abstractive Text Summarization with Sequence-to-Sequence Models](https://arxiv.org/pdf/1812.02303))

<center>
$$ J(\theta) = - \mathbb{E}_{x\sim P(x)}[\mathbb{E}_{y\sim P(y)}[logP(y|x;\theta)]]$$
$$ \approx - \frac{1}{N} \sum_{i=1}^{N} \sum_{y_{1:m} \in Y} P(y_{1:m}|x_{1:n}^i) logP(y_{1:m} | x_{1:n}^{i};\theta) $$
$$ = - \frac{1}{N} \sum_{i=1}^{N} logP(y_{1:m}^i | x_{1:n}^{i};\theta) $$
$$ = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{m} logP(y_{t}^i | x_{1:n}^{i}, y_{<t}^{i};\theta) $$
</center>
  
이 수식을 위의 그림과 연결시켜 다시 생각하면 아래와 같습니다. 일반적으로 Sequence-to-Sequence(Seq2Seq) 모델을 이용해 기계 번역 문제를 풀 때 토큰별로 Cross Entropy Loss 블 계산해서 전부 더한 것을 Total Loss로 생각하고 이를 사용해 최적화 하기 때문이죠. (Loss 최소화 = Liklihood 최대화)

![seq2seq_1](/assets/images/NLP_RL/seq2seq_1.png)

![seq2seq_2](/assets/images/NLP_RL/seq2seq_2.png)
*Fig. Seq2Seq 모델을 사용해서 타겟 토큰들을 AutoRegressive하게 만들어내는 과정, 최종 Loss는 각 토큰들을 정답 토큰들과 1:1비교해서 발생된 loss들의 합.*

이러한 Seq2Seq 모델은 Attention Mechanism Module을 더해 사용하기도 합니다. 
  
![seq2seq_attention](/assets/images/NLP_RL/seq2seq_attention.png)
*Fig. Seq2Seq Model with attention mechanism*


이제 우리는 아까 정의한 Loss를 미분하여 파라메터를 업데이트 하는 이른바 `경사 하강법 (gradient descent)`을 사용해 파라메터 최적화를 하게 됩니다.


<center>
$$ \theta \leftarrow \theta - \gamma \bigtriangledown_{\theta} J(\theta) $$
$$ \theta \leftarrow \theta + \gamma \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{m} \bigtriangledown_{\theta} logP(y_{t}^i | x_{1:n}^{i}, y_{<t}^{i};\theta) $$
</center>
  
아래는 Policy Gradient의 `경사 상승법 (gradient ascent)` 수식인데,

<center>
$$ \theta \leftarrow \theta + \gamma \bigtriangledown J(\theta) $$
$$ \theta \leftarrow \theta + \gamma Q^{\pi_{\theta}} (s_t,a_t) \bigtriangledown_{\theta} log \pi_{\theta}(a_t|s_t) $$
</center>

이 둘이 크게 다르지 않은것을 볼 수 있습니다. `log probability`를 미분하여 파라메터를 움직이는 유사한 방법인것이죠.

여기서 Policy Gradient의 수식을 조금 더 보완해 보도록 하겠습니다.

<center>
$$ \theta \leftarrow \theta + \gamma (G_t-b(S_t))Q^{\pi_{\theta}} (s_t,a_t) \bigtriangledown_{\theta} log \pi_{\theta}(a_t|s_t) $$
</center>

원래의 수식에서 $$(G_t-b(S_t))$$가 추가되었는데요, 이것이 의미하는 바는 다음과 같습니다. 예를 들어, 우리가 어떤 행동에 대해서 항상 양수를 리턴받는다고 할 때, 우리는 100점을 받는 상황이나 10000점을 받는 상황이나 모두 보상이 양수이기 때문에, 해당 행동을 독려한느 방향으로 파라메터가 업데이트 될 것이지만, 이는 사실 바람직하지 않습니다. 어떤 행동에 대한 보상의 평균이 5000일 경우, 100점은 잘 한 행동일까요? 아니죠, 그래서 그런것에 대한 보정을 해 준 것입니다.


## <mark style='background-color: #fff5b1'> Natural Language Generation and Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Supervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Unsupervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> References </mark>

