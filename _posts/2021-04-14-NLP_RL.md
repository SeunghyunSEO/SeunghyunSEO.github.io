---
title: Natural Language Generation with Reinforcement Learning
categories: Reinforcement_Learning_and_Deep_Reinforcement_Learning
tag: [RL]

toc: true
toc_sticky: true
---

이 글은 마키나락스에서 자연어 처리를 연구중이신 `김기현`님의 `자연어 처리 딥러닝 캠프, 파이토치 편`의 12장을 읽고 임시로 정리한 글 입니다. 

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> Natural Language Generation with Reinforcement Learning </mark>

강화학습을 이용해 자연어를 생성한다는 것은 시퀀스를 만들어내는 타겟 모델을 학습하는 방법으로 Maximum Likelihood Estimation, MLE를 사용하지 않고 
강화학습의 학습 방법인 정책 경사 알고리즘 (Policy Gradient)를 사용해 학습하는 것을 말합니다.


이번 글에서는 이것이 어떤 의미를 가지게 되고 (MLE와 어떻게 다른지), 어디서부터 기인?했는지에 대해서 짧게 이야기 해보도록 하겠습니다.

### <mark style='background-color: #dcffe4'> Ganerative Adversarial Network (GAN) </mark>

생성적 적대 신경망 (Generative Adversarial Network, GAN) 이라는 모델은 Ian Goodfellow가 제안한 기법으로 과거 부터 제안되어온 머신 러닝의 생성 모델들 중 하나로,
변분 오토 인코더 (Variational Auto Encoder, VAE)와 함께 현대 딥 러닝 방법론들 중의 대표적인 생성 모델 중 하나 입니다.

생성 모델은 데이터가 샘플링 됐을 법한 실제 데이터 분포를 추정하는 방법으로 판별 모델(Discriminative Model)과는 약간 다른데, 이렇게 실제 훈련 데이터셋이 샘플링 된 데이터 분포를 추정하게 되면 (이정도 분포를 추정하려면 데이터가 정교하고, 학습하는 방법이 고도화 돼야 합니다. 물로 계산량도 많이 들고) 얻은 분포를 통해서 훈련 집합에는 존재하지 않는 샘플들을 뽑아낼 수 있습니다.


GAN은 이러한 생성 모델들의 어려운 점을 잘 해결하기 위해 디자인 된 모델이며, 이런식으로 데이터 분포에서 훈련 데이터셋을 더 뽑아 훈련하는데 사용하게 되면 모델의 퍼포먼스를 올릴 수 있게 됩니다.  




## <mark style='background-color: #fff5b1'> Policy based Reinforcement Learning </mark>

### <mark style='background-color: #dcffe4'> MLE vs Policy Gradient </mark>

### <mark style='background-color: #dcffe4'> REINFORCE Algorithm </mark>






## <mark style='background-color: #fff5b1'> Natural Language Generation and Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Supervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Unsupervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> References </mark>

