---
title: Natural Language Generation with Reinforcement Learning
categories: Reinforcement_Learning_and_Deep_Reinforcement_Learning
tag: [RL]

toc: true
toc_sticky: true
---

이 글은 마키나락스에서 자연어 처리를 연구중이신 `김기현`님의 `자연어 처리 딥러닝 캠프, 파이토치 편`의 12장을 읽고 임시로 정리한 글 입니다. 

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> Natural Language Generation with Reinforcement Learning </mark>

강화학습을 이용해 자연어를 생성한다는 것은 시퀀스를 만들어내는 타겟 모델을 학습하는 방법으로 Maximum Likelihood Estimation, MLE를 사용하지 않고 
강화학습의 학습 방법인 정책 경사 알고리즘 (Policy Gradient)를 사용해 학습하는 것을 말합니다.


이번 글에서는 이것이 어떤 의미를 가지게 되고 (MLE와 어떻게 다른지), 어디서부터 기인?했는지에 대해서 짧게 이야기 해보도록 하겠습니다.

### <mark style='background-color: #dcffe4'> Ganerative Adversarial Network (GAN) </mark>

생성적 적대 신경망 (Generative Adversarial Network, GAN) 이라는 모델은 Ian Goodfellow가 제안한 기법으로 과거 부터 제안되어온 머신 러닝의 생성 모델들 중 하나로,
변분 오토 인코더 (Variational Auto Encoder, VAE)와 함께 현대 딥 러닝 방법론들 중의 대표적인 생성 모델 중 하나 입니다.

생성 모델은 데이터가 샘플링 됐을 법한 실제 데이터 분포를 추정하는 방법으로 판별 모델(Discriminative Model)과는 약간 다릅니다.






## <mark style='background-color: #fff5b1'> Policy based Reinforcement Learning </mark>

### <mark style='background-color: #dcffe4'> MLE vs Policy Gradient </mark>

### <mark style='background-color: #dcffe4'> REINFORCE Algorithm </mark>






## <mark style='background-color: #fff5b1'> Natural Language Generation and Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Supervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> Unsupervised Learning using Reinforcement Learning </mark>





## <mark style='background-color: #fff5b1'> References </mark>

