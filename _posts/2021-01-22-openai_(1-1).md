---
title: Introduction to RL (1) - Key Concepts in RL
categories: OpenAI_Spinning_UP
tag: [RL]

toc: true
toc_sticky: true
---

앞으로 제가 이해한 바 대로 원문을 최대한 짧고 직관적이게 번역할 겁니다 :)

Dive into Deep RL!

앞서 말했듯 앞으로 다루게 될 내용은 

* notation
* RL 알고리즘이 뭘 하는지에 대한 high-level explanation 
* 그리고 수학 쪼~금

간단하게 말해서, 강화학습은 trial and error를 통해 에이전트가 최종적으로 더 많은 보상을 얻을 수 있도록(당장은 이 행동이 안좋아보여도 결국에는 좋으면 됨, 당장 우리가 공부하기 싫어도 공부하는게 결국 나중에 좋은 보상이 되는 것 처럼) 학습하게 하는것에 대한 연구입니다. <br>

심플하게 말해서 좋았던(혹은 미래에 좋은 영향을 주는) 행동들은 더 자주 반복할 수 있게 (more likely to repeat) 아닌건 아니게끔 학습하는 거죠.  <br>

- <mark style='background-color: #fff5b1'> What Can RL Do? </mark>

그럼 말그대로 RL로 뭘 할 수 있을까?


컴퓨터에게 시뮬레이션 환경에서 로봇을 컨트롤 하게 할 수 있겠고

<video autoplay="" src="https://d4mucfpksywv.cloudfront.net/openai-baselines-ppo/knocked-over-stand-up.mp4" loop="" controls="" style="display: block; margin-left: auto; margin-right: auto; margin-bottom:1.5em; width: 100%; max-width: 720px; max-height: 80vh;">
</video>

리얼 월드에서 큐브를 맞춘다던가 하는 일도 할 수 있습니다.

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="https://www.youtube.com/embed/jwSbzNHGflM?ecver=1" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div>
<br />

그리고 굉장히 복잡한 바둑이나 ([Alphago](https://deepmind.com/research/alphago/)) 도타2 (한국에서 망한 롤의 원조인 5:5 AOS 게임... [Dota2](https://blog.openai.com/openai-five/)) 같은 게임에서 우수한 성과를 거둔것은 (이세돌을 이기거나, 프로게이머 상대로 5:5를 이김) 굉장히 유명한 일화입니다. ( 그 외에도 아타리 게임을 한다던가 등등 )
   

- <mark style='background-color: #fff5b1'> Key Concepts and Terminology </mark>

![image](https://user-images.githubusercontent.com/48202736/105507233-4ac0b280-5d0e-11eb-82c3-a7716cb479a4.png)

<center> Agent-environment interaction loop. </center>


RL에서 가장 주된 요소 두 개는 Agent 와 Environment 입니다 ( 게임으로 치면 게임환경과 내 캐릭터가 각각 환경과 에이전트일 것이고, 바둑으로 치면 바둑판이 환경이고 바둑기사가 Agent일 것입니다). <br>

Envirionment는 Agent가 살고있는 곳입니다. Agent는 observation of the state of the world를 보고 (바둑으로 치면 매 턴마다의 바둑 판 상태를 보는 것) 그 상황에 맞는 액션을 취하게 됩니다. ( 한 수를 두게 되는 것). <br>

이제 환경은 Agent가 한 행동에 따라 상호작용을 통해서 변하게 되는데 ( 한 수 뒀으니 당연히 바뀜 ), 이것 말고도 환경 그 자체로 스스로 변하기도 합니다. (바둑판에서는 그럴 일이 없겠지만 게임같은 환경이나 실제 환경에서 바람이 분다던가 하는 자체적인 변화를 말하는 것 같습니다. ) <br>


 - <mark style='background-color: #dcffe4'> states and observations </mark>
 
 - <mark style='background-color: #dcffe4'> action spaces </mark>
 
 - <mark style='background-color: #dcffe4'> policies </mark>

 - <mark style='background-color: #dcffe4'> trajectories </mark>
 
 - <mark style='background-color: #dcffe4'> different formulations of return </mark>
 
 - <mark style='background-color: #dcffe4'> the RL optimization problem </mark>
 
 - <mark style='background-color: #dcffe4'> value functions problem </mark>

- <mark style='background-color: #fff5b1'> (Optional) Formalism </mark>

