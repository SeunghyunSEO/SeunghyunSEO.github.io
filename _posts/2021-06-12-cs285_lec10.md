---
title: (yet) Lecture 10 - Optimal Control and Planning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 10의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=4SL0DnxC1GM&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=42)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-10.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

여태까지 cs285에서 배웠던 `Deep RL`의 흐름을 되짚어 보도록 합시다.

- Lec 2~4 : Deep Learning과 Deep RL은 어떻게 다른가? (behavior cloning)
- Lec 5~6 : `Policy (Actor) Gradient` 방법론은 무엇인가? 그리고 이 알고리즘을 강화하기 위한 (unbias하고 low-variance하게 만들기 위해서) 트릭들 + 현재 정책을 평가하는 가치기반 함수 (Q,V,A) 들을 추가해서 더 정교하게 만들 순 없을까? 즉, Critic을 추가한다! -> `Actor-Critic`
- Lec 7~8 : Policy를 직접적으로 배우지 않고 간접적으로 (implicitly) 배울 수는 없을까? -> `Value-based Algorithm` 우리가 환경에 대해 다 알고 (Transition Dynamics) state, action을 표로 간단하게 표현 가능할 경우 optimal policy를 `Dynamic Programming`을 통해서 구할 수 있다 -> Policy Iteration, Value Iteration -> 말도안되는 state는 tabular form으로 불가능하다 -> `Fitted Policy Iteration`, `Fitted Value Iteration` -> 근데 Transition Dynamics를 모른다면? -> Q-Iteration, `Fitted Q-Iteration`, `Q-Learning`, 특히 Q-Learning은 딥마인드가 2013년 아타리 문제를 푼다던가 하는 데 사용된 굉장히 general하면서 잘 작동하는 강력한 방법론이다.
- Lec 9 : 다시 정책 경사 방법론으로 돌아와서, 이를 더 잘하기 위해서 어떤 트릭들을 쓸 수 있을까? `Adavanced Policy Gradient` -> Trust Region Polict Optimization (TRPO), Natural Policy Gradient (NPG), Proximal Policy Optimization (PPO) (정책 경사 알고리즘이 너무 이상한 방향으로, 너무 크게 정책을 업데이트 하지 않고 잘 작동하도록 하는 트릭들을 수학적으로 증명(?)함)

이었습니다.

앞으로 배울 내용에 대해서 짧게 언급해보자면

- Lec 10~12 : Model-based RL
- Lec 13~14 : Exploration
- Lec 15~
 
가 될겁니다.


10장부터 12장까지 다룰 주제는 `Model-Based RL`로 1~9장까지 다룬 알고리즘들이 정책 기반, 가치 기반 할 거 없이 Transition Dynamics를 몰라도 됐고, 심지어 이를 알려고도 하지 않았던 (추론 하려고 하지도 않았음) 방법론인 Model-Free RL과는 다릅니다.


앞으로 12장까지 우리는 Model을 다 아는 경우에, 혹은 아닐경우 Model을 직접 모델링 하기도 하는 정책 기반, 가치 기반 알고리즘들에 대해서 알아보게 될 것입니다.


그런 의미에서 강의를 시작하는 지금은 뭔지 정확히 모르겠지만 10장에서 배울 내용은 "Optimal Control and Planning"이 되겠고, 

![slide1](/assets/images/CS285/lec-10/slide1.png)
*Slide. 1.*

이번 장에서 다룰 컨텐츠들은 아래와 같습니다.

![slide2](/assets/images/CS285/lec-10/slide2.png)
*Slide. 2.*


`Optimal Control and Planning` 이란 앞서 설명한 바와 같이 Model을 알고 있고 (access할 수 있고) 이를 사용해서 decision making을 하는 것이며, 이번장의 목표는 "discrete하거나 continuous한 space 를 가지는 문제들에 대해서 어떻게 planning을 하는가?" 입니다.


## <mark style='background-color: #fff5b1'> Introduction to model-based reinforcement learning </mark>

먼저 Model-Based RL에 대해서 얘기하기 위한 Recap입니다.

![slide3](/assets/images/CS285/lec-10/slide3.png)
*Slide. 3.*

*Slide. 3.*에는 일반적인 RL을 학습하기 위한 Objective가 나와있습니다. 이제는 너무나 익숙하죠?
목표는 Sum of Expected Reward를 최대화 모든 Trajectory하에서 최대화 하는 정책의 파라메터를 구하는건데요, 이 때 Trajectory $$\tau$$를 구하는 수식에 `Transition Dynamics (or Transition Probability)`인 $$p(s_{t+1} \vert s_t, a_t)$$가 수식에 껴있는걸 볼 수 있습니다.

![slide4](/assets/images/CS285/lec-10/slide4.png)
*Slide. 4.*

우리가 9장까지 다뤘던 내용들은 $$p(s_{t+1} \vert s_t, a_t)$$를 모르거나, 심지어 몰라도 되는 (구하려고 하지도 않음) 알고리즘들 이었습니다. 예를들어 7장의 Fitted Q-Iteration 는 심지어 Value Iteration이 Transition Dynamics를 알아야 한다는 이유로 이를 피하기 위해서 V 함수를 Q함수로 바꾼 알고리즘이었습니다.  


하지만 Transition Dynamics가 알려져있다거나, 이를 알려고 노력하면 학습하는 데 이를 써먹을 수 있지 않을까요?

![slide5](/assets/images/CS285/lec-10/slide5.png)
*Slide. 5.*

*Slide. 5.*에는 실제로 Dynamics가 이미 주어졌거나 (Atari 게임이나 바둑(Go)같은 경우 알려져 있다고 나와있네요, 딥마인드가 2015년 발표한 `Alphago`는 Model-Free RL과 Model-based RL를 합친 거라고 하네요.), Dynamics를 구하려는 시도들이 종종 있다고 합니다.

```
자율주행을 학습하는 경우에도 Dynamics를 쉽게 생각해서 주어졌다고 생각하거나(일반적인 경우), 쉽게 모델링 할 수 있으며 (밤에 주행하는 경우), 로봇을 시뮬레이션하는 환경에서도 Dynamics를 안다고 할 수 있다고 합니다. 

ex) 로봇의 다리 길이가 4개이며, 길이가 몇이고, 모터의 torque는 어떻고 등등의 system identification을 쉽게 얻을 수 있다고 하네요.
```

여기서 중요한 질문은 `과연 Dynamics를 아는게 도움이 되는가?` 인데, 당연히 대부분의 경우에는 도움이 된다고 합니다.
도움이 되게끔 설계한 알고리즘이 많이 있고 잘 작용한다고 합니다. 

![slide6](/assets/images/CS285/lec-10/slide6.png)
*Slide. 6.*

Model-Based RL은 먼저 `Transition Dyanmics`인 $$p(s_{t+1} \vert s_t, a_t)$$를 배우고 이를 기반으로 어떤 상태에서 어떤 행동을 취할 지를 배운다고 하는데요, *Slide. 6.*에 나와 있는 것처럼 `Lecture 10`에서는 이렇게 Model을 알 경우 어떻게 의사 결정을 하는지에 대해서 알아보고 `Lecture 11,12`에서는 과연 어떻게 이를 학습하는지에 대해서 알아본다고 교수님은 말씀합니다.

![slide7](/assets/images/CS285/lec-10/slide7.png)
*Slide. 7.*

*Slide. 7.*에 수식이 좀 겹쳐져있네요, RL의 Objective를 다시 써보면

$$
min_{a_1,\cdots,a_T} log p(\text{eaten by tiger} \vert a_1, \cdots,c_T)
$$

가 된다고 합니다.
주의해야 할 점은 위의 수식에 Policy는 더이상 존재하지 않습니다.
그저 state와 action 뿐이죠.


어쟀든 이렇게 야생의 환경에 우리가 놓여져 있다고 치면, 어떻게 행동할지에 대한 `Planning Objective`를 수식화 하는 합리적인 방법 중 하나는 호랑이에게 잡아먹힐 확률을 최소화 하기 위한 일련의 action을 계획(plan)하는 겁니다.


이를 `Planning Problem`이라고 하며, 이럴 경우 Policy를 도출해낼 필요가 없다고 합니다.
위의 수식을 다시 cost의 합을 줄이거나, reward의 합을 최대화 하는 일련의 action을 선택하는 수식으로 표현하면 아래와 같이 됩니다.

$$
\begin{aligned}
&
min_{a_1,\cdots,a_T} log p(\text{eaten by tiger} \vert a_1, \cdots,c_T)
& \\

&
min_{a_1,\cdots,a_T} \sum_{t=1}^T c(s_t,a_t) \text{ s.t. } s_t = f(s_{t-1},a_{t-1})
& \\
\end{aligned}
$$

이는 `deterministic dynamics case`이며, 나중에 이를 stochastic case로 바꿔서 다시 생각해본다고 합니다 (확률 분포와 기대값 을 써서 표현하겠죠?).


![slide8](/assets/images/CS285/lec-10/slide8.png)
*Slide. 8.*

`deterministic dynamics case`에 대해서 조금 더 생각해보도록 하겠습니다.
*Slide. 8.*에 나와있는 그림이 의미하는 바는 환경으로부터 초기 상태 $$s_1$$가 어딘지를 알게되고, 그 때 부터 Agent는 일련의 행동을 하는겁니다. 잘 학습되었다면, 일련의 행동이 끝났을 때 잘높은 보상을 받게 될 겁니다 (같은말이지만 반대로는 cost가 최저인).

(지금은 어떤 상태에서 어떤 행동을 하는게 최선의 선택인지를 결정하는게 아닙니다. 즉 앞서 말한 것 처럼 policy가 필요 없습니다.)

이를 수식으로 표현하면 *Slide. 8.*에 나와있는데요, 수식에 오류가 있어서 다시 쓰도록 하겠습니다.

$$
a_1, \cdots, a_T = arg max_{a_1,\cdots,c_T} \sum_{t=1}^T r(s_t,a_t) \text{ s.t. } s_{t+1} = f(s_t,a_t)
$$

deterministic dynamics case는 상당히 straight forward 했는데요, 이번엔 `stochastic case`에 대해서 알아보겠습니다.

 
![slide9](/assets/images/CS285/lec-10/slide9.png)
*Slide. 9.*

`stochastic case`의 경우에는 다음과 같이 일련의 action들을 조건부로 일련의 state들을 리턴하는 확률 분포를 정의하고,

$$
p_{\theta}(s_1,\cdots,s_T \vert a_1, \cdots, a_T) = p(s_1) \prod_{t=1^T} p(s_{t+1} \vert s_t,a_t)
$$

이 수식이 우리가 일반적으로 알고 있는 Trajectory 수식과 다른 점은 $$p(a_t \vert s_t)$$가 빠져있다는 겁니다. (deterministic과 마찬가지로 policy는 필요 없다는 뜻)

이 때의 `Plannig Objective`는 아래와 같이 쓸 수 있습니다.

$$
a_1, \cdots, a_T = arg max_{a_1,\cdots, a_T} \mathbb{E} [ \sum_t r(s_t,a_t) \vert a_1,\cdots,a_T ]
$$

앞서 잠깐 얘기한 것 처럼 deterministic case와 달리 확률 분포 $$p_{\theta}(s_1,\cdots,s_T \vert a_1, \cdots, a_T)$$와 이 분포 하의 기대값 $$\mathbb{E}$$가 들어가서 수식을 완성시키는 걸 알 수 있습니다.


이런 approach가 stochastic environment에서 나쁘지는 않지만
어떤 경우에는 별로 좋지않은 결과를 내놓을 수 있다고 하는데요,

즉 deterministic case와 다르게 우리가 지금 정의한 stochastic case의 수식을 사용한 경우에는
최적 (optimal)의 결과가 아닌 `sub-optimal`한 결과를 낸다고 합니다.


```
Sergey : 과연 어떤 경우에 이런 approach가 극도로 suboptimal한, 즉 그저 그런 결과를 내놓는 걸까요? 생각해보죠
```

그런 경우는 예를 들어 좋은 행동을 하기 위한 크리티컬한 정보가 미래에 주어지는 경우라고 하는데요,
예를 들어 교수님이 open-book 시험을 내준다고 생각하면 우리는 가능한 문제 (state) 에 대해서 다 생각해보고 이에 대해 답해보고 (action)를 해봐야 한다는 겁니다. (그치만 문제가 어떤 유형인지, 어느 챕터에서 낼건지 뭔지를 알려주면 수월하겠죠?, 아무튼 나올만한 문제를 다 고려하는 경우 굉장히 suboptimal한 결과값을 내놓는다고 합니다.) 하지만 그와 반대로 closed-book이지만 어떤 문제를 낼 지 알려준다면 이런 경우가 훨씬 쉽게 다가올거라고 합니다.

![slide10](/assets/images/CS285/lec-10/slide10.png)
*Slide. 10.*

*Slide. 10.*에는 `closed-loop vs open-loop`에 대한 내용이 그림으로 나와있는데요,
간단히 생각해서 지금까지 배운 내용이 open-loop이며, 이전에 정책을 통해서 컨트롤을 해나가는 과정을 closed-loop라고 할 수 있다고 합니다.

close-loop는 쌍방으로 계속 피드백을 받는 반면, open-loop의 경우에는 $$s_1$$에 대해서만 한번 리턴받고 그 뒤로는 agent가 플래닝한 결과를 리턴해버리는 `단방향소통 (one-way communication)`이라고 할 수 있습니다.

```
RL은 일반적으로는 closed-loop 문제를 풀 수 있다고 볼 수 있습니다.
```

![slide11](/assets/images/CS285/lec-10/slide11.png)
*Slide. 11.*


*Slide. 11.*에는 `stochastic closed-loop case`에 대한 내용이 나와있는데요,
그림을 보시면 $$s_1$$을 한번 리턴받고 Agent가 `transition dynamics를 이용해서`(이 점이 Model-Free RL인 그냥 Policy Gradient와 다르죠) policy를 리턴함을 볼 수 있습니다. 

Objective는 그냥 RL Objective와 다를 게 없고, trajectory 수식에 dynamics가 있는것을 알 수 있습니다.

그럼 그동안 Neural Network로 policy를 디자인한 것 처럼 하면 될까요?
그래도 되고 아니어도 된다고 하는데요, 왜냐면 신경망을 사용한다는 것은 마주칠 수 있는 모든 `state`를 고려하기 위한 approximator를 사용한다는 의미로 `global policy`를 찾는것과 같지만 지금은 되게 짧은 범위를 가지고 있는 $$s_1$$를 기반으로만 하면 되기 때문에 `local policy` 로 해결이 가능하다고 합니다. (곧 배우겠지만 `time-varing linear policy`같은 거라고 합니다.)


예를 들어, 어떤 trajectory를 따라서 날아가는 로켓을 컨트롤하는 이는 대기의 상태나 모터의 상황에 따라 계획된 경로를 이탈 하기 쉬운 경우 굉장히 stochastic한 세팅이지만, 이런 경우 크게 경로를 이탈하지는 않기 때문에 조금 이탈한 경로에 대해서 로켓을 기울이면서 (action) 경로를 수정해주는 등의 local policy를 linear한 방법으로 구해낼 수 있다고 합니다.


## <mark style='background-color: #fff5b1'> Open-Loop Planning </mark>

![slide13](/assets/images/CS285/lec-10/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-10/slide14.png)
*Slide. 14.*






### <mark style='background-color: #dcffe4'> Cross-Entropy Method (CEM) </mark>

![slide15](/assets/images/CS285/lec-10/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-10/slide16.png)
*Slide. 16.*


### <mark style='background-color: #dcffe4'> Monte Carlo Tree Search (MCTS) </mark>

![slide17](/assets/images/CS285/lec-10/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-10/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-10/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-10/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-10/slide21.png)
*Slide. 21.*


## <mark style='background-color: #fff5b1'> Trajectory Optimization with Derivatives </mark>

![slide23](/assets/images/CS285/lec-10/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-10/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-10/slide25.png)
*Slide. 25.*


### <mark style='background-color: #dcffe4'> Linear Case : LQR </mark>

![slide26](/assets/images/CS285/lec-10/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-10/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-10/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-10/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-10/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-10/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-10/slide32.png)
*Slide. 32.*


## <mark style='background-color: #fff5b1'> LQR for Stochastic and Nonlinear Systems </mark>

![slide34](/assets/images/CS285/lec-10/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-10/slide35.png)
*Slide. 35.*


### <mark style='background-color: #dcffe4'> Non-Linear Case : DDP / Iterative LQR </mark>

![slide36](/assets/images/CS285/lec-10/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-10/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-10/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-10/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-10/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-10/slide41.png)
*Slide. 41.*


## <mark style='background-color: #fff5b1'> Case Study and Additional Readings </mark>

![slide42](/assets/images/CS285/lec-10/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-10/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-10/slide44.png)
*Slide. 44.*

![slide45](/assets/images/CS285/lec-10/slide45.png)
*Slide. 45.*

![slide46](/assets/images/CS285/lec-10/slide46.png)
*Slide. 46.*



## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)






