---
title: (yet) Lecture 10 - Optimal Control and Planning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 10의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=4SL0DnxC1GM&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=42)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-10.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

여태까지 cs285에서 배웠던 `Deep RL`의 흐름을 되짚어 보도록 합시다.

- Lec 2~4 : Deep Learning과 Deep RL은 어떻게 다른가? (behavior cloning)
- Lec 5~6 : `Policy (Actor) Gradient` 방법론은 무엇인가? 그리고 이 알고리즘을 강화하기 위한 (unbias하고 low-variance하게 만들기 위해서) 트릭들 + 현재 정책을 평가하는 가치기반 함수 (Q,V,A) 들을 추가해서 더 정교하게 만들 순 없을까? 즉, Critic을 추가한다! -> `Actor-Critic`
- Lec 7~8 : Policy를 직접적으로 배우지 않고 간접적으로 (implicitly) 배울 수는 없을까? -> 우리가 환경에 대해 다 알고 (Transition Dynamics) state, action을 표로 간단하게 표현 가능할 경우 optimal policy를 `Dynamic Programming`을 통해서 구할 수 있다 -> `Policy Iteration`, `Value Iteration` -> 근데 Transition Dynamics를 모른다면? -> `Q-Iteration`, `Q-Learning`, 특히 Q-Learning은 딥마인드가 2013년 아타리 문제를 푼다던가 하는 데 사용된 굉장히 general하면서 잘 작동하는 강력한 방법론이다.
- Lec 9 : 다시 정책 경사 방법론으로 돌아와서, 이를 더 잘하기 위해서 어떤 트릭들을 쓸 수 있을까? `Adavanced Policy Gradient` -> Trust Region Polict Optimization (TRPO), Natural Policy Gradient (NPG), Proximal Policy Optimization (PPO) (정책 경사 알고리즘이 너무 이상한 방향으로, 너무 크게 정책을 업데이트 하지 않고 잘 작동하도록 하는 트릭들을 수학적으로 증명(?)함)

이었습니다.

앞으로 배울 내용에 대해서 짧게 언급해보자면

- Lec 10~12 : Model-based RL
- Lec 13~14 : Exploration
- Lec 15~
 
가 될겁니다.


즉 10장부터 12장까지 다룰 주제는 `Model-Based RL`로 1~9장까지 다룬 알고리즘들이 정책 기반, 가치 기반 할 거 없이 Transition Dynamics를 몰라도 됐고, 심지어 이를 알려고도 하지 않았던 (추론 하려고 하지도 않았음) 방법론인 `Model-Free RL` 이었던것과는 다릅니다.


앞으로 12장까지 우리는 Model을 다 아는 경우에, 혹은 아닐경우 Model을 직접 모델링 하기도 하는 정책 기반, 가치 기반 알고리즘들에 대해서 알아보게 될 것입니다.


그런 의미에서 강의를 시작하는 지금은 뭔지 정확히 모르겠지만 10장에서 배울 내용은 "Optimal Control and Planning"이 되겠고, 

![slide1](/assets/images/CS285/lec-10/slide1.png)
*Slide. 1.*

이번 장에서 다룰 컨텐츠들은 아래와 같습니다.

![slide2](/assets/images/CS285/lec-10/slide2.png)
*Slide. 2.*


Optimal Control and Planning 이란 앞서 설명한 바와 같이 Model을 알고 있고 (access할 수 있고) 이를 사용해서 decision making을 하는 것이며, 이번장의 목표는 "discrete하거나 continuous한 space 를 가지는 문제들에 대해서 어떻게 planning을 하는가?" 입니다.


## <mark style='background-color: #fff5b1'> Introduction to model-based reinforcement learning </mark>

먼저 Model-Based RL에 대해서 얘기하기 위한 Recap입니다.

![slide3](/assets/images/CS285/lec-10/slide3.png)
*Slide. 3.*

*Slide. 3.*에는 일반적인 RL을 학습하기 위한 Objective가 나와있습니다. 이제는 너무나 익숙하죠?
목표는 Sum of Expected Reward를 최대화 모든 Trajectory하에서 최대화 하는 정책의 파라메터를 구하는건데요, 이 때 Trajectory $$\tau$$를 구하는 수식에 `Transition Dynamics (or Transition Probability)`인 $$p(s_{t+1} \vert s_t, a_t)$$가 수식에 껴있는걸 볼 수 있습니다.

![slide4](/assets/images/CS285/lec-10/slide4.png)
*Slide. 4.*

우리가 9장까지 다뤘던 내용들은 $$p(s_{t+1} \vert s_t, a_t)$$를 모르거나, 심지어 몰라도 되는 (구하려고 하지도 않음) 알고리즘들 이었습니다. 예를들어 7장의 Fitted Q-Iteration 는 심지어 Value Iteration이 Transition Dynamics를 알아야 한다는 이유로 이를 피하기 위해서 V 함수를 Q함수로 바꾼 알고리즘이었습니다.  


하지만 Transition Dynamics가 알려져있다거나, 이를 알려고 노력하면 학습하는 데 이를 써먹을 수 있지 않을까요?

![slide5](/assets/images/CS285/lec-10/slide5.png)
*Slide. 5.*

*Slide. 5.*에는 실제로 Dynamics가 이미 주어졌거나 (Atari 게임이나 바둑(Go)같은 경우 알려져 있다고 나와있네요, 딥마인드가 2015년 발표한 `Alphago`는 Model-Free RL과 Model-based RL를 합친 거라고 하네요.), Dynamics를 구하려는 시도들이 종종 있다고 합니다.

여기서 중요한 질문은 `과연 Dynamics를 아는게 도움이 되는가?` 인데, 당연히 대부분의 경우에는 도움이 된다고 합니다.

![slide6](/assets/images/CS285/lec-10/slide6.png)
*Slide. 6.*

Model-Based RL은 먼저 `Transition Dyanmics`인 $$p(s_{t+1} \vert s_t, a_t)$$를 배우고 이를 기반으로 어떤 상태에서 어떤 행동을 취할 지를 배운다고 하는데요, *Slide. 6.*에 나와 있는 것처럼 `Lecture 10`에서는 이렇게 Model을 알 경우 어떻게 의사 결정을 하는지에 대해서 알아보고 `Lecture 11,12`에서는 과연 어떻게 이를 학습하는지에 대해서 알아본다고 교수님은 말씀합니다.

![slide7](/assets/images/CS285/lec-10/slide7.png)
*Slide. 7.*

![slide8](/assets/images/CS285/lec-10/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-10/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-10/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-10/slide11.png)
*Slide. 11.*


## <mark style='background-color: #fff5b1'> Open-Loop Planning </mark>

![slide13](/assets/images/CS285/lec-10/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-10/slide14.png)
*Slide. 14.*


### <mark style='background-color: #dcffe4'> Cross-Entropy Method (CEM) </mark>

![slide15](/assets/images/CS285/lec-10/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-10/slide16.png)
*Slide. 16.*


### <mark style='background-color: #dcffe4'> Monte Carlo Tree Search (MCTS) </mark>

![slide17](/assets/images/CS285/lec-10/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-10/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-10/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-10/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-10/slide21.png)
*Slide. 21.*


## <mark style='background-color: #fff5b1'> Trajectory Optimization with Derivatives </mark>

![slide23](/assets/images/CS285/lec-10/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-10/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-10/slide25.png)
*Slide. 25.*


### <mark style='background-color: #dcffe4'> Linear Case : LQR </mark>

![slide26](/assets/images/CS285/lec-10/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-10/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-10/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-10/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-10/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-10/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-10/slide32.png)
*Slide. 32.*


## <mark style='background-color: #fff5b1'> LQR for Stochastic and Nonlinear Systems </mark>

![slide34](/assets/images/CS285/lec-10/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-10/slide35.png)
*Slide. 35.*


### <mark style='background-color: #dcffe4'> Non-Linear Case : DDP / Iterative LQR </mark>

![slide36](/assets/images/CS285/lec-10/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-10/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-10/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-10/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-10/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-10/slide41.png)
*Slide. 41.*


## <mark style='background-color: #fff5b1'> Case Study and Additional Readings </mark>

![slide42](/assets/images/CS285/lec-10/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-10/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-10/slide44.png)
*Slide. 44.*

![slide45](/assets/images/CS285/lec-10/slide45.png)
*Slide. 45.*

![slide46](/assets/images/CS285/lec-10/slide46.png)
*Slide. 46.*



## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)






