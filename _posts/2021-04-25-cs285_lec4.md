---
title: (미완) Lecture 4 - Introduction to Reinforcement Learning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 4의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=jds0Wh9jTvE&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=11)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 강의의 주제는 "Introduction to Reinforcement Learning" 입니다. 본격적으로 강화 학습이 무엇인지에 대한 정의와 컨셉에 대해서 알아보는 챕터 입니다. 

![slide1](/assets/images/CS285/lec-4/slide1.png)
*Slide. 1.*


## <mark style='background-color: #fff5b1'> Definitions </mark>

2장에서 배웠던 걸 다시 리마인드 하면서 챕터가 시작되는데요,

![slide3](/assets/images/CS285/lec-4/slide3.png)
*Slide. 3.*

다 배웠던 것이므로 글에서는 넘어가도록 겠습니다. 강화학습의 목표는 `정책 (Policy)` $$\pi_{\theta}(a_t \vert s_t)$$를 학습하는 것 입니다. 심층 강화학습 에서는 이 정책이 `심층 신경망 (Deep Neural Network)`으로 디자인 되어 있으며, 이러한 정책을 하는 방법은 크게 두 가지로 나눌 수 있습니다.

- `Policy`를 directly 하는 방법 
- 다른 가치 함수 (`Value Function`) 같은 Object를 통해 implicitly 학습하는 방법

이러한 방법론들에 대해서 앞으로 알아보게 될겁니다.


했던 얘기를 그래도 조금만 살펴보자면, Observation과 State를 구분짓는 가장 큰 차이점은 `Markov Property`를 만족하는가 였고, Observation은 full State를 추론 하는 데 필요한 정보를 모두 포함하고 있을수도, 아닐 수도 있는, State의 어떠한 Stochastic Function 이라고 할 수 있엇습니다.


앞으로 강의에서는 State에 접근할 수 있는 `Fully Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert s_t $$) 과 Observation에만 접근할 수 있는 `Partially Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert o_t $$) 에 대해서 알아볼겁니다.

![slide4](/assets/images/CS285/lec-4/slide4.png)
*Slide. 4.*

*Slide. 4.*도 그냥 지난 시간에 배운 내용의 Recap 인데요, Imitation Learning이란 사람의 행동을 그대로 매 순간 순간 Supervised Learning 하면서 따라가는 거였죠. Observation - Action tuple을 학습 셋으로 사용해서 정책을 학습했습니다.


하지만 이번 장에서의 목표는 이러한 Expert data 없이 정책을 학습을 하는 알고리즘들에 대해서 알아보는 것이 될 겁니다.


![slide5](/assets/images/CS285/lec-4/slide5.png)
*Slide. 5.*

그러기 위해서 우리는 매 장면 장면 어떤 행동을 하는지를 Supervised Learning 하지 않기 위한 새로운 Objective를 도입하게 되는데요, 이를 `보상 함수 (Reward Function)` 라고 합니다. 이제 매 장면마다 어떤 행동을 해야 하는지 일일히 Labeling 되어 있지 않기 때문에 어떤 상황에서 어떤 행동을 하는 것이 옳은지를 판단해야 하는데 그것을 보상 함수가 말해주게 되는거죠.

$$r(s,a)$$ 

위와 같은 수식으로 나타내는 보상 함수는 일반적으로 $$s,a$$ 두 가지 모두에 대해서 condition 되지만 $$s$$만을 condition할 때도 있습니다. 함수가 리턴하는 결과값은 벡터가 아닌 스칼라 값 입니다.


자율 주행을 하는 경우 사고 없이 잘 달리면 매 순간 좋은 값 (양수) 을 얻고, 사고가 나게 되면 그 순간에는 좋지 않은 값 (음수) 을 얻겠죠. 
하지만 강화 학습에서는 현재 상태의 보상 만을 고려하여 어떤 행동을 할 지를 선택하지 않습니다.

그러니까 자율 주행을 하는데 현재 시점에 아무도 없어서 속도를 최대한으로 내서 빠르게 달리는 것이 결국에는 사고 위험성이 높아서 나중에는 최악의 상황 (교통사고)를 낼 수도 있다는걸 사람이 알 듯, 기계도 현재 "빠르게 달리는게 목표에 빨리 도달하겠지?" 뿐만 아니라 미래의 교통 사고 가능성까지 생각하여 적당한 속도로 달리게 한다는 거죠.


즉 강화 학습은 `미래 보상 (Future Rewards)`을 필수적으로 생각하는 것이 중요합니다. 

```
Sergey는 이를 'The heart of the decision making problems' 혹은 'The heart of the reinforcement learning problems' 이라고 하네요.
```

이러한 term들을 가지고 

- State ($$s_t$$)
- Action ($$a_t$$)
- Reward ($$r(a_t \vert s_t)$$)
- State Transition Probability

우리는 `Markov Decision Process (MDP)`(또는 `Decision Process on Markovian State`라는 것)을 정의할 수 있게 됩니다. 


자 이제 MDP에 대한 Full formal definition을 빌드업 하도록 하겠습니다.

![slide6](/assets/images/CS285/lec-4/slide6.png)
*Slide. 6.*






![slide7](/assets/images/CS285/lec-4/slide7.png)
*Slide. 7.*


![slide8](/assets/images/CS285/lec-4/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-4/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-4/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-4/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-4/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-4/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-4/slide14.png)
*Slide. 14.*


![slide15](/assets/images/CS285/lec-4/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-4/slide16.png)
*Slide. 16.*







## <mark style='background-color: #fff5b1'> Algorithms </mark>

![slide18](/assets/images/CS285/lec-4/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-4/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-4/slide20.png)
*Slide. 19.*

![slide21](/assets/images/CS285/lec-4/slide21.png)
*Slide. 21.*







## <mark style='background-color: #fff5b1'> Value Functions </mark>


![slide23](/assets/images/CS285/lec-4/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-4/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-4/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-4/slide26.png)
*Slide. 26.*






## <mark style='background-color: #fff5b1'> Types of Algorithms </mark>

![slide28](/assets/images/CS285/lec-4/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-4/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-4/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-4/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-4/slide32.png)
*Slide. 32.*

![slide33](/assets/images/CS285/lec-4/slide33.png)
*Slide. 33.*






### <mark style='background-color: #dcffe4'> Tradeoffs Between Algorithms </mark>

![slide35](/assets/images/CS285/lec-4/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-4/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-4/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-4/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-4/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-4/slide40.png)
*Slide. 40.*






### <mark style='background-color: #dcffe4'> Examples of Algorithms </mark>

![slide42](/assets/images/CS285/lec-4/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-4/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-4/slide44.png)
*Slide. 44.*

![slide45](/assets/images/CS285/lec-4/slide45.png)
*Slide. 45.*

![slide46](/assets/images/CS285/lec-4/slide46.png)
*Slide. 46.*








## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)














