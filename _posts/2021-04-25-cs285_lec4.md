---
title: (미완) Lecture 4 - Introduction to Reinforcement Learning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 4의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=jds0Wh9jTvE&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=11)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 강의의 주제는 "Introduction to Reinforcement Learning" 입니다. 본격적으로 강화 학습이 무엇인지에 대한 정의와 컨셉에 대해서 알아보는 챕터 입니다. 

![slide1](/assets/images/CS285/lec-4/slide1.png)
*Slide. 1.*


## <mark style='background-color: #fff5b1'> Definitions </mark>

2장에서 배웠던 걸 다시 리마인드 하면서 챕터가 시작되는데요,

![slide3](/assets/images/CS285/lec-4/slide3.png)
*Slide. 3.*

다 배웠던 것이므로 글에서는 넘어가도록 겠습니다. 강화학습의 목표는 `정책 (Policy)` $$\pi_{\theta}(a_t \vert s_t)$$를 학습하는 것 입니다. 심층 강화학습 에서는 이 정책이 `심층 신경망 (Deep Neural Network)`으로 디자인 되어 있으며, 이러한 정책을 하는 방법은 크게 두 가지로 나눌 수 있습니다.

- `Policy`를 directly 하는 방법 
- 다른 가치 함수 (`Value Function`) 같은 Object를 통해 implicitly 학습하는 방법

이러한 방법론들에 대해서 앞으로 알아보게 될겁니다.


했던 얘기를 그래도 조금만 살펴보자면, Observation과 State를 구분짓는 가장 큰 차이점은 `Markov Property`를 만족하는가 였고, Observation은 full State를 추론 하는 데 필요한 정보를 모두 포함하고 있을수도, 아닐 수도 있는, State의 어떠한 Stochastic Function 이라고 할 수 있엇습니다.


앞으로 강의에서는 State에 접근할 수 있는 `Fully Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert s_t $$)) 과 Observation에만 접근할 수 있는 `Partially Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert o_t $$)) 에 대해서 알아볼겁니다.

![slide4](/assets/images/CS285/lec-4/slide4.png)
*Slide. 4.*

*Slide. 4.*도 그냥 지난 시간에 배운 내용의 Recap 인데요, Imitation Learning이란 사람의 행동을 그대로 매 순간 순간 Supervised Learning 하면서 따라가는 거였죠. Observation - Action tuple을 학습 셋으로 사용해서 정책을 학습했습니다.


하지만 이번 장에서의 목표는 이러한 Expert data 없이 정책을 학습을 하는 알고리즘들에 대해서 알아보는 것이 될 겁니다.


![slide5](/assets/images/CS285/lec-4/slide5.png)
*Slide. 5.*

그러기 위해서 우리는 매 장면 장면 어떤 행동을 하는지를 Supervised Learning 하지 않기 위한 새로운 Objective를 도입하게 되는데요, 이를 `보상 함수 (Reward Function)` 라고 합니다. 이제 매 장면마다 어떤 행동을 해야 하는지 일일히 Labeling 되어 있지 않기 때문에 어떤 상황에서 어떤 행동을 하는 것이 옳은지를 판단해야 하는데 그것을 보상 함수가 말해주게 되는거죠.

$$r(s,a)$$ 

위와 같은 수식으로 나타내는 보상 함수는 일반적으로 $$s,a$$ 두 가지 모두에 대해서 condition 되지만 $$s$$만을 condition할 때도 있습니다. 함수가 리턴하는 결과값은 벡터가 아닌 스칼라 값 입니다.


자율 주행을 하는 경우 사고 없이 잘 달리면 매 순간 좋은 값 (양수) 을 얻고, 사고가 나게 되면 그 순간에는 좋지 않은 값 (음수) 을 얻겠죠. 
하지만 강화 학습에서는 현재 상태의 보상 만을 고려하여 어떤 행동을 할 지를 선택하지 않습니다.

그러니까 자율 주행을 하는데 현재 시점에 아무도 없어서 속도를 최대한으로 내서 빠르게 달리는 것이 결국에는 사고 위험성이 높아서 나중에는 최악의 상황 (교통사고)를 낼 수도 있다는걸 사람이 알 듯, 기계도 현재 "빠르게 달리는게 목표에 빨리 도달하겠지?" 뿐만 아니라 미래의 교통 사고 가능성까지 생각하여 적당한 속도로 달리게 한다는 거죠.


즉 강화 학습은 `미래 보상 (Future Rewards)`을 필수적으로 생각하는 것이 중요합니다. 

```
Sergey는 이를 'The heart of the decision making problems' 혹은 'The heart of the reinforcement learning problems' 이라고 하네요.
```

이러한 term들을 가지고 

- State ($$s_t$$)
- Action ($$a_t$$)
- Reward ($$r(a_t \vert s_t)$$)
- State Transition Probability

우리는 `Markov Decision Process (MDP)`(또는 `Decision Process on Markovian State`라는 것)을 정의할 수 있게 됩니다. 


자 이제 MDP에 대한 Full formal definition을 빌드업 하도록 하겠습니다.

![slide6](/assets/images/CS285/lec-4/slide6.png)
*Slide. 6.*

먼저 `Markov Chain`에 대해서 알아볼 건데요, 이는 Stochastic Processes 분야를 개척한 [Andrey Markov](https://en.wikipedia.org/wiki/Andrey_Markov) 라는 수학자가 이름 지었습니다.

(Markov Chain, Markov Reward Process (MRP), Markov Decision Process (MDP) 모두 비슷하지만 조금씩 다른 개념 입니다.)


Markov Chain의 개념은 굉장히 간단한데요, 우선 마르코프 체인은 두 가지로 구성되어 있습니다.

$$
M=\{S,T\}
$$

- $$S$$ : State Space $$\rightarrow$$ (states $$s \in S$$)
- $$T$$ : Transition Operator (or Transition Probability, Dynamics Function) $$\rightarrow$$ $$p(s_{t+1} \vert s_t)$$

Transition Operator는 $$s_t$$ 에서 $$s_{t+1}$$로 상태가 바뀔 확률을 나타내는데요, 이 것을 `Operator`라고 하는 이유는 다음과 같기 때문입니다.

$$
let \space \mu_{t,i} = p(s_i = i)
$$

<center>
$$\vec{\mu_t} $$ is a vector of probabilities 
</center>

$$
let \space T_{i,j} = p(s_{t+1}=i \vert s_t = j)
$$

<center>
then $$\vec{\mu_{t+1}} = T \vec{\mu_t}$$
</center>

Discrete한 상태가 여러 개 있다면, 어떤 time-step t에서의 상태를 벡터로 나타낼 수 있을 것이며, 해당 state($$s_t$$\j)에서 다른 state($$s_{t+1}=i$$) 로 이동할 확률은 바로 `Transition Probability Matrix`와 `Vector of State Probability`를 행렬 곱하는 `연산 (Operate)`이 되기 때문입니다.


위의 개념들을 이용한 Markov Chain의 Graphical Model은 *Slide. 6.*의 하단에 잘 나타나 있습니다. 
여기서 그래프의 edge가 나타내는게 바로 Transition Probability가 되며, Markov Chain은 Markov Property를 만족하게 됩니다.

$$
p(s_{t+1} \vert s_t, s_{t-1}, s_{t-2}, s_{t-3}, \cdots) = p(s_{t+1} \vert s_t)
$$

하지만 Markov Chain 만으로는 decision making problem을 해결할 수 없는데요, 바로 행동 (Action), $$a_t$$에 대한 개념이 없기 때문입니다.

![slide7](/assets/images/CS285/lec-4/slide7.png)
*Slide. 7.*

MDP는 위의 Markov Chain 개념에 몇가지 (additional objects)를 더 추가한 것인데요, 1950년대에 [Richard Bellman](https://en.wikipedia.org/wiki/Richard_E._Bellman)에 의해 제안된 개념입니다.

$$
M=\{S,A,T,r\}
$$

Markov Chain과 비교해서 `Action Space` 와 `Reward Function` $$A,r$$이 추가된거죠. (MRP에서는 A가 없이 r만 있는데, 여기서는 다루지 않는 것 같으니 생략하겠습니다.)
Action Space는 State Space와 마찬가지로 이산적 (discrete) 일 수도, 연속적 (continuous) 일 수도 있습니다. 


이제 Graphical Model에는 Action이 추가됐으며, Transition Operator에도 $$a_t$$가 추가로 condition된 것을 알 수 있습니다. 
(당연히 Markov Property를 만족해야 하니, $$s_t,a_t$$ 에 의해서만 다음 상태 $$s_{t+1}$$이 결정됩니다.)

그리고 Transition Operator는 더 이상 Matrix (2-Dimensional)이 아닌 Tensor (3-Dimensional) 이 됩니다. 
(왜냐하면 이제는 현재 상태, 다음 상태, 현재 행동 세 가지를 고려해야 하기 때문입니다.)


Markov Chain에서와 마찬가지로 선형 대수의 개념을 이용해 다음 상태에 대한 확률을 Transition Operator와 현재 상태, 현재 행동의 확률을 가지고 있는 벡터들과의 연산을 통해 구할 수 있습니다. (*Slide. 7.* 하단 참조)

$$
let \space \mu_{t,j} = p(s_t=j)
$$

$$
let \space \xi_{t,k} = p(a_t=k)
$$

$$
let \space T_{i,j,k} = p(s_{t+1}=i \vert s_t=j, a_t=k)
$$

$$
\mu_{t+1,i} = \sum_{j,k} T_{i,j,k} \mu_{t,j} \xi_{t,k} 
$$


![slide8](/assets/images/CS285/lec-4/slide8.png)
*Slide. 8.*

보상 함수 (Reward Function)은 상태 (state)와 행동 (action) 의 카티전 곱 (Cartesian Product) 을 어떠한 real value number로 매핑해주는 함수 입니다. 

$$
r \colon S \times A \rightarrow \mathbb{R}
$$

보상 함수는 아래와 같이 나타낼 수 있으며,

$$
r(s_t,a_t) 
$$

강화학습의 목적은 `sum of future rewards`를 Maximize 하는 것 이며, 이제 이에 대해 알아볼겁니다. 
( cf) maximize log-likelihood or minimize negative log-likelihood in Common Deep Learning Approaches )


![slide9](/assets/images/CS285/lec-4/slide9.png)
*Slide. 9.*

하지만 알아보기 전에 MDP를 조금 더 확장해서 `partially observed MDP`로 생각해 보도록 할건데요,



`Partially Observed MDP`는 MDP에 또 몇가지 개념이 추가된 것입니다.

$$
M=\{S,A,O,T,\varepsilon,r\}
$$

cf)

$$
M_{Markov \space Chain}=\{S,A\}
$$

$$
M_{MDP}=\{S,A,T,r\}
$$

이제 State Space, Action Space 외에 `Observation Space`, $$O$$와 `Emission Probability`, $$\varepsilon$$가 추가됐습니다.

- $$S$$ : State Space, state $$s \in S$$ (discrete or continuous)
- $$A$$ : Action Space, action $$a \in A$$ (discrete or continuous)
- $$O$$ : Observation Space, observation $$o \in O$$ (discrete or continuous)

Observation, $$o$$는 state에 depend하며, Emission Probability는 $$s_t$$ 에서 $$o_t$$를 관측할 확률을 나타냅니다 ($$p(o_t \vert s_t)$$).

일반적으로 Reward Function은 state와 action에 의거해 값을 산출하지만, `Partially Observed MDP`나 `palmdp`는  True State에 접근하지 않고, Observation에 근거해서 decision making을 합니다.





***

Markov Chain, MDP, MRP에 대해서 추가자료를 조금 첨부해 보도록 하겠습니다.


(이하 모든 이미지의 출처 : [2015 UCL Course on RL from David Silver](https://www.davidsilver.uk/teaching/))

- `Markov Chain`



![markov_chain1](/assets/images/CS285/lec-4/markov_chain1.png)
*Additive Fig. Markov Chain Example: Student Markov Chain. 평범한 대학생의 동선을 Markov Chain으로 나타낸 겁니다. (집에서는 잠만 자네요...)*

![markov_chain3](/assets/images/CS285/lec-4/markov_chain3.png)
*Additive Fig. Markov Chain의 Transition Probability Matrix (Operator)는 위와 같습니다. 이에 따라 상태가 변할 수도 아닐 수도 있습니다. (나중에 다루겠지만 우리가 이 행렬을 아는 경우는 일반적으로 없습니다. 게임에서 내가 몬스터를 공격했을 때 miss가 뜰지, 아닐지는 보통 모르니까요, 몇번 때려보고 통계를 내서 행렬로 쓰거나 할 수 있습니다.)*

![markov_chain2](/assets/images/CS285/lec-4/markov_chain2.png)
*Additive Fig. Markov Chain을 통해서 에피소드 (episode)를 몇 개 샘플링 할 수 있습니다.*



- `Markov Reward Process (MRP)`

![mrp1](/assets/images/CS285/lec-4/mrp1.png)
*Additive Fig. Markov Reward Process Example: Student MRP. 평범한 대학생의 동선을 MRP으로 나타낸 겁니다. Markov Chain과 다르게 Reward가 생겼죠.*


(아래는 곧 다룰거지만 미리 첨부하겠습니다.)

***

![mrp2](/assets/images/CS285/lec-4/mrp2.png)
*Additive Fig. 아직 얘기는 안했지만 강화 학습에서는 에피소드의 총 Reward를 고려하는데, 이 Reward에는 일반적으로 감가율 (discount factor, $$\gamma$$)를 적용합니다. 이는 현재 상태에서 행동을 취할 때 현재 부터 미래 가치를 전부 고려하기는 하지만 그래도 나비 효과처럼 나중에 생길 미래의 가치 보다는 현재의 가치에 중점을 두겠다는 의미를 내포합니다.*

![mrp3](/assets/images/CS285/lec-4/mrp3.png)
*Additive Fig. 감가율 (discount factor, $$\gamma$$)이 0일 경우 현재 가치만을 고려하게 됩니다. (오늘만 사는거죠...)*


![mrp4](/assets/images/CS285/lec-4/mrp4.png)
*Additive Fig. 감가율 (discount factor, $$\gamma$$)이 0.9일 경우 미래 가치를 적절하게 고려하게 됩니다.*

![mrp4](/assets/images/CS285/lec-4/mrp4.png)
*Additive Fig. 감가율 (discount factor, $$\gamma$$)이 1일 경우.*

***

- `Markov Decision Process (MDP)`


![mdp1](/assets/images/CS285/lec-4/mdp1.png)
*Additive Fig. Markov Decision Process Example: Student MDP.*


![mdp2](/assets/images/CS285/lec-4/mdp2.png)
*Additive Fig. Optimal Policy for Student MDP. 곧 Policy를 어떻게 학습하는지, 과연 Optimal Policy는 존재하며, 이는 어떻게 구하는지에 대해 배울 것입니다.*


***






자 이제 위에서 배운 것들에 근거해서 강화 학습의 목적 함수 (The Objective for Reinforcement Learning) 를 수학적으로 정의 해 보도록 하겠습니다 . 

![slide10](/assets/images/CS285/lec-4/slide10.png)
*Slide. 10.*

앞서 말했듯 강화 학습은 결국 "어떤 상태에서 어떤 행동을 해야 하는가?" 를 알려주는 `정책 (Policy)`을 학습하는 거고 이 정책을 directly(explicitly) 학습 할 수도 있으며, implicitly 학습 할 수도 있습니다.

먼저 explicitly 학습하는 것에 대해서 생각해 보죠.


우선 `Partially Observed MDP`가 아닌 `MDP`상황에서 생각 해 보도록 하겠습니다. 즉, state에 condition이 걸린 $$\pi_{\theta}(a \vert s)$$ 인 거죠. 
*Slide. 10.*에서 보면 심층 신경망 (Deep Neural Network)로 정책을 표현했기 때문에 learned parameter $$\theta$$는 신경망의 전체 파라메터와 같다는 걸 알 수 있습니다.


그림이 나타내는 바는 다음과 같습니다.

- 어떤 상태 $$s$$를 네트워크(정책)에 넣는다.
- 정책이 적절한 $$a$$를 뱉는다. (이는 정의하기 나름이기 때문에 이산적인 값일 수도 (softmax), 연속적인 값일 수도 있다.)
- $$s$$와 $$a$$를 상태 천이 행렬 $$T$$와 연산하여 다음 상태 $$s_{t+1}$$를 구한다. (상태가 변할 수도, 아닐 수도 있다.)

우리는 이렇게 구해진 일련의 궤적? (Trajectories)에 대한 확률 분포 (probability distribution)을 구할 수 있는데, 이는 아래와 같이 쓸 수 있다.

$$
p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t )
$$


```
위의 수식의 의미는 결국 "어떤 상태에서, 어떤 행동을 하고, 그럼 어떤 상태가 되고, 또 어떤 행동을하고 ... 결국 도달하는 상태는?" 을 의미하는 것 같습니다. 
```


Trajectories는 어떤 시점 $$T$$ (우리가 정한) 까지의 일련의 상태와 행동들인데, 이제 우리의 control problem은 이 일련의 상태와 행동들이 끝날 때까지 decision making을 계속 해야 하는 finite horizon 입니다. 
(곧 끝이 정해지지 않은 inifinite horizon version에 대해서도 생각 해 볼 것이지만 우선은 쉬운 이해를 위해 finite하다고 하겠습니다, 그리고 이러한 finite한 sequence를 하나의 에피소드 (episode)라고 합니다.) 


위의 수식은 어떠한 시작 시점 $$p(s_1)$$ 부터 chain rule을 이용해 간단하게 유도된 공식이며, chain rule은 일반적으로 과거의 모든 상태들을 참조 (조건부, conditional)해야 하는게 맞지만 우리는 Markov Property를 만족하는 상황에 대해서 생각을 하기로 했기 때문에 바로 직전의 상태만이 현재 상태를 결정 짓게 됩니다. ($$ p(s_{t+1} \vert s_t, a_t ) $$)


그리고 위의 수식을 간결하게 나타내기 위해서 앞으로는 $$p_{\theta} (\tau)$$ 라고 나타내도록 하겠습니다. 

$$
p_{\theta} (\tau) = p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t )
$$

이제 Trajectory까지 정의했으니 우리는 강화학습의 목적 함수를 아래와 같이 써볼 수 있겠습니다.

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim  p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] 
$$

수식이 나타내는 바는 Trajectory Distribution 하에 Expected Value인데요, 즉, 강화 학습의 목적식이 나타내는 것은 "Trajectory를 따를 때 매 순간 마다의 보상 값 (reward)들의 기대값의 합이 최대가 되도록 신경망을 업데이트 하는 것" 입니다.

여기서 기대값이 굉장히 중요한데, 이는 정책 (Policy)과 초기 상태 (Initial State)에 대한 분포 그리고 상태 천이 행렬 (State Transition Probability)의 `Stochasticity`를 해결 해 줍니다.

 
물론 이러한 목적 함수의 수식은 다양한 variation이 존재하며, 앞으로의 강의들에서 자세히 다룰 예정이라고 합니다.
어쨌든 이제 우리는 정책 함수를 학습할 수 있는 가장 간단한 버전의 수식을 정의하게 되었습니다.

```
Sergey 선생님이 잠깐 여기서 강의를 멈추고 (우리는 글을 읽는것을 멈춰!야겠죠) 수식의 의미를 곱씹어 봐야 한다고 합니다.
Trajectory란 과연 어떤 의미인가?,
이 경로(궤적)를 따라서 보상 함수에 대해서 기대값을 취한 다는건 과연 어떤 의미인지?,
학습이 되면서 파라메터가 변하는건 Trajectory에는 과연 어떤 영향을 주는지? 
등등 
(파라메터 Theta는 Trajectory와 Policy 모두에 영향을 주죠)

아무튼 이 부분이 가장 중요하며, 단박에 이해하기 어려운 부분이기 때문에 시간을 좀 들이는게 좋다고 합니다.
```

(개인적으로 기대값 (Expectation, $$\mathbb{E}$$)을 쓰는 이유는 우리가 "어떤 상태에서 어떤 행동을 하는가" 에는 굉장히 많은 옵션이 존재하고, 그렇기 떄문에 Trajectory도 굉장히 많을 텐데, 이렇게 다양하게 sampling할 수 있는 시나리오에 대해 모두 고려하겠다는 의미 같습니다.)




이번에는 우리가 정의한 Trajectory를 Markov Chain과 연관지어 생각을 해 보겠다고 합니다.

![slide11](/assets/images/CS285/lec-4/slide11.png)
*Slide. 11.*

현재 상태에서 다음 상태로, 현재 상태에서 어떤 행동을 하는지는 *Slide. 11.*의 하단에 있는 그림과 같겠죠.

원래 Markov Chain은 상태와 상태 천이 행렬만이 존재하나, 위의 그림은 상태 공간 (State Space)에 행동 공간 (Action Space) 이라는 개념이 추가된 (Augmented) 것이라고 볼 수 있습니다. 여기서 Action이라는 것은 State에 condition되어있기 때문에 State와 Action을 그룹지어 생각해 볼 수 있겠습니다.




![slide12](/assets/images/CS285/lec-4/slide12.png)
*Slide. 12.*

*Slide. 12.*에서 Augmented 된 State를 볼 수 있습니다. 
이제 후에 있을 여러 공식의 유도에서 수월하게 공식을 쓰기 위해 Objective Function을 조금 더 간단하게 써 보도록 하겠습니다.


우리는 이제 Trajectory Distribution이 Augmented Space (State + Action)를 가지는 Markov Chain을 따른 다고 했죠.

그렇기 때문에 $$p( (s_{t+1},a_{t+1}) \vert (s_t,a_t) )$$는 아래의 수식으로 표현이 가능합니다.

$$
p( (s_{t+1},a_{t+1}) \vert (s_t,a_t) ) = p(s_{t+1} \vert s_t, a_t) \pi_{\theta}(a_{t+1} \vert s_{t+1})
$$

(MDP Transitions 과 Policy의 곱으로 나타냄)


수식의 의미는 상태 $$s_t$$에서 정책을 따라 행동 $$a_t$$ 를 했을 때, p(s_{t+1} \vert s_t, a_t)에 따라서 상태가 변하거나 그대로이거나 하게 되고, 그 상태에서 또 어떤 행동을 할 확률 $$a_{t+1}$$ 입니다.


***

좀 더 생각해볼까요, 다음과 같은 4차원 격자 모양의 미로가 있다고 생각 해 보겠습니다.

![maze](/assets/images/CS285/lec-4/maze.png)
*Additive Fig.*


존재하는 State는 4개가 됩니다.

$$s_1,s_2,s_3,s_4$$

$$\mu_{t,i} = p(s_t=i)$$

Markov Chain에서는 State $$S$$와 Transition Operator $$T$$만이 존재했고 다음 상태는 현재 상태에만 의존하죠 $$p(s_{t+1} = i \vert s_t = j)$$.

그렇다면 현재 상태에서 다음 상태로 가는 확률은 아래와 같았습니다.

$$\vec{\mu_{t+1}} = T \vec{\mu_t}$$

여기서 존재할 수 있는 상태는 4개 이므로 어떤 시점의 상태를 나타내는 벡터는 $$\mu_t \in \mathbb{R}^4$$ 이죠.

그리고 Transition Probability (Operator)는 $$T \in \mathbb{R}^{4 \times 4}$$ 차원이 됩니다.

그렇다면 현재 상태에서 다음 상태로 갈 확률은 아래의 곱 처럼 나타낼 수 있는데,

$$

\begin{bmatrix}
S_1\\ 
S_2\\ 
S_3\\ 
S_4
\end{bmatrix}

=

\begin{bmatrix}
S_{11} & S_{12} & S_{13} & S_{14} \\ 
S_{21} & S_{22} & S_{23} & S_{24}\\ 
S_{31} & S_{32} & S_{33} & S_{34}\\ 
S_{41} & S_{42} & S_{43} & S_{44}
\end{bmatrix}


\begin{bmatrix}
S_1\\ 
S_2\\ 
S_3\\ 
S_4
\end{bmatrix}

$$

우리는 지금 State와 Action을 묶어서 생각하는 Augmented Markov Chain을 생각하기로 했기 때문에 `S1,S2,S3,S4` 4차원의 State Space와, `상,하,좌,우`로 움직일 수 있는 마찬가지로 4차원의 Action Space가 있다면 $$p(s_t,a_t)$$는 $$4\times4$$로 16차원이 됩니다. 그러므로 Transition Probability $$T$$도 $$\mathbb{R}^{16 \times 16}$$이 됩니다.


***

여기서 $$p(s_t,a_t)$$는 `State-Action Marginal`이라고 합니다.







![slide13](/assets/images/CS285/lec-4/slide13.png)
*Slide. 13.*

이를 이용해서 우리는 원래의 Objective 를 

***

<center>
"An expected value under the trajectory distribution of the sum of rewards"
</center>

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim  p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] 
$$

***

Linearity of Expectation 을 이용해 아래와 같이 나타낼 수 있습니다.

***

<center>
"The sum over time of the expected values under state actual marginal in markov chain of the reward of that time step"
</center>

$$
\theta^{\ast} = argmax_{\theta} \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p_{\theta}(s_t,a_t)}[r(s_t,a_t)]
$$

***

(Expectation의 선형성을 이용해 $$\sum$$을 밖으로 빼냈네요.) 


이런 수학적인 재배열이 쓸모없어 보일 수 있으나 우리가 이를 $$t=1$$ 부터 $$T$$까지가 정해진 `Finite Horizon Case`가 아니라 `Infinite Horizon Case`를 생각한다면 유용하다고 합니다.








![slide14](/assets/images/CS285/lec-4/slide14.png)
*Slide. 14.*


`Infinite Horizon Case`에서는 $$t$$가 $$\infty$$ 가 되기 때문에 이렇게 양수의 reward를 무한정 더하면 Objective 또한 $$\infty$$를 return하게 됩니다.그렇기 때문에 Objective를 finite하게 만들기 위해서 사용되는 몇 가지 방법이 있는데요, 보상의 총 합을 어떤 상수 time-step, T로 나누는 `average reward formulation`을 사용하는 방법같은게 있지만, 잘 사용되는 방법은 아니며 일반적으로 감쇠율 (discount factor, $$\gamma$$)를 적용한 total reward를 사용 하는 것으로 해결합니다.  


자 이제 우리가 어떻게 실제로 `Infinite Horizon Objective`를 정의하는지 알아보도록 하겠습니다.

$4p(s_{t+1},a_{t+1})$$는 이전 상태 $$p(s_t, a_t)$$에만 영향을 받는데, $$p(s_t, a_t)$$와 `State-Action Transition Operator`, $$T$$를 곱해서

$$
p((s_{t+1},a_{t+1}) \vert s_t, a_t) = T p(s_t, a_t)
$$

위와 같이 나타낼 수 있습니다.
그리고 더 일반적으로는 아래와 처럼 나타낼 수 있습니다.

$$
p((s_{t+k},a_{t+k}) \vert s_t, a_t) = T^k p(s_t, a_t)
$$
 
 
 
여기서 Lecturer는 질문을 하나 던지는데, 바로 "`State-Action Marginal`$$p(s_t,a_t)$$ 가 어떠한 고정된 분포, `즉 정상 확률 분포 (Stationary Distribution)`으로 수렴할 수 있느냐?" 입니다.

```
What is Stationary Distribution in Markov Chain?

Markov Chain은 ... 그래프 위에서 매 턴마다 정해진 확률에 따라 다른 노드로 이동하는 것과 같습니다.
...(중략)
그렇다면 확률이 정해져 있다면 동선에 특정한 패턴이 존재하지는 않을까요? 예를 들어, “100번 이동했다면 평균적으로 3번은 출발지점에 돌아올 것이다”와 같은 예측을 할 수도 있을 것 같습니다.

예 맞습니다. 항상은 아니지만 확률이 정해져 있으므로 “특정조건”을 만족할 때 일정한 패턴이 나타납니다. 어떤 지점에서 시작하더라도, 상태 사이를 충분히 많은 횟수 이동하게 되면 각 상태의 방문횟수의 비율이 일정한 값으로 수렴하게 됩니다. 다시 말하면, 상태들의 방문횟수의 비율이 특정 확률분포로 수렴하게 되고 이 분포를 stationary distribution이라 부릅니다.

```

(출처 : [Markov Chain Monte Carlo 샘플링의 마법](https://www.secmem.org/blog/2019/01/11/mcmc/))



`Stationary`하다는 정의는 아래와 같은데, 

$$
\mu = T \mu
$$

이는 어떤 $$\mu$$라는 상태에서 $$T$$를 곱해도, 즉 transition이 일어나도 제자리인 것을 의미하고, 이 때의 $$\mu$$를 `Stationary Disribution`이라고 하는 것입니다. 
그리고 여기에 `Ergodicity`나 `Chain being Aperiodic`등의 몇 가지 테크닉을 이용하면 Stationary Disribution이 존재한다는 것을 쉽게 보일 수 있다고 합니다.


러프하게 말해서 Chain이 Aperiodic 하다는 것은 말 그대로 반복되는 주기가 없다는 것이고 Ergodic 하다는 것은 어떤 상태든 다른 상태로 도달 할 수 있고 ($$s_1$$에서 $$s_2,s_3,\cdots$$ 등 어떤 다른 상태로 이동할 확률이 모두 0이 아님), 만약 ergodic 하지 않을 경우 MDP 에서 어디를 시작 점으로 하느냐가 굉장히 중요해지는 문제가 생기고 이럴 경우 Stationary Distribution은 존재하지 않게되기 때문에, 이는 중요한 특성 중 하나라고 합니다.   


어쨌든 몇 가지 가정을 깔면 우리는 Stationary Distribution이 존재한다는 것을 알 수 있고, 그게 무엇인지 해도 구할 수 있으며 ($$(T-I)\mu=0$$에 의해), 어떠한 초기 상태, $$\mu_1$$를 앞으로 충분히 진행 시행시키면, 결국 Stationary Distribution $$\mu$$ 로 수렴하게 되고, 그렇기 때문에 $$t \rightarrow \infty$$면 State-Action Marginal에 기대값을 취한 것의 합인 $$\sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p_{\theta}(s_t,a_t)}[r(s_t,a_t)]$$이 `Stationary Distribution terms`에 의해서 dominate되고 말게 됩니다.

![slide15](/assets/images/CS285/lec-4/slide15.png)
*Slide. 15.*

그러니까 만약 우리가 Stationary Distribution에 포함 되지 않는 $$\mu_1,\mu_2,\mu_3,\cdots$$ 로 시작해도, $$T \rightarrow \infty$$ 를 취하게 되면 결국 Stationary Distribution 분포를 생각해서 다시 나타낼 수 있는데,  이 말의 의미는 만약 우리가 `Average Reward Case`를 활용하여

$$
\theta^{\ast} = argmax_{\theta} \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p_{\theta}(s_t,a_t)}[r(s_t,a_t)]
$$

위의 수식을 아래 처럼 만들고,

$$
\theta^{\ast} = argmax_{\theta} \frac{1}{T} \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p_{\theta}(s_t,a_t)}[r(s_t,a_t)]
$$

$$T \rightarrow \infty$$를 취하면 이 값은 Stationary Distribution 하의 보상 값의 기대값이 됩니다.

$$
\theta^{\ast} = argmax_{\theta} \frac{1}{T} \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p_{\theta}(s_t,a_t)}[r(s_t,a_t)] \rightarrow \mathbb{E}_{(s,a) \sim p_{\theta}(s,a)}[r(s,a)]
$$


(위의 수식에서 샘플링을 하는 분포가 $$(s,a) \sim p_{\theta}(s,a)$$인데, $$p_{\theta}(s,a)$$가 나타내는 분포가 Stationary Distribution 입니다.)


이를 통해서 우리는 $$T \rightarrow \infty$$인 `Infinite Horizon Case`에 대한 적절한 Objective를 정의할 수 있게 된 겁니다.



***

- Infinite Horizon Case

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{(s,a) \sim p_{\theta}(s,a)}[r(s,a)]
$$

- Finite Horizon Case

$$
\theta^{\ast} = argmax_{\theta} \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p_{\theta}(s_t,a_t)}[r(s_t,a_t)]
$$


***





![slide16](/assets/images/CS285/lec-4/slide16.png)
*Slide. 16.*

마지막으로 대부분의 강화학습 알고리즘 관통하는 원리에 대해서 얘기를 하고 Subsection을 끝내려고 하는데, 
그것은 바로 "강화학습이란 언제나 Expectation, $$\mathbb{E}$$를 최적화 하는 task" 라는 겁니다. 

우리가 여태까지 강화학습이라는 것을, 가장 높은 보상값들을 얻어는 액션들을 고른다는 측면에서 얘기를 했지만,
우리가 놓쳐서는 안될 개념이 바로 기대 값 (Expectation, $$\mathbb{E}$$)입니다. 

즉 보상들에 대해서 언제나 기대 값을 취해야 한다는 건데요 (Expected Values of Rewards), 기대 값을 취한다는 것의 흥미로운 점은 동일한 분포들의 파라메터에 대해 연속적 (continuous) 으로 될 수 있다는 건데, 심지어 우리가 기대값을 취할 함수 (function) 가 매우 불연속 (discontinuous) 하더라도 그렇다는 점에 있습니다. 

바로 이러한 특성 때문에, 대부분의 강화 학습의 알고리즘들이 미분 불가능 (Non-Differentiable) 해 보이는 목적 함수 (Objective)에도 (예를 들어, 극단적으로  이기면 +1, 지면 -1에 대해서만 생각하는, 이진 보상을 가정하는 (Binary Rewards)의 경우),  경사 하강법 (Gradient Descent) 같은 매끄러운 최적화 기법들 (Smooth Optimization Methods) 을 사용해 최적화 할 수 있게 되는 겁니다. 


간단한 예시를 생각해 보도록 하겠습니다.

![nondiff_example](/assets/images/CS285/lec-4/nondiff_example.png){: width="50%"}
*Additive Fig.*


***

자율주행을 하는 차가 있다고 생각해 보겠습니다.
정상적으로 도로에서 주행하면 Reward는 +1을 리턴하고, 아니면 -1을 리턴합니다. 

- on the road : $$+1$$
- fall off the road : $$-1$$

차의 위치 (position)에 대한 함수로 최적화를 진행한다고 생각해 보도록 하겠습니다.

***

당연히 위의 보상 함수는 굉장히 불연속적이거나, 다소 미분 불가능 하기 때문에 차의 위치에 대해서 최적화 할 수 없습니다.
 
- $$r(x)$$ not smooth 

하지만 만약에, 우리가 어떠한 행동 (Action)에 대한 정책 (Policy)를 정의하면 (추상적으로? 떨어지고 안떨어지고를 확률적으로 고른다.)

- $$\pi_{\theta}(a=fall)=\theta$$, Bernoulli Dist

이는 파라메터 $$\theta$$를 가지는 베르누이 랜덤 변수 (Bernoulli Random Variable)라고 얘기할 수 있고, 그렇기에 $$\theta$$ 확률로 길에 stay, $$1-\theta$$ 확률로 떨어지게 됩니다.

여기서 재밌는 점이 바로 $$\pi_{\theta}$$에 관해서 기대값을 취하게 되면

- $$\mathbb{E}_{\pi_{\theta}}[r(x)]$$ smooth in $$\theta$$

신기하게도 연속적이게 되고, 이는 미분 가능합니다.

- $$\mathbb{E}_{\pi_{\theta}}[r(x)] = \theta (+1) + (1-\theta)(-1)$$.


이 점이 바로 앞으로 강화 학습 알고리즘을 설명할 때, 비록 매끄럽지 못 하거나 (non-smooth), 혹은 심지어 매끄럽지 못 하고 (non-smooth) 미분 불가능한 함수들에 대한 기대 값들인, 희박한 보상 함수 (Sparse Reward Function)를 갖더라도 최적화를 할 수 있는지에 대한 key가 되는 것입니다.

 






## <mark style='background-color: #fff5b1'> Algorithms </mark>

이제 본격적으로 강화 학습의 기본적인 알고리즘들에 대해서 알아보도록 하겠습니다.


강화학습의 알고리즘들은 계속 반복해서 말해왔던 것 처럼 결국 `Policy를 학습하는 것`이 목표이며, 이를 directly 최적화 할 수도 있고, 가치 함수 (Value Function) 등을 정의해서 이를 최적화 함으로써 implicitly 학습 할 수도 있습니다. 그렇기 때문에 대부분의 알고리즘들은 결국 비슷한 구조 (Anatomy)를 공유하게 되는데, 이는 세 가지 기본적인 파트로 이루어져 있습니다.

![slide18](/assets/images/CS285/lec-4/slide18.png)
*Slide. 18.*

***

- Orange Block : `Generate Samples` (i.e. run the policy) $$\rightarrow$$ 강화학습은 시행 착오 (Trial and Error)를 통해 학습을 하는 알고리즘인데, `시행 (Trial)` 이라는 것은 여기서 현재 가지고 있는 정책 (Policy)을 주어진 환경 (Environment)에서 돌리는 (run) 것입니다. 즉 MDP를 가지고 상호작용을 해서 샘플들을 모으는 것인데 (여기서 샘플은 Trajectory Distribution에서 Trajectories를 뽑는걸 의미함), 이를 후술하겠지만 `탐색 (Exploration)` 이라고 합니다. 때로는 우리가 학습할 정책을 이용해서 Trajectory를 샘플링하지 않고, 우리가 학습할 정책과 별개의 정책을 사용해서 Trajectory를 샘플링 할 때도 있습니다. (Off Policy Algorithm?)
- Green Block : `Fit a Model` / `Estimate the Return` $$\rightarrow$$ 이는 모델을 학습하는 것으로 모델은 말 그대로 `Model-based RL` 알고리즘의 환경에 대한 역학 (시스템 역학?, Dynamics, Model) 이 될 수도 있고, 가치 함수 (Value Function) 같은 Implicit Model이 될 수도 있으며, 이 단계에서는 현재 정책 (Current Policy)에 대해서 "얼마나 이 정책이 합리적인가?", "얼마나 잘 작용하는가?", 혹은 "어떤 종류의 보상 (Rewards)를 얻을 것인가?" 를 측정해 (estimate) 리턴하게 됩니다.
- Blue Block : `Improve the Policy` $$\rightarrow$$ 이는 실제로 정책 (Policy)을 더 좋은 방향으로 수정하는 파트입니다.

***

앞으로 후술할 알고리즘들은 모두 위의 세가지 요소를 반복하는 것들이 될것이며, 물론 어떤 알고리즘은 어떤 요소가 매우 간단하거나 복잡할 수도 있겠지만 큰 맥락은 비슷할 겁니다.


자 이제 강화학습이 학습되는 과정을 다시 한 번 생각해 보도록 하겠습니다.

일단 현재의 Policy를 돌려보면(run) 우리는 Sample Trajectories를 얻을 수 있죠. (`주황색 박스`)

![traject1](/assets/images/CS285/lec-4/traject1.png){: width="50%"}
*Additive. Fig.*

이 중에는 더 좋은 Trajectory가 있을 수 있겠죠?

![traject2](/assets/images/CS285/lec-4/traject2.png){: width="50%"}
*Additive. Fig.*

우리는 뭐가 좋은 Trajectory인지를 알기 위해서 각 Trajectory가 좋고 나쁜 걸 평가 (evaluate) 할 수 있고 (`초록색 박스`),

![traject3](/assets/images/CS285/lec-4/traject3.png){: width="50%"}
*Additive. Fig.*

좋다고 생각하는 Trajectories이 등장할 확률을 높히는 방향으로 (make good trajectory be more likely and bad trajectory be less likely) 파라메터를 학습하면 됩니다 (`파란색 박스`).

![traject4](/assets/images/CS285/lec-4/traject4.png){: width="50%"}
*Additive. Fig.*

이는 다음 챕터에서 다루게 될 `Policy Gradient Algorithm`의 굉장히 high-level의 컨셉입니다.

![slide19](/assets/images/CS285/lec-4/slide19.png)
*Slide. 19.*

`초록색 박스`는 *Slide. 19.*에서 보시는바와 같이 정말 단순히 매 상태 (state) 마다의 보상의 합 (sum of rewards) 이며, 이렇게 측정한 값이 얼마나 우리의 policy가 좋은가를 나타내고, `파란색 박스`는 Policy 파라메터에 대해 경사 하강법 (Gradient Descent)을 하는 거라고 생각하시면 됩니다.


다시 한 번 `Trial and Error 스타일의 강화학습`은 아래와 같습니다.

- Run policy 
- Get some trajectories
- Measure how good those Trajectories are
- Modify the policy to make the better trajectories have a higher probability

하지만 위와 다른 방식으로도 학습을 하는 방법이 있는데요, 바로 `Model-based RL` 입니다. 
이는 바로 `오차 역전파 (Error Backpropagation)`를 사용하는 방법입니다.

![slide20](/assets/images/CS285/lec-4/slide20.png)
*Slide. 19.*

이러한 방법론은 `초록색 박스`에서 `Model`을 학습하는데, 다시 말해 뭔가 정책과는 다른 `Neural Network (NN)`, $$f_{\phi}$$을 학습한다는 겁니다.

- learn $$f_{\phi}$$ such that $$s_{t+1} \approx f_{\phi}(s_t,a_t)$$

여기서 $$f_{\phi}$$는 $$(s_t,a_t)$$를 given으로 다음 상태(state)를 추론하는 네트워크를 의미하고 바로 `주황색 박스`에서 만들어진 데이터를 지도 학습 (Supervised Learning)을 통해 학습하는 겁니다. 즉 아예 다른 네트워크를 하나 더 가지게 되는것이고, 이런 경우는 초록색 박스가 이전에 단순히 보상의 합을 구하는 방법론보다는 복잡한 경우라고 생각할 수 있습니다. 
단순 합을 구하는 방법론은 몇 초 밖에 안걸리겠지만 네트워크를 통해 추론해서 학습하는 것은 몇 분이 걸리거나 자율 주행같이 고해상도 이미지를 이용하는 경우 몇 시간이 걸릴 수도 있습니다.
그리고 `파란색 박스` 에서는 $$\pi_{\theta}$$를 학습하기 위해서 $$f_{\phi}$$와 $$r$$을 통한 Error Backpropagation을 사용합니다.


(후에 이러한 Model-based RL에 대해서도 설명할테니 지금 잘 이해가 가지 않아도 너무 걱정 말라고 하네요.)

***

![rl_algorithms_9_12.png](/assets/images/CS285/lec-4/rl_algorithms_9_12.png)
*Additive. Fig. [A non-exhaustive, but useful taxonomy of algorithms in modern RL from OpenAI.](https://spinningup.openai.com/en/latest/index.html) 강화 학습 알고리즘은 크게 Model-Free RL, Model-based RL로 나눌 수 있다.*

***

마지막으로 이번 Subsection에서 생각해 볼 것은, 전체 "프로세스에서 가장 expensive한 파트는 과연 무엇이고, cheap한 파트는 어디일까?" 입니다.

![slide21](/assets/images/CS285/lec-4/slide21.png)
*Slide. 21.*

우선 첫 번째로 `주황색 박스`는 우리가 풀고자하는 문제가 어떤것이냐에 따라 cost가 갈립니다. 만약 우리가 현실 세계 (Real-World) 에서 데이터를 샘플링 하려고 한다면(i.e. using a real car, a real robot, a real-power grid or a real chemical plant), real-time으로 데이터(Trajectory)를 수집해야 하기 때문에 매우 expensive해 질 겁니다. 만약 매 학습 Iteration마다 수천 장의 샘플이 필요하다면 말도 못할 정도로 expensive 해지겠죠? 반면 `MuJoCo Simulator`를 통해서 샘플링을 하게 된다면 이는 별 문제가 되지 않을 겁니다. (Homework 1](http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw1.pdf) 참조) 

 
두 번째로 `초록색 박스` 또한 어떻게 학습을 할 것이냐에 따라서 그 cost가 다릅니다. 만약 아까 살펴본 대로 단순히 보상을 합해서 return 하는것으로 policy를 estimate하면 이는 굉장히 cheap하지만, $$s_{t+1} = f_{\phi}(s_t,a_t)$$ 같이 전혀 다른 네트워크 모델을 학습해야 한다면 매우 expensive할 수 있습니다. 후자의 경우 강화 학습의 `Inner Loop`로 꽤 규모가 큰 지도 학습을 돌려야 하기 때문이죠.


마지막으로 `파란색 박스`도 초록색 박스와 비슷한데, `Gradient Step을 한번만 진행` 해도 되느냐, 아니면 `policy와 model에 대해서 Error Backpropagation을 진행`해야 하느냐에 따라 cost가 크게 차이납니다.


곧 있으면 배우게 될 Policy를 implicitly 학습하는 `Q-Learning`의 경우에는 cost를 초록색 박스에 대부분 투자하고, 파란색 박스는 그저 `argmax`가 된다고 합니다.





## <mark style='background-color: #fff5b1'> Value Functions </mark>

이번에는 강화 학습 알고리즘을 디자인하고 강화 학습의 Objective에 대해서 개념적으로 생각해 보는 데 있어 수학적으로 매우 유용한 `가치 함수 (Value Function)`에 대한 개념에 대해 살펴보도록 하겠습니다.

![slide23](/assets/images/CS285/lec-4/slide23.png)
*Slide. 23.*

우선 다시 한 번 강화 학습의 Objective에 대해서 생각해 보도록 하겠습니다.

$$\mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_{t=1}^T r(s_t,a_t) ]$$

위의 수식은 "Trajectory Distribution에 대한 보상의 합의 기대값" 혹은 "시간에 따른 모든 State-Action Marginal에서의 기대 보상 (expected reward)의 합"
이라는 의미를 가지고 있죠.


이는 Expectation을 반복해서 (recursively) 쓰면 아래와 같이 다시 아래 처럼 쓸 수 있는데요,
이전에 Trajectory Distributio을 Chain Rule을 통해서 여러 분포의 곱으로 표현했던 것이 기억이 나실 겁니다.

$$
p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t )
$$

비슷한 방법으로 그 분포에 대해서 기대값을 여러 `기대값들을 나열해서 (Nested Expectation)` 표현할 수 있습니다. 

$$
\mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [r(s_1,a_1) + \qquad \qquad \qquad \qquad \qquad \qquad \qquad \vert s_1 ]
$$

$$
\mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [r(s_1,a_1) + \mathbb{E}_{s_2 \sim p(s2 \vert s1,a1}[ \qquad \qquad \qquad \qquad \vert s_1,a_1] \vert s_1 ]
$$

$$
\mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [r(s_1,a_1) + \mathbb{E}_{s_2 \sim p(s2 \vert s1,a1}[ \mathbb{E}_{a_2 \sim \pi(a_2 \vert s_2} [r(s2,a2) + \cdots \vert s_2] \vert s_1,a_1] \vert s_1 ]
$$

이 수식이 의미하는 바는 뭘까요?

가장 바깥쪽의 (outermost) 기대 값부터 살펴보겠습니다.

$$
\mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [r(s_1,a_1) + \qquad \qquad \qquad \qquad \qquad \qquad \qquad \vert s_1 ]
$$

가장 바깥쪽은 $$p(s_1)$$에 대한 기대값이고, 그 안에 있는 것은 현재 상태와, 정책에 따른 $$a_1$$의 분포 $$\pi(a_1 \vert s_1)$$ 에 대한 기대값이며, 우리가 $$s_1,a_1$$ 모두에 대한 기대 값을 갖고 있기 때문에 첫 번째 보상 값, $$r(s_1,a_1)$$을 정의할 수 있으며, 안에 있는 기대값 (Inner Expectation)이 $$s_1$$에 대한 조건부 였기 때문에 이를 표현해 준 겁니다.


이제 위의 수식에 존재하는 공백에 다른 보상 (Reward)들을 채워 넣으면서 더 전개를 해 보도록 하겠습니다. 

$$
\mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [r(s_1,a_1) + \mathbb{E}_{s_2 \sim p(s2 \vert s1,a1}[ \qquad \qquad \qquad \qquad \vert s_1,a_1] \vert s_1 ]
$$

위의 수식은 또 그 안에 $$s_1,a_1$$을 query로 써서 (given) 두 번째 상태 $$s_2$$에 대한 기대값을 추가한 것이며

$$
\mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [r(s_1,a_1) + \mathbb{E}_{s_2 \sim p(s2 \vert s1,a1}[ \mathbb{E}_{a_2 \sim \pi(a_2 \vert s_2} [r(s2,a2) + \cdots \vert s_2] \vert s_1,a_1] \vert s_1 ]
$$

이에 대한 $$a_2$$에 대한 기대값을 추가하고, 보상을 또 추가하고 이를 끝나는 시간 $$T$$ 까지 반복에 반복에 반복을 하면 됩니다.

***

- $$s_1$$ : state distribution $$p(s_1)$$에 대한 기대값
- $$a_1$$ : 그 state들에 대한 action distribution $$\pi(a_1 \vert s_1)$$에 대한 기대값 (policy로부터)
- $$s_2$$ : state-action distribution $$p(s_2 \vert s_1,a_1)$$에 대한 기대값 
- $$a_2$$ : action distribution $$\pi(a_2 \vert s_2)$$에 대한 기대값 (policy로부터)

$$\vdots$$ 

- $$s_T$$ : state-action distribution $$p(s_T \vert s_{T-1},a_{T-1})$$에 대한 기대값 
- $$a_T$$ : action distribution $$\pi(a_T \vert s_T)$$에 대한 기대값 (policy로부터)

***

자 이제 우리는 아래의 간단한 수식을 복잡하게 표현을 해 봤는데요, 이렇게 한 데에는 이유가 있습니다.

$$
\mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_{t=1}^T r(s_t,a_t) ]
$$

$$
\mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [r(s_1,a_1) + \mathbb{E}_{s_2 \sim p(s2 \vert s1,a1}[ \mathbb{E}_{a_2 \sim \pi(a_2 \vert s_2} [r(s2,a2) + \cdots \vert s_2] \vert s_1,a_1] \vert s_1 ]
$$

만약 우리가 $$ r(s_1,a_1) + \mathbb{E}_{s_2 \sim p(s2 \vert s1,a1}[ \mathbb{E}_{a_2 \sim \pi(a_2 \vert s_2} [r(s2,a2) + \cdots \vert s_2] \vert s_1,a_1] $$ 부분을 안다면, 그러니까 이 함수를 안다면 어떨까요? 
우리는 이 부분을 다음과 같이 정의할 수 있습니다.


$$ 
Q(s_1,a_1) = r(s_1,a_1) + \mathbb{E}_{s_2 \sim p(s2 \vert s1,a1}[ \mathbb{E}_{a_2 \sim \pi(a_2 \vert s_2} [r(s2,a2) + \cdots \vert s_2] \vert s_1,a_1] 
$$

그렇다면 우리는 원래의 Objective를 아래와 같이 다시 간단하게 표현할 수 있습니다.

$$
\mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_{t=1}^T r(s_t,a_t) ] = \mathbb{E}_{s_1 \sim p(s_1)} [ \mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [Q(s_1,a_1) \vert s_1] ]
$$

만약 우리가 $$Q(s_1,a_1)$$을 안다면 우리는 첫 번째 time-step 에서 Policy를 최적화 (Optimizing) 하는 것은 굉장히 쉬워집니다.
즉 우리는 바로 위의 수식에서 정책 $$\pi(a_1 \vert s_1)$$를 고를 때, 기대 값 $$\mathbb{E}_{a_1 \sim \pi(a_1 \vert s_1} [Q(s_1,a_1) \vert s_1]$$을 가장 크게 하는 정책을 고르면 되는 겁니다. 
즉 모든 action을 테스트해보고 100프로 확률로 그걸 고르면 되는거죠.


다시 말해 이렇게 가장 큰 Q값을 뱉는 정책에 최대의 확률을 부여하는, argmax를 취하는 겁니다.

![slide24](/assets/images/CS285/lec-4/slide24.png)
*Slide. 24.*

이를 조금 더 일반화 해 보고자, `큐 함수 (Q-Function)`를 *Slide. 24.*의 최상단에 있는 수식 처럼 나타낼 수 있습니다.
즉 $$s_t$$ 상태에서 정책 $$\pi$$를 따라 어떤 $$a_t$$를 취함으로써 얻는 보상의 "기대값"들의 총 합 입니다.


이와 비슷한 개념으로 `가치 함수 (Value Function)` 라는 것도 정의할 수 있는데요, 

$$
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'} \vert s_t, a_t]
$$

$$
V^{\pi}(s_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t]
$$

이는 큐 함수와 다르게 $$s_t$$에 대해서만 condition되어 있다는 점에서 차이가 있습니다. 
가치 함수가 시사하는 바는, 만약 우리가 $$s_t$$에서 시작 한다면, 여기서 policy를 `전개 (roll out)`해서 총 보상 (total reward)을 얻게 되는거죠.


가치 함수는 또 아래 처럼 쓸 수도 있는데요,

$$
V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi_{a_t \vert s_t}} [Q^{\pi}(s_t,a_t)]
$$

이는 action들에 대한 큐 함수의 기대 값을 나타냅니다.

마지막으로 여기서 얻을 수 있는 것은 

$$
\mathbb{E}_{s_1 \sim p(s_1)} [V^{\pi} (s_1)]
$$

$$s_1$$에서의 가치 함수에 대한 기대 값으로 Objective의 최종적인 폼을 나타낼 수 있다는 겁니다.






자 이제, `큐 함수 (Q-Function)`과 `가치 함수 (Value-Functions)`이 어디에 사용하며, 왜 좋은지에 대해서 얘기해보도록 하겠습니다.

![slide25](/assets/images/CS285/lec-4/slide25.png)
*Slide. 25.*

2슬라이드 전쯤에 이미, 우리가 큐 함수를 가지고 있다면, 첫 번째 time-step에서 더 나은 Policy를 선택할 수 있다는 걸 알 수 있었습니다.


***

- `Idea 1` : If we have policy $$\pi$$, and we know $$Q^{\pi}(s,a)$$, then we can `improve` $$\pi$$ :
  - Set $$\pi'(a \vert s) = 1$$ if $$a = argmax_a Q^{\pi}(s,a)$$ (가장 큰 가치함수 값을 리턴하는 행동에 확률을 1로 부여함) 
  - This policy is at least as good as $$\pi$$ (and probably better)! (이렇게 argmax로 선택된 행동의 정책은 이전의 정책보단 좋을 것임)
  - and it doesn't matter what $$\pi$$ is (정책이 뭐였던 상관없이 더 나은 정책으로 업데이트 할 수 있음)

***


우리가 어떻게 더 나은 Policy를 구하는 방법은 어떤 상태에서 선택 가능한 모든 행동 $$a$$에 대해서 가치 함수를 다 돌려보고, 이 중에 가장 큰 값을 리턴하는 $$a$$에 확률 1을 부여하는 거죠. (확률의 합은 당연하게도 1)

우리는 이러한 행위를 첫 번째 time-step에만 적용하지 않고 모든 time-step에 대해서 적용할 수 있습니다.


이게 바로 `Policy Iteration` 이라 불리는 방법론의 기본적인 매커니즘이며 `큐 러닝 (Q-Learning)`에 사용되는 알고리즘이지만, 지금 이해가 가지 않더라도 나중에 다시 설명할 것이기 때문에 너무 걱정하지 말라고 합니다.


큐 함수와 가치 함수를 사용하는 또 다른 아이디어가 아래 있는데요, (이 또한 다음 lecture에서 설명하니 감만 잡으라고 합니다.) 

***

- `Idea 2` : Compute gradient to increase probability of good actions a: (여러 선택 가능한 행동 옵션들 중에서 좋은 행동이라고 할만한 것의 확률을 더 높히고, 아닌것들은 내림)  
  - If $$Q^{\pi}(s,a) > V^{\pi}(s)$$, then $$a$$ is better than average
  - Modify $$\pi(a \vert s)$$ to increase probability of $$a$$ if $$Q^{\pi}(s,a) > V^{\pi}(s)$$

***

직관적으로 $$Q^{\pi}(s,a) > V^{\pi}(s)$$일경우, "$$a$$라는 행동이 평균보다 좋을 것"이라는 것은  $$V^{\pi}(s)$$가 현재 상태에서 가능한 모든 행동들에 대해 (현재 정책을 사용해) 기대값을 취하는, 그야말로 평균적인 상황을 가정하는 것이기 때문에, 이것보다 점수가 높다는 것은 그 행동이 평균보다 낫다는 겁니다. 그렇기 때문에 $$Q^{\pi}(s,a) > V^{\pi}(s)$$인 $$a$$의 확률을 올리는 (여기서는 1로 만들지는 않습니다.) 방향으로 $$\pi$$의 파라메터를 수정하는 겁니다 (경사 하강법 (gradient descent) 사용).


이 두 번째 아이디어는 강화 학습에서 매우 매우 매우 (very very very) 중요하기 때문에, 앞으로 `Model-Free RL` 알고리즘을 논할 때 지속적으로 revisit하면서 앞으로 강의를 전개할거라고 합니다.




![slide26](/assets/images/CS285/lec-4/slide26.png)
*Slide. 26.*

위의 세가지 박스로 묘사된 `Anatomy of a RL Algorithm` 기억 나시죠?

여기서 초록색 박스는 방금까지 논했던 `큐 함수를 사용 (Fit)`하거나 `큐 함수나 가치 함수를 학습하는 (Learn)` 것으로 볼 수 있으며, 근본적으로 이러한 큐 함수와 가치 함수가 바로 `얼마나 현재 가지고 있는 정책이 좋은가?`를 평가하는 함수라고 할 수 있습니다. 그리고 이를 이용해서 파란색 박스에서 정책을 업데이트 하는겁니다.






## <mark style='background-color: #fff5b1'> Types of Algorithms </mark>

이번에는 강화 학습 알고리즘의 여러 타입들에 대해서 빠르게 훑어보는 (whirlwind tour) 시간을 갖겠다고 합니다.

![slide28](/assets/images/CS285/lec-4/slide28.png)
*Slide. 28.*

`정책 경사 (Policy Graidnet) 알고리즘`들은 아래의 Objective를 직접적으로 $$\theta$$에 대해 미분한 뒤 이를 이용해서 경사 하강법 (gradient descent)를 사용해 학습합니다.

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim  p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] 
$$

`가치함수 기반 (Value-based) 방법론`들은 Optimal Policy를 위해 뉴럴 네트워크로 모델링 된 가치 함수나 큐 함수를 추정해 정책을 implicitly 학습합니다.


`Actor-Critic 방법론`은 위의 두 알고리즘의 hybrid 버전입니다. 이는 큐 함수나 가치 함수를 배우고, 이를 더 나은 정책 경사(?)를 계산하는 데 사용해서 정책을 업데이트 합니다.


마지막으로 `Model-based RL 알고리즘`은 `Transition Model (상태 천이 행렬, $$T$$)`을 추정하고 이를 Explicit Policy 없이 Planning 하는 데 사용하거나, 아니면 그 자체를 Policy를 학습하는 데 사용합니다. 물론 이 상태 천이 행렬이 사용되는 다양한 방법론들이 존재합니다.



먼저 Model-based RL Algorithm에 대해서 좀 더 얘기해보자면,

![slide29](/assets/images/CS285/lec-4/slide29.png)
*Slide. 29.*

Model-based RL Algorithm은 슬라이드의 초록색 박스에서 일반적으로 $$p(s_{t+1} \vert s_t a_t)$$를 배우는 것을 포함하게 됩니다.
당연히 NN으로 모델링 되어있고 $$s_t,a_t$$를 입력으로 (given) $$s_{t+1}$$에 대한 확률 분포를 출력하거나 `Deterministic Model`이라면 $$t+1$$상태를 directly 출력하게 됩니다.
파란색 박스에는 몇 가지 다른 옵션들이 있는데요,

![slide30](/assets/images/CS285/lec-4/slide30.png)
*Slide. 30.*

- Just use the model to plan. (no policy)

예를 들어 어떤 규칙으로 체스 게임이 굴러가는지를 배우거나, `몬테카를로 트리 서치 (Monte Carlo Tree Search, MCTS)`같은 `Discrete Planning Algorithm`를 체스를 하는 데 사용하거나, 로봇을 위한 `Physics of a Continuous Environment`를 배울고 어떠한 이를 `Optimal Control`이나 `Trajectory Optimization Procedure`를 사용해서 로봇을 조종할 수 있습니다.


- Backpropagate gradients into the policy

이 경우에는 
학습된 모델을 


![slide31](/assets/images/CS285/lec-4/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-4/slide32.png)
*Slide. 32.*

![slide33](/assets/images/CS285/lec-4/slide33.png)
*Slide. 33.*






### <mark style='background-color: #dcffe4'> Tradeoffs Between Algorithms </mark>

![slide35](/assets/images/CS285/lec-4/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-4/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-4/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-4/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-4/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-4/slide40.png)
*Slide. 40.*






### <mark style='background-color: #dcffe4'> Examples of Algorithms </mark>

강의에 마지막 부분에서는 Deep RL 알고리즘의 몇가지 예시를 설명합니다. 
이 알고리즘들은 당연히 다른 챕터에서 다룰 거라고 얘기합니다. 
해당 예제들의 비디오들을 따로 이 글에 첨부하지 않을 것이므로 궁금하신 분들은 ([Lecture 4, part 6](https://www.youtube.com/watch?v=hfj9mS3nTLU&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=20))에서 확인하시기 바랍니다

![slide42](/assets/images/CS285/lec-4/slide42.png)
*Slide. 42.*

*Slide. 42.*에서는 가치 함수를 학습하는 방법론부터 (implicitly learing Policy), 정책을 바로 학습하는 정책 경사 방법론들 (directly learing Policy), 그리고 이 둘의 중간인 (hybrid) Actor-Critic 알고리즘, 그리고 상태 천이 행렬 (Transition Probability, $$s_t \rightarrow s_{t+1}$$)을 학습하면서 정책을 학습하는 Model-based RL 까지 전반적으로 설명 했습니다.

***

그래도 다시 한 번 적어보도록 하겠습니다.

- Value Function Fitting Methods
  - DQN (Q-Learning)
  - TD Learning (Temporal Difference Learning)
  - Fitted Value Iteration
- Policy Gradient Methods
  - REINFORCE
  - Natural Policy Gradient
  - TRPO (True Region Policy Optimization)
  - PPO (Proximal Policy Optimization)
- Actor-Critic Algorithms
  - A3C (Asynchronous Advantage Actor-Critic)
  - SAC (Soft Actor-Critic)
  - DDPG (Deep Deterministic Policy Gradient)
- Model-based RL Algorithms
  - Dyna
  - Guided Policy Search
  - MPPO
  - SVG (Stochastic Value Gradient)

등등

***

![slide43](/assets/images/CS285/lec-4/slide43.png)
*Slide. 43.*

위의 예제는 아주 유명한 2013년에 출판된 [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)인데요, Policy를 raw한 픽셀로부터 학습한 DQN의 결과물 입니다. CNN (Convolutional Neural Network)을 사용한 가치 함수 기반 방법론인 Q-Learning으로 학습했는데요, NN을 사용해서 모델링한 $$Q(s,a)$$을 estimate하는 것을 학습하기 때문에 Deep Q-Learning 즉 `DQN`이라고 불리게 되는 알고리즘을 제안하게 되었습니다. 아타리 게임은 이산적인 행동 확률 분포 (Discrete Action Space)를 사용합니다. 

![slide44](/assets/images/CS285/lec-4/slide44.png)
*Slide. 44.*

다음 예제는 2015년에 나온 로보틱스 관련 논문인 [End-to-End Training of Deep Visuomotor Policies](https://arxiv.org/pdf/1504.00702)인데요, `Guided Policy Search`라는 Model-based RL Algorithm을 제안한 논문입니다. 

![slide45](/assets/images/CS285/lec-4/slide45.png)
*Slide. 45.*
 
그 다음은 Policy Gradient의 예제인데요, 마찬가지로 예제의 논문 2015년에 제안된 논문으로, 제목은 [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438) 입니다. 논문에서 사용된 방법론은 TRPO등을 혼합해서 사용한(?) `Actor-Critic Algorithm`으로 휴머노이드 로봇이 어떻게 걷는지를 학습시키는 논문이었습니다.

![slide46](/assets/images/CS285/lec-4/slide46.png)
*Slide. 46.*

마지막으로 [QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation](https://arxiv.org/pdf/1806.10293)를 소개하는데, 이는 2018년에 제안된 논문이고 (Lecturer인 Sergey Levine이 교신저자), 물건을 집는 로봇 (Grasping Robot)을 학습하는 걸 목적으로 제안된 논문이라고 합니다. 이 또한 아타리 논문과 다르지 않게 (다만 Continuous Action Space를 다루기 위해서 좀 수정한) `Q-Learning`으로 학습했다고 합니다.
 





## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [2015 UCL Course on RL from David Silver](https://www.davidsilver.uk/teaching/)












