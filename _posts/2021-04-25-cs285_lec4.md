---
title: (미완) Lecture 4 - Introduction to Reinforcement Learning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 4의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=jds0Wh9jTvE&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=11)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 강의의 주제는 "Introduction to Reinforcement Learning" 입니다. 본격적으로 강화 학습이 무엇인지에 대한 정의와 컨셉에 대해서 알아보는 챕터 입니다. 

![slide1](/assets/images/CS285/lec-4/slide1.png)
*Slide. 1.*


## <mark style='background-color: #fff5b1'> Definitions </mark>

2장에서 배웠던 걸 다시 리마인드 하면서 챕터가 시작되는데요,

![slide3](/assets/images/CS285/lec-4/slide3.png)
*Slide. 3.*

다 배웠던 것이므로 글에서는 넘어가도록 겠습니다. 강화학습의 목표는 `정책 (Policy)` $$\pi_{\theta}(a_t \vert s_t)$$를 학습하는 것 입니다. 심층 강화학습 에서는 이 정책이 `심층 신경망 (Deep Neural Network)`으로 디자인 되어 있으며, 이러한 정책을 하는 방법은 크게 두 가지로 나눌 수 있습니다.

- `Policy`를 directly 하는 방법 
- 다른 가치 함수 (`Value Function`) 같은 Object를 통해 implicitly 학습하는 방법

이러한 방법론들에 대해서 앞으로 알아보게 될겁니다.


했던 얘기를 그래도 조금만 살펴보자면, Observation과 State를 구분짓는 가장 큰 차이점은 `Markov Property`를 만족하는가 였고, Observation은 full State를 추론 하는 데 필요한 정보를 모두 포함하고 있을수도, 아닐 수도 있는, State의 어떠한 Stochastic Function 이라고 할 수 있엇습니다.


앞으로 강의에서는 State에 접근할 수 있는 `Fully Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert s_t $$) 과 Observation에만 접근할 수 있는 `Partially Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert o_t $$) 에 대해서 알아볼겁니다.

![slide4](/assets/images/CS285/lec-4/slide4.png)
*Slide. 4.*

*Slide. 4.*도 그냥 지난 시간에 배운 내용의 Recap 인데요, Imitation Learning이란 사람의 행동을 그대로 매 순간 순간 Supervised Learning 하면서 따라가는 거였죠. Observation - Action tuple을 학습 셋으로 사용해서 정책을 학습했습니다.


하지만 이번 장에서의 목표는 이러한 Expert data 없이 정책을 학습을 하는 알고리즘들에 대해서 알아보는 것이 될 겁니다.


![slide5](/assets/images/CS285/lec-4/slide5.png)
*Slide. 5.*

그러기 위해서 우리는 매 장면 장면 어떤 행동을 하는지를 Supervised Learning 하지 않기 위한 새로운 Objective를 도입하게 되는데요, 이를 `보상 함수 (Reward Function)` 라고 합니다. 이제 매 장면마다 어떤 행동을 해야 하는지 일일히 Labeling 되어 있지 않기 때문에 어떤 상황에서 어떤 행동을 하는 것이 옳은지를 판단해야 하는데 그것을 보상 함수가 말해주게 되는거죠.

$$r(s,a)$$ 

위와 같은 수식으로 나타내는 보상 함수는 일반적으로 $$s,a$$ 두 가지 모두에 대해서 condition 되지만 $$s$$만을 condition할 때도 있습니다. 함수가 리턴하는 결과값은 벡터가 아닌 스칼라 값 입니다.


자율 주행을 하는 경우 사고 없이 잘 달리면 매 순간 좋은 값 (양수) 을 얻고, 사고가 나게 되면 그 순간에는 좋지 않은 값 (음수) 을 얻겠죠. 
하지만 강화 학습에서는 현재 상태의 보상 만을 고려하여 어떤 행동을 할 지를 선택하지 않습니다.

그러니까 자율 주행을 하는데 현재 시점에 아무도 없어서 속도를 최대한으로 내서 빠르게 달리는 것이 결국에는 사고 위험성이 높아서 나중에는 최악의 상황 (교통사고)를 낼 수도 있다는걸 사람이 알 듯, 기계도 현재 "빠르게 달리는게 목표에 빨리 도달하겠지?" 뿐만 아니라 미래의 교통 사고 가능성까지 생각하여 적당한 속도로 달리게 한다는 거죠.


즉 강화 학습은 `미래 보상 (Future Rewards)`을 필수적으로 생각하는 것이 중요합니다. 

```
Sergey는 이를 'The heart of the decision making problems' 혹은 'The heart of the reinforcement learning problems' 이라고 하네요.
```

이러한 term들을 가지고 

- State ($$s_t$$)
- Action ($$a_t$$)
- Reward ($$r(a_t \vert s_t)$$)
- State Transition Probability

우리는 `Markov Decision Process (MDP)`(또는 `Decision Process on Markovian State`라는 것)을 정의할 수 있게 됩니다. 


자 이제 MDP에 대한 Full formal definition을 빌드업 하도록 하겠습니다.

![slide6](/assets/images/CS285/lec-4/slide6.png)
*Slide. 6.*

먼저 `Markov Chain`에 대해서 알아볼 건데요, 이는 Stochastic Processes 분야를 개척한 [Andrey Markov](https://en.wikipedia.org/wiki/Andrey_Markov) 라는 수학자가 이름 지었습니다.

(Markov Chain, Markov Reward Process (MRP), Markov Decision Process (MDP) 모두 비슷하지만 조금씩 다른 개념 입니다.)


Markov Chain의 개념은 굉장히 간단한데요, 우선 마르코프 체인은 두 가지로 구성되어 있습니다.

$$
M=\{S,T\}
$$

- $$S$$ : State Space $$\rightarrow$$ (states $$s \in S$$)
- $$T$$ : Transition Operator (or Transition Probability, Dynamics Function) $$\rightarrow$$ $$p(s_{t+1} \vert s_t)$$

Transition Operator는 $$s_t$$ 에서 $$s_{t+1}$$로 상태가 바뀔 확률을 나타내는데요, 이 것을 `Operator`라고 하는 이유는 다음과 같기 때문입니다.

$$
let \space \mu_{t,i} = p(s_i = i)
$$

$$\vec{\mu_t} $$ is a vector of probabilities 

$$
let \space T_{i,j} = p(s_{t+1}=i \vert s_t = j)
$$

then $$\vec{mu_{t+1}} = T \vec{\mu_t}$$


Discrete한 상태가 여러 개 있다면, 어떤 time-step t에서의 상태를 벡터로 나타낼 수 있을 것이며, 해당 state($$s_t$$\j)에서 다른 state($$s_{t+1}=i$$) 로 이동할 확률은 바로 `Transition Probability Matrix`와 `Vector of State Probability`를 행렬 곱하는 `연산 (Operate)`이 되기 때문입니다.


위의 개념들을 이용한 Markov Chain의 Graphical Model은 *Slide. 6.*의 하단에 잘 나타나 있습니다. 
여기서 그래프의 edge가 나타내는게 바로 Transition Probability가 되며, Markov Chain은 Markov Property를 만족하게 됩니다.

$$
p(s_{t+1} \vert s_t, s_{t-1}, s_{t-2}, s_{t-3}, \cdots) = p(s_{t+1} \vert s_t)
$$

하지만 Markov Chain 만으로는 decision making problem을 해결할 수 없는데요, 바로 행동 (Action), $$a_t$$에 대한 개념이 없기 때문입니다.

![slide7](/assets/images/CS285/lec-4/slide7.png)
*Slide. 7.*

MDP는 위의 Markov Chain 개념에 몇가지 (additional objects)를 더 추가한 것인데요, 1950년대에 [Richard Bellman](https://en.wikipedia.org/wiki/Richard_E._Bellman)에 의해 제안된 개념입니다.

$$
M=\{S,A,T,r\}
$$

Markov Chain과 비교해서 `Action Space` 와 `Reward Function` $$A,r$$이 추가된거죠. (MRP에서는 A가 없이 r만 있는데, 여기서는 다루지 않는 것 같으니 생략하겠습니다.)
Action Space는 State Space와 마찬가지로 이산적 (discrete) 일 수도, 연속적 (continuous) 일 수도 있습니다. 


이제 Graphical Model에는 Action이 추가됐으며, Transition Operator에도 $$a_t$$가 추가로 condition된 것을 알 수 있습니다. 
(당연히 Markov Property를 만족해야 하니, $$s_t,a_t$$ 에 의해서만 다음 상태 $$s_{t+1}$$이 결정됩니다.)

그리고 Transition Operator는 더 이상 Matrix (2-Dimensional)이 아닌 Tensor (3-Dimensional) 이 됩니다. 
(왜냐하면 이제는 현재 상태, 다음 상태, 현재 행동 세 가지를 고려해야 하기 때문입니다.)


Markov Chain에서와 마찬가지로 선형 대수의 개념을 이용해 다음 상태에 대한 확률을 Transition Operator와 현재 상태, 현재 행동의 확률을 가지고 있는 벡터들과의 연산을 통해 구할 수 있습니다. (*Slide. 7.* 하단 참조)

$$
let \space \mu_{t,j} = p(s_t=j)
$$

$$
let \space \xi_{t,k} = p(a_t=k)
$$

$$
let \space T_{i,j,k} = p(s_{t+1}=i \vert s_t=j, a_t=k)
$$

$$
\mu_{t+1,i} = \sum_{j,k} T_{i,j,k} \mu_{t,j} \xi_{t,k} 
$$


Reward Function은 



![slide8](/assets/images/CS285/lec-4/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-4/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-4/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-4/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-4/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-4/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-4/slide14.png)
*Slide. 14.*


![slide15](/assets/images/CS285/lec-4/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-4/slide16.png)
*Slide. 16.*







## <mark style='background-color: #fff5b1'> Algorithms </mark>

![slide18](/assets/images/CS285/lec-4/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-4/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-4/slide20.png)
*Slide. 19.*

![slide21](/assets/images/CS285/lec-4/slide21.png)
*Slide. 21.*







## <mark style='background-color: #fff5b1'> Value Functions </mark>


![slide23](/assets/images/CS285/lec-4/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-4/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-4/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-4/slide26.png)
*Slide. 26.*






## <mark style='background-color: #fff5b1'> Types of Algorithms </mark>

![slide28](/assets/images/CS285/lec-4/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-4/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-4/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-4/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-4/slide32.png)
*Slide. 32.*

![slide33](/assets/images/CS285/lec-4/slide33.png)
*Slide. 33.*






### <mark style='background-color: #dcffe4'> Tradeoffs Between Algorithms </mark>

![slide35](/assets/images/CS285/lec-4/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-4/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-4/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-4/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-4/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-4/slide40.png)
*Slide. 40.*






### <mark style='background-color: #dcffe4'> Examples of Algorithms </mark>

![slide42](/assets/images/CS285/lec-4/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-4/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-4/slide44.png)
*Slide. 44.*

![slide45](/assets/images/CS285/lec-4/slide45.png)
*Slide. 45.*

![slide46](/assets/images/CS285/lec-4/slide46.png)
*Slide. 46.*








## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)














