---
title: (미완) Lecture 4 - Introduction to Reinforcement Learning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 4의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=jds0Wh9jTvE&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=11)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 강의의 주제는 "Introduction to Reinforcement Learning" 입니다. 본격적으로 강화 학습이 무엇인지에 대한 정의와 컨셉에 대해서 알아보는 챕터 입니다. 

![slide1](/assets/images/CS285/lec-4/slide1.png)
*Slide. 1.*


## <mark style='background-color: #fff5b1'> Definitions </mark>

2장에서 배웠던 걸 다시 리마인드 하면서 챕터가 시작되는데요,

![slide3](/assets/images/CS285/lec-4/slide3.png)
*Slide. 3.*

다 배웠던 것이므로 글에서는 넘어가도록 겠습니다. 강화학습의 목표는 `정책 (Policy)` $$\pi_{\theta}(a_t \vert s_t)$$를 학습하는 것 입니다. 심층 강화학습 에서는 이 정책이 `심층 신경망 (Deep Neural Network)`으로 디자인 되어 있으며, 이러한 정책을 하는 방법은 크게 두 가지로 나눌 수 있습니다.

- `Policy`를 directly 하는 방법 
- 다른 가치 함수 (`Value Function`) 같은 Object를 통해 implicitly 학습하는 방법

이러한 방법론들에 대해서 앞으로 알아보게 될겁니다.


했던 얘기를 그래도 조금만 살펴보자면, Observation과 State를 구분짓는 가장 큰 차이점은 `Markov Property`를 만족하는가 였고, Observation은 full State를 추론 하는 데 필요한 정보를 모두 포함하고 있을수도, 아닐 수도 있는, State의 어떠한 Stochastic Function 이라고 할 수 있엇습니다.


앞으로 강의에서는 State에 접근할 수 있는 `Fully Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert s_t $$) 과 Observation에만 접근할 수 있는 `Partially Observed Reinforcement Learning Algorithm` ($$ \pi_{\theta} (a_t \vert o_t $$) 에 대해서 알아볼겁니다.

![slide4](/assets/images/CS285/lec-4/slide4.png)
*Slide. 4.*

*Slide. 4.*도 그냥 지난 시간에 배운 내용의 Recap 인데요, Imitation Learning이란 사람의 행동을 그대로 매 순간 순간 Supervised Learning 하면서 따라가는 거였죠. Observation - Action tuple을 학습 셋으로 사용해서 정책을 학습했습니다.


하지만 이번 장에서의 목표는 이러한 Expert data 없이 정책을 학습을 하는 알고리즘들에 대해서 알아보는 것이 될 겁니다.


![slide5](/assets/images/CS285/lec-4/slide5.png)
*Slide. 5.*

그러기 위해서 우리는 매 장면 장면 어떤 행동을 하는지를 Supervised Learning 하지 않기 위한 새로운 Objective를 도입하게 되는데요, 이를 `보상 함수 (Reward Function)` 라고 합니다. 이제 매 장면마다 어떤 행동을 해야 하는지 일일히 Labeling 되어 있지 않기 때문에 어떤 상황에서 어떤 행동을 하는 것이 옳은지를 판단해야 하는데 그것을 보상 함수가 말해주게 되는거죠.

$$r(s,a)$$ 

위와 같은 수식으로 나타내는 보상 함수는 일반적으로 $$s,a$$ 두 가지 모두에 대해서 condition 되지만 $$s$$만을 condition할 때도 있습니다. 함수가 리턴하는 결과값은 벡터가 아닌 스칼라 값 입니다.


자율 주행을 하는 경우 사고 없이 잘 달리면 매 순간 좋은 값 (양수) 을 얻고, 사고가 나게 되면 그 순간에는 좋지 않은 값 (음수) 을 얻겠죠. 
하지만 강화 학습에서는 현재 상태의 보상 만을 고려하여 어떤 행동을 할 지를 선택하지 않습니다.

그러니까 자율 주행을 하는데 현재 시점에 아무도 없어서 속도를 최대한으로 내서 빠르게 달리는 것이 결국에는 사고 위험성이 높아서 나중에는 최악의 상황 (교통사고)를 낼 수도 있다는걸 사람이 알 듯, 기계도 현재 "빠르게 달리는게 목표에 빨리 도달하겠지?" 뿐만 아니라 미래의 교통 사고 가능성까지 생각하여 적당한 속도로 달리게 한다는 거죠.


즉 강화 학습은 `미래 보상 (Future Rewards)`을 필수적으로 생각하는 것이 중요합니다. 

```
Sergey는 이를 'The heart of the decision making problems' 혹은 'The heart of the reinforcement learning problems' 이라고 하네요.
```

이러한 term들을 가지고 

- State ($$s_t$$)
- Action ($$a_t$$)
- Reward ($$r(a_t \vert s_t)$$)
- State Transition Probability

우리는 `Markov Decision Process (MDP)`(또는 `Decision Process on Markovian State`라는 것)을 정의할 수 있게 됩니다. 


자 이제 MDP에 대한 Full formal definition을 빌드업 하도록 하겠습니다.

![slide6](/assets/images/CS285/lec-4/slide6.png)
*Slide. 6.*

먼저 `Markov Chain`에 대해서 알아볼 건데요, 이는 Stochastic Processes 분야를 개척한 [Andrey Markov](https://en.wikipedia.org/wiki/Andrey_Markov) 라는 수학자가 이름 지었습니다.

(Markov Chain, Markov Reward Process (MRP), Markov Decision Process (MDP) 모두 비슷하지만 조금씩 다른 개념 입니다.)


Markov Chain의 개념은 굉장히 간단한데요, 우선 마르코프 체인은 두 가지로 구성되어 있습니다.

$$
M=\{S,T\}
$$

- $$S$$ : State Space $$\rightarrow$$ (states $$s \in S$$)
- $$T$$ : Transition Operator (or Transition Probability, Dynamics Function) $$\rightarrow$$ $$p(s_{t+1} \vert s_t)$$

Transition Operator는 $$s_t$$ 에서 $$s_{t+1}$$로 상태가 바뀔 확률을 나타내는데요, 이 것을 `Operator`라고 하는 이유는 다음과 같기 때문입니다.

$$
let \space \mu_{t,i} = p(s_i = i)
$$

<center>
$$\vec{\mu_t} $$ is a vector of probabilities 
</center>

$$
let \space T_{i,j} = p(s_{t+1}=i \vert s_t = j)
$$

<center>
then $$\vec{\mu_{t+1}} = T \vec{\mu_t}$$
</center>

Discrete한 상태가 여러 개 있다면, 어떤 time-step t에서의 상태를 벡터로 나타낼 수 있을 것이며, 해당 state($$s_t$$\j)에서 다른 state($$s_{t+1}=i$$) 로 이동할 확률은 바로 `Transition Probability Matrix`와 `Vector of State Probability`를 행렬 곱하는 `연산 (Operate)`이 되기 때문입니다.


위의 개념들을 이용한 Markov Chain의 Graphical Model은 *Slide. 6.*의 하단에 잘 나타나 있습니다. 
여기서 그래프의 edge가 나타내는게 바로 Transition Probability가 되며, Markov Chain은 Markov Property를 만족하게 됩니다.

$$
p(s_{t+1} \vert s_t, s_{t-1}, s_{t-2}, s_{t-3}, \cdots) = p(s_{t+1} \vert s_t)
$$

하지만 Markov Chain 만으로는 decision making problem을 해결할 수 없는데요, 바로 행동 (Action), $$a_t$$에 대한 개념이 없기 때문입니다.

![slide7](/assets/images/CS285/lec-4/slide7.png)
*Slide. 7.*

MDP는 위의 Markov Chain 개념에 몇가지 (additional objects)를 더 추가한 것인데요, 1950년대에 [Richard Bellman](https://en.wikipedia.org/wiki/Richard_E._Bellman)에 의해 제안된 개념입니다.

$$
M=\{S,A,T,r\}
$$

Markov Chain과 비교해서 `Action Space` 와 `Reward Function` $$A,r$$이 추가된거죠. (MRP에서는 A가 없이 r만 있는데, 여기서는 다루지 않는 것 같으니 생략하겠습니다.)
Action Space는 State Space와 마찬가지로 이산적 (discrete) 일 수도, 연속적 (continuous) 일 수도 있습니다. 


이제 Graphical Model에는 Action이 추가됐으며, Transition Operator에도 $$a_t$$가 추가로 condition된 것을 알 수 있습니다. 
(당연히 Markov Property를 만족해야 하니, $$s_t,a_t$$ 에 의해서만 다음 상태 $$s_{t+1}$$이 결정됩니다.)

그리고 Transition Operator는 더 이상 Matrix (2-Dimensional)이 아닌 Tensor (3-Dimensional) 이 됩니다. 
(왜냐하면 이제는 현재 상태, 다음 상태, 현재 행동 세 가지를 고려해야 하기 때문입니다.)


Markov Chain에서와 마찬가지로 선형 대수의 개념을 이용해 다음 상태에 대한 확률을 Transition Operator와 현재 상태, 현재 행동의 확률을 가지고 있는 벡터들과의 연산을 통해 구할 수 있습니다. (*Slide. 7.* 하단 참조)

$$
let \space \mu_{t,j} = p(s_t=j)
$$

$$
let \space \xi_{t,k} = p(a_t=k)
$$

$$
let \space T_{i,j,k} = p(s_{t+1}=i \vert s_t=j, a_t=k)
$$

$$
\mu_{t+1,i} = \sum_{j,k} T_{i,j,k} \mu_{t,j} \xi_{t,k} 
$$


![slide8](/assets/images/CS285/lec-4/slide8.png)
*Slide. 8.*

보상 함수 (Reward Function)은 상태 (state)와 행동 (action) 의 카티전 곱 (Cartesian Product) 을 어떠한 real value number로 매핑해주는 함수 입니다. 

$$
r \colon S \times A \rightarrow \mathbb{R}
$$

보상 함수는 아래와 같이 나타낼 수 있으며,

$$
r(s_t,a_t) 
$$

강화학습의 목적은 `sum of future rewards`를 Maximize 하는 것 이며, 이제 이에 대해 알아볼겁니다. 
( cf) maximize log-likelihood or minimize negative log-likelihood in Common Deep Learning Approaches )


![slide9](/assets/images/CS285/lec-4/slide9.png)
*Slide. 9.*

하지만 알아보기 전에 MDP를 조금 더 확장해서 `partially observed MDP`로 생각해 보도록 할건데요,



`Partially Observed MDP`는 MDP에 또 몇가지 개념이 추가된 것입니다.

$$
M=\{S,A,O,T,\varepsilon,r\}
$$

cf)

$$
M_{Markov \space Chain}=\{S,A\}
$$

$$
M_{MDP}=\{S,A,T,r\}
$$

이제 State Space, Action Space 외에 `Observation Space`, $$O$$와 `Emission Probability`, $$\varepsilon$$가 추가됐습니다.

- $$S$$ : State Space, state $$s \in S$$ (discrete or continuous)
- $$A$$ : Action Space, action $$a \in A$$ (discrete or continuous)
- $$O$$ : Observation Space, observation $$o \in O$$ (discrete or continuous)

Observation, $$o$$는 state에 depend하며, Emission Probability는 $$s_t$$ 에서 $$o_t$$를 관측할 확률을 나타냅니다 ($$p(o_t \vert s_t)$$).

일반적으로 Reward Function은 state와 action에 의거해 값을 산출하지만, `Partially Observed MDP`나 `palmdp`는  True State에 접근하지 않고, Observation에 근거해서 decision making을 합니다.





***

Markov Chain, MDP, MRP에 대해서 추가자료를 조금 첨부해 보도록 하겠습니다.


(이하 모든 이미지의 출처 : [2015 UCL Course on RL from David Silver](https://www.davidsilver.uk/teaching/))

- `Markov Chain`



![markov_chain1](/assets/images/CS285/lec-4/markov_chain1.png)
*Additive Fig. Markov Chain Example: Student Markov Chain. 평범한 대학생의 동선을 Markov Chain으로 나타낸 겁니다. (집에서는 잠만 자네요...)*

![markov_chain3](/assets/images/CS285/lec-4/markov_chain3.png)
*Additive Fig. Markov Chain의 Transition Probability Matrix (Operator)는 위와 같습니다. 이에 따라 상태가 변할 수도 아닐 수도 있습니다. (나중에 다루겠지만 우리가 이 행렬을 아는 경우는 일반적으로 없습니다. 게임에서 내가 몬스터를 공격했을 때 miss가 뜰지, 아닐지는 보통 모르니까요, 몇번 때려보고 통계를 내서 행렬로 쓰거나 할 수 있습니다.)*

![markov_chain2](/assets/images/CS285/lec-4/markov_chain2.png)
*Additive Fig. Markov Chain을 통해서 에피소드 (episode)를 몇 개 샘플링 할 수 있습니다.*



- `Markov Reward Process (MRP)`

![mrp1](/assets/images/CS285/lec-4/mrp1.png)
*Additive Fig. Markov Reward Process Example: Student MRP. 평범한 대학생의 동선을 MRP으로 나타낸 겁니다. Markov Chain과 다르게 Reward가 생겼죠.*


(아래는 곧 다룰거지만 미리 첨부하겠습니다.)

***

![mrp2](/assets/images/CS285/lec-4/mrp2.png)
*Additive Fig. 아직 얘기는 안했지만 강화 학습에서는 에피소드의 총 Reward를 고려하는데, 이 Reward에는 일반적으로 감가율 (discount factor, $$\gamma$$)를 적용합니다. 이는 현재 상태에서 행동을 취할 때 현재 부터 미래 가치를 전부 고려하기는 하지만 그래도 나비 효과처럼 나중에 생길 미래의 가치 보다는 현재의 가치에 중점을 두겠다는 의미를 내포합니다.*

![mrp3](/assets/images/CS285/lec-4/mrp3.png)
*Additive Fig. 감가율 (discount factor, $$\gamma$$)이 0일 경우 현재 가치만을 고려하게 됩니다. (오늘만 사는거죠...)*


![mrp4](/assets/images/CS285/lec-4/mrp4.png)
*Additive Fig. 감가율 (discount factor, $$\gamma$$)이 0.9일 경우 미래 가치를 적절하게 고려하게 됩니다.*

![mrp4](/assets/images/CS285/lec-4/mrp4.png)
*Additive Fig. 감가율 (discount factor, $$\gamma$$)이 1일 경우.*

***

- `Markov Decision Process (MDP)`


![mdp1](/assets/images/CS285/lec-4/mdp1.png)
*Additive Fig. Markov Decision Process Example: Student MDP.*


![mdp2](/assets/images/CS285/lec-4/mdp2.png)
*Additive Fig. Optimal Policy for Student MDP. 곧 Policy를 어떻게 학습하는지, 과연 Optimal Policy는 존재하며, 이는 어떻게 구하는지에 대해 배울 것입니다.*


***






자 이제 위에서 배운 것들에 근거해서 강화 학습의 목적 함수 (The Objective for Reinforcement Learning) 를 수학적으로 정의 해 보도록 하겠습니다 . 

![slide10](/assets/images/CS285/lec-4/slide10.png)
*Slide. 10.*

앞서 말했듯 강화 학습은 결국 "어떤 상태에서 어떤 행동을 해야 하는가?" 를 알려주는 `정책 (Policy)`을 학습하는 거고 이 정책을 directly(explicitly) 학습 할 수도 있으며, implicitly 학습 할 수도 있습니다.

먼저 explicitly 학습하는 것에 대해서 생각해 보죠.


우선 `Partially Observed MDP`가 아닌 `MDP`상황에서 생각 해 보도록 하겠습니다. 즉, state에 condition이 걸린 $$\pi_{\theta}(a \vert s)$$ 인 거죠. 
*Slide. 10.*에서 보면 심층 신경망 (Deep Neural Network)로 정책을 표현했기 때문에 learned parameter $$\theta$$는 신경망의 전체 파라메터와 같다는 걸 알 수 있습니다.


그림이 나타내는 바는 다음과 같습니다.

- 어떤 상태 $$s$$를 네트워크(정책)에 넣는다.
- 정책이 적절한 $$a$$를 뱉는다. (이는 정의하기 나름이기 때문에 이산적인 값일 수도 (softmax), 연속적인 값일 수도 있다.)
- $$s$$와 $$a$$를 상태 천이 행렬 $$T$$와 연산하여 다음 상태 $$s_{t+1}$$를 구한다. (상태가 변할 수도, 아닐 수도 있다.)

우리는 이렇게 구해진 일련의 궤적? (Trajectories)에 대한 확률 분포 (probability distribution)을 구할 수 있는데, 이는 아래와 같이 쓸 수 있다.

$$
p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t )
$$


```
위의 수식의 의미는 결국 "어떤 상태에서, 어떤 행동을 하고, 그럼 어떤 상태가 되고, 또 어떤 행동을하고 ... 결국 도달하는 상태는?" 을 의미하는 것 같습니다. 
```


Trajectories는 어떤 시점 $$T$$ (우리가 정한) 까지의 일련의 상태와 행동들인데, 이제 우리의 control problem은 이 일련의 상태와 행동들이 끝날 때까지 decision making을 계속 해야 하는 finite horizon 입니다. 
(곧 끝이 정해지지 않은 inifinite horizon version에 대해서도 생각 해 볼 것이지만 우선은 쉬운 이해를 위해 finite하다고 하겠습니다, 그리고 이러한 finite한 sequence를 하나의 에피소드 (episode)라고 합니다.) 


위의 수식은 어떠한 시작 시점 $$p(s_1)$$ 부터 chain rule을 이용해 간단하게 유도된 공식이며, chain rule은 일반적으로 과거의 모든 상태들을 참조 (조건부, conditional)해야 하는게 맞지만 우리는 Markov Property를 만족하는 상황에 대해서 생각을 하기로 했기 때문에 바로 직전의 상태만이 현재 상태를 결정 짓게 됩니다. ($$ p(s_{t+1} \vert s_t, a_t ) $$)


그리고 위의 수식을 간결하게 나타내기 위해서 앞으로는 $$p_{\theta} (\tau)$$ 라고 나타내도록 하겠습니다. 

$$
p_{\theta} (\tau) = p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t )
$$

이제 Trajectory까지 정의했으니 우리는 강화학습의 목적 함수를 아래와 같이 써볼 수 있겠습니다.

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim  p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] 
$$

수식이 나타내는 바는 Trajectory Distribution 하에 Expected Value인데요, 즉, 강화 학습의 목적식이 나타내는 것은 "Trajectory를 따를 때 매 순간 마다의 보상 값 (reward)들의 기대값의 합이 최대가 되도록 신경망을 업데이트 하는 것" 입니다.

여기서 기대값이 굉장히 중요한데, 이는 정책 (Policy)과 초기 상태 (Initial State)에 대한 분포 그리고 상태 천이 행렬 (State Transition Probability)의 `Stochasticity`를 해결 해 줍니다.

 
물론 이러한 목적 함수의 수식은 다양한 variation이 존재하며, 앞으로의 강의들에서 자세히 다룰 예정이라고 합니다.
어쨌든 이제 우리는 정책 함수를 학습할 수 있는 가장 간단한 버전의 수식을 정의하게 되었습니다.

```
Sergey 선생님이 잠깐 여기서 강의를 멈추고 (우리는 글을 읽는것을 멈춰!야겠죠) 수식의 의미를 곱씹어 봐야 한다고 합니다.
Trajectory란 과연 어떤 의미인가?,
이 경로(궤적)를 따라서 보상 함수에 대해서 기대값을 취한 다는건 과연 어떤 의미인지?,
학습이 되면서 파라메터가 변하는건 Trajectory에는 과연 어떤 영향을 주는지? 
등등 
(파라메터 Theta는 Trajectory와 Policy 모두에 영향을 주죠)

아무튼 이 부분이 가장 중요하며, 단박에 이해하기 어려운 부분이기 때문에 시간을 좀 들이는게 좋다고 합니다.
```

(개인적으로 기대값 (Expectation, $$\mathbb{E}$$)을 쓰는 이유는 우리가 "어떤 상태에서 어떤 행동을 하는가" 에는 굉장히 많은 옵션이 존재하고, 그렇기 떄문에 Trajectory도 굉장히 많을 텐데, 이렇게 다양하게 sampling할 수 있는 시나리오에 대해 모두 고려하겠다는 의미 같습니다.)




이번에는 우리가 정의한 Trajectory를 Markov Chain과 연관지어 생각을 해 보겠다고 합니다.

![slide11](/assets/images/CS285/lec-4/slide11.png)
*Slide. 11.*

현재 상태에서 다음 상태로, 현재 상태에서 어떤 행동을 하는지는 *Slide. 11.*의 하단에 있는 그림과 같겠죠.

원래 Markov Chain은 상태와 상태 천이 행렬만이 존재하나, 위의 그림은 상태 공간 (State Space)에 행동 공간 (Action Space) 이라는 개념이 추가된 (Augmented) 것이라고 볼 수 있습니다. 여기서 Action이라는 것은 State에 condition되어있기 때문에 State와 Action을 그룹지어 생각해 볼 수 있겠습니다.




![slide12](/assets/images/CS285/lec-4/slide12.png)
*Slide. 12.*

*Slide. 12.*에서 Augmented 된 State를 볼 수 있습니다. 
이제 후에 있을 여러 공식의 유도에서 수월하게 공식을 쓰기 위해 Objective Function을 조금 더 간단하게 써 보도록 하겠습니다.


우리는 이제 Trajectory Distribution이 Augmented Space (State + Action)를 가지는 Markov Chain을 따른 다고 했죠.

그렇기 때문에 $$p( (s_{t+1},a_{t+1}) \vert (s_t,a_t) )$$는 아래의 수식으로 표현이 가능합니다.

$$
p( (s_{t+1},a_{t+1}) \vert (s_t,a_t) ) = p(s_{t+1} \vert s_t, a_t) \pi_{\theta}(a_{t+1} \vert s_{t+1})
$$

(MDP Transitions 과 Policy의 곱으로 나타냄)


수식의 의미는 상태 $$s_t$$에서 정책을 따라 행동 $$a_t$$ 를 했을 때, p(s_{t+1} \vert s_t, a_t)에 따라서 상태가 변하거나 그대로이거나 하게 되고, 그 상태에서 또 어떤 행동을 할 확률 $$a_{t+1}$$ 입니다.


***

좀 더 생각해볼까요, 다음과 같은 4차원 격자 모양의 미로가 있다고 생각 해 보겠습니다.

![maze](/assets/images/CS285/lec-4/maze.png)
*Additive Fig.*


존재하는 State는 4개가 됩니다.

$$s_1,s_2,s_3,s_4$$

$$\mu_{t,i} = p(s_t=i)$$

Markov Chain에서는 State $$S$$와 Transition Operator $$T$$만이 존재했고 다음 상태는 현재 상태에만 의존하죠 $$p(s_{t+1} = i \vert s_t = j)$$.

그렇다면 현재 상태에서 다음 상태로 가는 확률은 아래와 같았습니다.

$$\vec{\mu_{t+1}} = T \vec{\mu_t}$$

여기서 존재할 수 있는 상태는 4개 이므로 어떤 시점의 상태를 나타내는 벡터는 $$\mu_t \in \mathbb{R}^4$$ 이죠.

그리고 Transition Probability (Operator)는 $$T \in \mathbb{R}^{4 \times 4}$$ 차원이 됩니다.

그렇다면 현재 상태에서 다음 상태로 갈 확률은 아래의 곱 처럼 나타낼 수 있는데,

$$

\begin{bmatrix}
S_1\\ 
S_2\\ 
S_3\\ 
S_4
\end{bmatrix}

=

\begin{bmatrix}
S_{11} & S_{12} & S_{13} & S_{14} \\ 
S_{21} & S_{22} & S_{23} & S_{24}\\ 
S_{31} & S_{32} & S_{33} & S_{34}\\ 
S_{41} & S_{42} & S_{43} & S_{44}
\end{bmatrix}


\begin{bmatrix}
S_1\\ 
S_2\\ 
S_3\\ 
S_4
\end{bmatrix}

$$

우리는 지금 State와 Action을 묶어서 생각하는 Augmented Markov Chain을 생각하기로 했기 때문에 `S1,S2,S3,S4` 4차원의 State Space와, `상,하,좌,우`로 움직일 수 있는 마찬가지로 4차원의 Action Space가 있다면 $$p(s_t,a_t)$$는 $$4\times4$$로 16차원이 됩니다. 그러므로 Transition Probability $$T$$도 $$\mathbb{R}^{16 \times 16}$$이 됩니다.


***



![slide13](/assets/images/CS285/lec-4/slide13.png)
*Slide. 13.*


이를 이용해서 우리는 원래의 Objective 를 

***

<center>
"An expected value under the trajectory distribution of the sum of rewards"
</center>

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim  p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] 
$$

***

Linearity of Expectation 을 이용해 아래와 같이 나타낼 수 있습니다.

***

<center>
"The sum over time of the expected values under state actual marginal in markov chain of the reward of that time step"
</center>

$$
\theta^{\ast} = argmax_{\theta} \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p_{\theta}(s_t,a_t)}[r(s_t,a_t)]
$$

***

(Expectation의 선형성을 이용해 $$\sum$$을 밖으로 빼냈네요.) 


이런 수학적인 재배열이 쓸모없어 보일 수 있으나 우리가 이를 $$t=1$$ 부터 $$T$$까지가 정해진 `Finite Horizon Case`가 아니라 `Infinite Horizon Case`를 생각한다면 유용하다고 합니다.




![slide14](/assets/images/CS285/lec-4/slide14.png)
*Slide. 14.*


***

(stationary distribution에 대한 내용 다시 작성해야함)

***

`Infinite Horizon Case`에서는 $$t$$가 $$\infty$$ 가 되기 때문에 이렇게 양수의 reward를 무한정 더하면 Objective 또한 $$\infty$$를 return하게 됩니다.



![slide15](/assets/images/CS285/lec-4/slide15.png)
*Slide. 15.*






![slide16](/assets/images/CS285/lec-4/slide16.png)
*Slide. 16.*

강화학습이란 언제나 Expectation, $$\mathbb{E}$$를 최적화 하는 task입니다. 
우리는 항상 보상들의 기대 값들 (Expected Values of Rewards)에 대해서 고려해야 합니다.





## <mark style='background-color: #fff5b1'> Algorithms </mark>

![slide18](/assets/images/CS285/lec-4/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-4/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-4/slide20.png)
*Slide. 19.*

![slide21](/assets/images/CS285/lec-4/slide21.png)
*Slide. 21.*







## <mark style='background-color: #fff5b1'> Value Functions </mark>


![slide23](/assets/images/CS285/lec-4/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-4/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-4/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-4/slide26.png)
*Slide. 26.*






## <mark style='background-color: #fff5b1'> Types of Algorithms </mark>

![slide28](/assets/images/CS285/lec-4/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-4/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-4/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-4/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-4/slide32.png)
*Slide. 32.*

![slide33](/assets/images/CS285/lec-4/slide33.png)
*Slide. 33.*






### <mark style='background-color: #dcffe4'> Tradeoffs Between Algorithms </mark>

![slide35](/assets/images/CS285/lec-4/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-4/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-4/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-4/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-4/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-4/slide40.png)
*Slide. 40.*






### <mark style='background-color: #dcffe4'> Examples of Algorithms </mark>

![slide42](/assets/images/CS285/lec-4/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-4/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-4/slide44.png)
*Slide. 44.*

![slide45](/assets/images/CS285/lec-4/slide45.png)
*Slide. 45.*

![slide46](/assets/images/CS285/lec-4/slide46.png)
*Slide. 46.*








## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [2015 UCL Course on RL from David Silver](https://www.davidsilver.uk/teaching/)












