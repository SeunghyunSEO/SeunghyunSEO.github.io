---
title: (미완) Lecture 7 - Value Function Methods

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 7의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=pP_67mTJbGw&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=28)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 챕터에서 다룰 내용은 `Value Function based Method`입니다.

![slide1](/assets/images/CS285/lec-7/slide1.png)
*Slide. 1.*

Lecture 9과 그 이후에 Policy-based Method를 몇번 더 다룰 것 같지만 
앞으로 `7,8장`은 Value based로 `policy를 implicit 하게` 배우는 Value Iteration 같은 방법론과 더 나아가서는 (Deep) Q-Learning에 대해서 배우게 될겁니다. 

![rl_taxonomy_intellabs_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_intellabs_for_lec6.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_openai_for_lec6.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))




## <mark style='background-color: #fff5b1'> Recap </mark>

먼저 지난 강의에서 다뤘던 내용을 복습하면서 강의를 시작합니다.

![slide2](/assets/images/CS285/lec-7/slide2.png)
*Slide. 2.*

지난 강의에서는 `Critic`을 도입한 정책 경사 알고리즘인 `Actor-Critic Algorithm`에 대해서 알아봤습니다.
*Slide. 2.*에 나와있는 알고리즘은 Trajectory를 뽑아놓고 학습하는 `Batch-mode`로 2,3,4번 스텝이 중요했었죠.
그리고 언제나 정책 경사 기반 알고리즘들의 목표는 Variance를 줄이되 unbiased함을 유지하는 것이었습니다.







## <mark style='background-color: #fff5b1'> Can we omit policy gradient entirely? </mark>

7장의 주제인 `Value Function based Method`은 "`policy에 대한 요소를 빼고` Value Function을 잘 학습시켜서 
학습시킨 함수를 통해 어떻게 행동할지?를 결정할 순 없을까?" 라는 아이디어에서 출발합니다.


이게 가능한 이유는 우리가 어떤 상태에 놓여져 있을때, Policy를 통해서 가장 확률이 높은 행동을 취해 다음 스텝으로 나아가는 게 기본적인 매커니즘인데,
모든 state마다 점수를 부여하고 (가치) 그 상태에서 취할 수 있는 여러 행동중에 어떤 행동을 통해서 다음 state로 한스텝 전진했을 때의 가치가 가장 큰 것을 고르면 이것이 policy를 통해서 가장 높은 확률을 고르는거나 다름 없기 때문입니다.
그러니까 `explicit policy neural network`는 더이상 필요가 없는거죠.


말로하려니 어려운데 이 아이디어가 지금 와닿지 않더라도 이번 글에서 계속 다루게 될 것이니 천천히 같이 알아가면 될 것 같습니다.



![slide3](/assets/images/CS285/lec-7/slide3.png)
*Slide. 3.*

앞서 많이 다뤘던 `Advantage Function`은 "현재 상태에서 $$a_t$$ 라는 행동을 하는게 다른 옵션들을 골랐을때의 평균보다 얼마나 좋은가?" 를 나타냈죠.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$

앞서말한 것 처럼 $$A^{\pi}$$에서 `가장 높은 값 (argmax)`을 나타내는 행동을 하나 뽑으면 어떨까요?

$$
arg max_{a_t} A^{\pi} (s_t,a_t) \scriptstyle{\text{ best action from } s_t \text{ , if we then follow } \pi } \\
$$

우리가 $$\pi$$ 정책을 계속 따른다는 가정하에, $$s_t$$상황에서 `최선의 선택`이 될겁니다 (그게 리턴하는 값이 제일 높으니까요).


그러니까 또 한번 말하지만, 이런 방식으로 policy를 암시적 (implicit)으로 정의하면 되니까 policy를 나타내는 명시적인 (explicit) 네트워크를 둘 필요가 없게 되는거죠.



하지만 policy가 명시적 (explicit)으로가 아니라 암시적 (implicit)으로 존재한다고 하지만 그래도 우리가 하고 싶은 것은 policy를 계속해서 업데이트 해 나가는 것입니다. 이 때 우리가 알고리즘을 진행하면서 얻게 될 `새로운 (업데이트 될) policy`는 아래와 같이 정의할 수 있습니다. (거듭 말하지만 네트워크가 따로 있는건 아닙니다.)

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.

$$

이렇게 함으로써 매 iteration마다 새로운 policy를 얻을 수 있습니다.
우리는 이제 `Neural Netowrk for Advantage Function`만 학습하면 됩니다.

Anatomy를 보시면 아래와 같은 차이가 있습니다.

![policy_vs_value](/assets/images/CS285/lec-7/policy_vs_value.png)
*Fig. Blue Box가 다른것을 알 수 있다. 정책 경사 알고리즘과 다르게 가치 기반 알고리즘은 더이상 Blue Box에서 정책을 학습하지 않는다.*






아래의 *Slide. 4.*에 나와있는것은 `Policy Iteration` 알고리즘의 high-level idea 입니다. 

![slide4](/assets/images/CS285/lec-7/slide4.png)
*Slide. 4.*

`Policy Iteration`은 간단합니다. 

- 1.현재 정책을 평가한다. 
- 2.정책을 업데이트한다. 
- 3.이를 정책이 수렴할 때 까지 반복한다. (더이상 정책을 업데이트해도 이전과 다를 것이 없을 때 까지)

위의 1,2번 두 가지 스텝을 반복하는겁니다. 정책을 평가하고 업데이트하는 것을 반복하기 때문에 `Policy Iteration`이란 이름이 붙은거죠.


2번째 스텝은 discrete action space일 경우 다순히 argmax를 취하면 되는것이니 굉장히 간단한데요 (continuous일 경우 조금 복잡하지만 나중에 다룰것임),
1번이 조금 번거롭습니다.


과연 1번은 어떻게 해야할까요? 과연 어떻게 $$\pi$$를 평가해야 하는걸까요?



이전에 배웠듯 Advantage는 아래와 같습니다.


$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

&
A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E} [ V^{\pi}(s') ] - V^{\pi}(s)
&

\end{aligned}
$$

(여기서 $$s'$$는 다음 state이고, $$s$$는 현재 state)


결국 $$A^{\pi}(s,a)$$를 argmax 해서 정책을 업데이트해야 하므로, 이를 위해 `step 1`인 $$A^{\pi}(s,a)$$를 평가, 즉 $$V^{\pi}(s)$$를 `평가 (evaluate)` 해봅시다. ( $$A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E} [ V^{\pi}(s') ] - V^{\pi}(s)$$ 이므로 )





![slide5](/assets/images/CS285/lec-7/slide5.png)
*Slide. 5.*

$$V^{\pi}(s)$$를 구하는 방법에는 `동적 프로그래밍 (Dynamic Programming, DP)`가 있는데요,
이에 대해 설명하기 위해서 일단 우리가 아래의 세가지를 안다고 하겠습니다.

- Very Small Discrete State Space : $$s$$ - 미로 16칸 
- Very Small Discrete State Space : $$a$$ - 상,하,좌,우
- State Transition Probability : $$p(s' \vert s,a)$$ 

여기서 상태 천이 행렬은 $$16 \times 16 \times 4$$으로 3차원 Tensor가 됩니다.
또한 매 상태마다 가치가 매겨질테니 $$V^{\pi}(s)$$는 16개가 됩니다.


이는 잘 알려진 DP 세팅인데요, Transition Probability를 몰라도 되는 `Model-Free RL`과는 다르지만 ($$p(s' \vert s,a)$$가 있으므로) 일단은 이 세팅에서 DP를 유도하고, 그 다음에 Model-Free로 넘어가겠다고 하네요.

```
여기서 동적 프로그래밍 (Dynamic Programming, DP)란 어떤 문제를 풀 때
이를 여러개의 작은 문제 (sub-problem)로 쪼개서 푸는 것을 말합니다.

즉 Policy Iteration이나 Value Iteration같은 경우 모든 가능한 state에 가치를 부여하고, 이를 기반으로 업데이트하고, 또 부여하고 ... 를 반복함으로써 복잡한 문제를 풀게 됩니다.

DP는 완벽하게 환경을 알 경우 (상태 천이 행렬 등) optimal solution을 계산할 수 있게 해줍니다.
```

사실 위와 같은 세팅은, action space 와 state space 모두 얼마 안돼서 방대한 state-action space를 뉴럴 네트워크로 근사해야만 했던 다른 방법론들과 다르게 `표 형태 (tabular form, table form)`로 표현할 수 있고, 매 state마다 가치를 직접적으로 다 부여할 수 있습니다.
이러한 세팅에서 lecture 6에서 봤던 가치 함수에 대한 `Bootstrapped update`는 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
&
V^{\pi}(s) = \mathbb{E}_{a \sim \pi(a \vert s)} [Q^{\pi}(s,a)] 
& \\

&
V^{\pi}(s) \leftarrow \mathbb{E}_{a \sim \pi(a \vert s)} [r(s,a) + \gamma \mathbb{E}_{s' \sim p(s' \vert s,a)} [V^{\pi}(s')] ]
& \\
\end{aligned}
$$

여기서 우리가 앞으로 가지고 업데이트할 수식은 argmax를 통해서 가장 크게 리턴되는 값만 1이고 나머지는 0인 `one-hot policy`이기 때문에

$$
\pi(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.

$$

아래와 같이 수식을 간단히 해서 나타낼 수 있습니다.

$$
V^{\pi}(s) \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s,\pi(s))} [V^{\pi}(s')] ]
$$

(모든 가능한 액션을 다 해본다는 기대값이 없어졌습니다. 더이상 policy가 `좌로 갈확률 0.21, 우로 갈 확률 0.37 ...` 처럼 확률적 (stochastic)으로 주어지지 않고 원핫 형태로 주어져 어떤 상태에서 취할 행동은 `정해져 (deterministic)` 있기 때문입니다.)


![slide6](/assets/images/CS285/lec-7/slide6.png)
*Slide. 6.*

Advantage를 추정하는건 결국 Value function $$V^{\pi}(s)$$를 평가하는 것 이었기 때문에 (모든 state마다 점수를 부여),
*Slide. 6.*에서 보시는 바와 같이 반복적으로 매 state마다 value를 부여하고 (반복적으로), 그리고 정책을 업데이트하고 ... 를 반복하면 됩니다.


***

요약하자면 

```
1. 지금 상황은 Tabular를 만들 수 있을 정도로 작은 state와 action space이므로 tabular를 만든다. 
2. 4x4짜리 격자구조에 모두 가치를 매긴다. (계산 가능함)
3. 여기서 argmax를 통해 모든 상황별로 어떤 액션을 취하는게 베스트인지를 알 수 있으며, 이 policy로 과거 policy를 대체한다.
4. 2번과 3번을 반복한다.

5. policy가 더이상 변하지 않으면 수렴한 것이다.
```

![v_s](/assets/images/CS285/lec-7/v_s.png)
*Fig. 몇 번이나 evaluation을 반복했는지는 모르겠으나 매 state(격자)마다 가치 ($$V^{\pi}(s)$$)가 부여되어있다.*

***

```
여기서 반드시 알아야 할 것이 있는데요, Policy Iteration을 잘 보시면 학습해야할 파라메터가 아예 없습니다.
즉 이는 강화 '학습'을 한다고 볼 수는 없고, 어떤 환경에 대한 정보가 다 주어져있고, state와 action space가 작은 table 에 다 표현할 수 있을만큼 작을 때,
더이상 policy가 변하지 않을 때까지 반복하면서 (DP) optimal policy를 결국 찾아내는 겁니다.

여기에는 Neural Network 사용되지 않았기에 (지금까지는) optimal policy을 찾을 수 있다는게 보장이 되어있습니다.
```


Policy Iteration이 explicit한 정책 없이 잘 작동하는 것 처럼 보이지만 사실 이보다 더 간단한 DP porcess가 있고, 이는 아래의 *Slide. 7.*에 나와있습니다.


![slide7](/assets/images/CS285/lec-7/slide7.png)
*Slide. 7.*


key idea는 바로 `policy iteration은 결국 policy가 필요하다는건데 아예 이것도 없애버리고, 내가 어떤 상태(s)에서 할 수 있는 행동들(a)이 있고, 이에 대응하는 Q(s,a) 값들을 다 구한 뒤에, 어떤 s에서 그냥 Q(s,a)가 최대인걸 찾으면 되지 않을까?` 입니다.

우리가 아래의 `new policy`를 argmax를 통해서 구하는 과정에서

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.
$$

사용하는 것은 Advantage function 이었는데요, 이는 현재 reward + (다음 state의 기대값 - 현재 state의 가치) 였죠.

$$
A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E}[ V^{\pi} (s') ] - V^{\pi}(s)
$$

여기서 $$V^{\pi}(s)$$를 빼버리면 이는 Q Function과 다를 바 없다는 걸 알게됩니다.
왜냐하면 $$a_t$$에 관해서 argmax 해버리는 것이기$$V^{\pi}(s)$$ 에는 a가 없습니다).


그리고 Advantage function를 의 의미를 다시 곱씹어봐도 `어떤 상태에서 이 행동이 모든 행동을 고려해서 구한 '평균' 가치보다 얼마나 좋은가?` 였기 때문에 `평균`을 나타내는 $$V^{\pi}(s)$$를 빼도 상관 없는거죠.


$$
arg max_{a_t} A^{\pi} (s,a) = arg max_{a_t} Q^{\pi} (s_t,a_t)
$$

즉 $$Q^{\pi} (s_t,a_t)$$를 통해서도 new policy를 구할 수 있는 겁니다.


우리는 이를 이용해서 새로운 알고리즘을 생각해볼 수 있는데요, 이는 `Value Iteration`이라고 하며 아래와 같은 과정을 따릅니다.

- 1.$$Q(s,a)$$ 를 구한다. (마찬가지로 table을 만든다. 근데 여기서는 s에 대해서만 ($$s_1,s_2,\cdots,s_N$$) 만들지말고 (s,a)에 대해서 만드는것, 예를 들어 가능한 행동이 '상,하,좌,우' 면 (s1,상), (s1,하), ...이렇게 만드는 것임) 
- 2.$$Q(s,a)$$에서 action에 대해 max인 값을 구해서 $$V(s)$$를 구한다.

![s_a](/assets/images/CS285/lec-7/s_a.png){: width="50%"}
*Fig. (s,a)를 다 구해야 함. 예를 들어 state수가 16개인 미로이며, 행동이 '상,하,좌,우' 4개면, 4x16 = 64*


`Policy Iteration`는 Policy gradient와 다르게 explicit하게 policy를 네트워크로 정의하고 계산하는 과정이 빠졌죠. 거기에 추가로 `Value Iteration`은 step 2를 $$max_{a_t} Q$$으로 바껴서 아예 policy를 업데이트 하지도 않는 더욱 단순한 procedure가 되는 겁니다. 

***

하지만 여기서 프로세스가 한단계 더 간단해 질 수 있는데요, 왜냐면 Value Iteration의 step2를 step1에 넣을 수 있기 때문입니다. step 1에 Value Function이 들어가는 요소는 $$V^{\pi}(s')$$뿐이고 이를 $$max_{a} Q^{\pi}(s,a)$$로대체할 수 있는거죠.그러면 우리는 Q 함수만 표현하면 되게 되는데요, 이는 나중에 배우게 될 Value-based Method의 핵심인 `Q-Learning`과 관련이 있습니다.

***


`Policy Iteration`과 `Value Iteration`을 비교하면 아래와 같습니다.


![policy_iteration_vs_value_iteration](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration.png){: width="80%"}
*Fig. Policy Iteration VS Value Iteration*


![policy_iteration_vs_value_iteration2](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration2.png)
*Fig. Policy Iteration VS Value Iteration with Deep RL Anatomy*







지금까지 `Policy Iteration`과 `Value Iteration`에 대해서 알아봤는데요, 수식만 가지고는 확실하게 와닿지 않을 것 같아 아래의 몇가지 예제를 준비해봤습니다.

### <mark style='background-color: #dcffe4'> Policy Iteration Example - 4x4 Grid World </mark>




강화학습 분야의 세계적인 석학 [Richard S. Sutton ](https://scholar.google.ca/citations?user=6m4wv6gAAAAJ&hl=en) 의 책이자 강화학습 입문서로 유명한 [Reinforcement Learning: An Introduction.](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)를 보면 아래와 같이 `Policy Iteration`이 어떻게 동작하는지에 대한 figure가 있습니다.

![sutton_RLbook2020_policy_iteration1](/assets/images/CS285/lec-7/sutton_RLbook2020_policy_iteration1.png)
*Fig. 4x4 Grid World에서 음영 부분으로만 가면 되고, 매번 이동할 때 마다 -1점씩 점수를 까먹으므로 최단경로로 가는게 제일 좋은 정책일 것이다.*

`Policy Iteration`을 통해서 k번째 스텝의 $$V^{\pi}$$와 이를 argmax한 $$\pi$$를 보면 아래와 같습니다.

![sutton_RLbook2020_policy_iteration2](/assets/images/CS285/lec-7/sutton_RLbook2020_policy_iteration2.png)
*Fig. 반복하다보면 결국 어떤 상태에서 어떤 행동을 해야 하는지에 대한 deterministic optimal policy를 얻게 된다.*


***

이를 조금 더 디테일하게 나타내면 아래와 같이 나타낼 수 있습니다.
(출처는 $$\rightarrow$$ [Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/))

![im20](/assets/images/CS285/lec-7/im_20.png){: width="80%"}
*Fig. 서튼의 책과 같은 세팅, 같은 목표*

![im21](/assets/images/CS285/lec-7/im_21.png){: width="40%"}
*Fig. 맨 처음 아무것도 안했을 때의 $$V^{\pi}_0$$. 모든 state의 가치가 0으로 초기화되어 있다.*

![im22](/assets/images/CS285/lec-7/im_22.png)
*Fig. $$V^{\pi}_1$$를 모든 상태마다 계산하는 과정, 위의 figure에 적혀있는 수식은 6번째 타일 (모든 타일이 state인 것), $$V^{\pi}_1(6)$$에 대한 value를 계산하는 과정이다.*

위에서 맨 처음 policy는 '상,하,좌,우'로 움직일 확률이 모두 25%이기 때문에 0.25가 곱해지는 겁니다.


(위에서 가치를 계산하는데 쓰인 수식은 `Bellman Equation` 이라는 것으로, 강의 내내 다뤘던 것과 Form이 약간 다를 뿐 내용은 같은것이긴 한데, 왜인지 Sergey님이 강화학습에서 매우 중요하고 다른 서적들에서 맨 처음에 다루고 넘어가는 이 Bellman Equation이라는 단어를 언급을 안합니다. 아마 의도가 있을 것 같은데... . 하긴 근데 이 강의는 다른 책들과 달리 DP를 이용한 정책, 가치 반복 알고리즘 (학습 아님) -> 정책 경사로 넘어가는게 아니라, 정책 경사부터 했죠. 아마 의도가 있을 것 같습니다...)


![im23](/assets/images/CS285/lec-7/im_23.png){: width="40%"}
*Fig. $$V^{\pi}_1$$ 를 격자구조에 나타낸 결과*

과연 첫 번째 평가를 (state마다 가치 부여) 마치고난 뒤의 정책은 어떻게 업데이트되어야 할까요? 2번째 타일을 기준으로 생각해보면 어떤 행동을 취해야 그 다음 state에서의 가치가 가장 높은지를 알 수 있고 (1번 타일로가야 0점으로 제일 높죠?) argmax를 취하니 당연히 정책은 '왼쪽=1' 이고 나머지는 0이게 될겁니다. 

![im24](/assets/images/CS285/lec-7/im_24.png){: width="80%"}
*Fig. $$V^{\pi}_2$$를 모든 상태마다 계산하는 과정*

![im25](/assets/images/CS285/lec-7/im_25.png){: width="80%"}
*Fig. $$V^{\pi}_1$$를 모든 상태마다 계산하는 과정 2*

![im26](/assets/images/CS285/lec-7/im_26.png)
*Fig. $$V^{\pi}_k$$가 매 스텝마다 어떻게 바뀌는지를 보여줌*

![im28](/assets/images/CS285/lec-7/im_28.png)
*Fig. 어떻게 argmax를 통해서 policy를 찾아내는지에 대한 과정 (random policy로 시작했지만 이번 스텝이 끝나면 결국 현재 가치 함수를 기반으로 선택된 policy가 리턴될 것임)*

![im30](/assets/images/CS285/lec-7/im_30.png)
*Fig. 이렇게 Evaluation (E), Improvement (I) 를 반복하다보면 optimal policy에 도달하게 됨*








### <mark style='background-color: #dcffe4'> Value Iteration Example - Grid World </mark>

아래는 `Value Iteration` 알고리즘을 통해 시간이 흐름에 따라서 각 state(격자)의 Value가 어떻게 변하는지를 알려줍니다. 

![nvidia_value_iteration](/assets/images/CS285/lec-7/nvidia_value_iteration.png)
*Fig. Value Iteration은 모든 time step에서 모든 state(여기서는 모든 격자)에 대해 가치를 매깁니다. S는 starting point이며 G는 goal state 이고, T는 함정이기 때문에 여기에 빠지면 감점이고, 검은색 박스는 막다른곳입니다. 알고리즘이 진행되지 않은 Iteration이 0일때를 보시면 reward가 trap과 goal에만 각각 음수,양수로 표현된 걸 알 수 있습니다. 그리고 시간이 지남에 따라서 value가 다른 격자들에도 번지게 되고, 결국 수렴하게 됩니다. 함정에 빠져서 얼마의 감점을 받는지를 결정하는 Penalty Value나 G에 도달했을 때의 보상값에 따라서 다른 솔루션을 얻을 수 있다고 합니다.*

(출처 : [Deep Learning in a Nutshell: Reinforcement Learning By Tim Dettmers](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/))

이것도 어떻게 계산이 됐는지 잠깐 보도록 하겠습니다. Value Iteration을 우리는 아래처럼 정의 했었는데요,

![policy_iteration_vs_value_iteration](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration.png){: width="80%"}
*Fig. Policy Iteration VS Value Iteration*

[Value Iteration from Pieter Abbeel, UC Berkeley ](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa11/slides/mdps-intro-value-iteration.pdf) 자료를 보고 설명하기 위해서 아래의 수식으로 생각해보겠습니다 (같은 의미임).

![pieter_figure1](/assets/images/CS285/lec-7/pieter_figure1.png)
*Fig. Equation for Value Iteration*

위의 수식을 이용하여 아래의 $$<3,3>$$ state의 Value를 처음으로 계산해보면 아래와 같은 수식을 얻을 수 있습니다.

![pieter_figure2](/assets/images/CS285/lec-7/pieter_figure2.png)

$$<3,3>$$ 입장에서 max가 되는 행동은 `right`이 되기 때문에 right이라는 행동을 취했을 때 우리가 다음 state가 될 수 있는 후보군은 `자기 자신, 오른쪽, 아래`가 되고, 그렇게 될 확률들이 각각 (0.1, 0.8, 0.1)이라고 할 때 생각할 때 ( $$\sum$$ `<3,3>에서 다른 state로 이동할 확률`=( 각각 $$0.1,0.8,0.1$$) $$\odot$$ (`현재 state 보상`($$ 각각 0, 0, 0$$) + `다음 state 가치`( 각각 $$0, 1, 0$$)) 가 되는겁니다. 여기서 $$\odot$$는 element-wise product 입니다. (그리고 여기서 0.9는 $$\gamma$$ 때문에 생긴거같은데, Pieter Abbeel의 슬라이드에 있는 수식에는 이게 없네요.)


그리고 이를 반복해서 아래처럼 Value를 모든 격자에 뿌리게 되는 겁니다.

![pieter_figure3](/assets/images/CS285/lec-7/pieter_figure3.png)

그리고 최종적으로 수렴이 된 솔루션을 통해서 어떤 격자에 랜덤하게 놓이더라도 그 주변에 이동할 수 있는 옵션들 중에서 가치가 가장 높은 곳을 타고 이동하기만 하면 그것이 최적의 policy가 되는겁니다.



여기서 이 수식을 계산 하려면 어떤 상태에서 어떤 행동을 했을때 어떤 상태로 움직일 것인가?를 나타내는 `State Transition probability`, $$p(s' \vert s,a)$$ 를 알고 있어야 합니다.


아무튼 이런식으로 솔루션을 구하게되면 stating point부터 이동 가능한 state들을 보고 점수가 높은 곳을 따라서 이동하면 그게 곧 policy가 됩니다.


***

추가적으로 Policy Iteration과 Value Iteration중 무엇이 좋은가?가 궁금하시다면 아래를 읽어보시길 바랍니다.

```
Policy iteration and value iteration, which is best? If you have many actions or you start from a fair policy then choose policy iteration. If you have few actions and the transition is acyclic then chose value iteration. If you want the best from the two world then give a look to the modified policy iteration algorithm.
```

(출처 : [Dissecting Reinforcement Learning-Part.1 from Massimiliano Patacchiola](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html))


***







## <mark style='background-color: #fff5b1'> Fitted Value Iteration & Q-Iteration </mark>

다시 돌아와서, 지금까지 우리는 어떻게 `tabular 형식`으로 표현된 가치 함수를 배우는가 알아봤습니다.
이는 매우 작은 state-action space를 가졌기에 표 형식으로 표현이 가능했고, 그러므로 뉴럴네트워크를 통해서 함수 근사 (function approximation)를 할 필요가 없었습니다. 


하지만 당연히 실제 문제에서 이런 tabular 형식으로 하는건 말이 안되기 때문에 뉴럴 네트워크를 사용해서 근사를 해야겠죠?

![slide9](/assets/images/CS285/lec-7/slide9.png)
*Slide. 9.*

왜 이것이 문제가 되는지 생각을 해보자면 아래의 비디오 게임을 가지고 자율 주행 학습을 한다고 생각해볼 때
가능한 state의 수는, 이미지 사이즈가 $$200 \times 200$$ 이고 RGB컬러일 경우 모든 pixel마다 가질 수 있는 Value를 가져야 하므로

$$
\vert S \vert = (255^3)^{200 \times 200}
$$

이 되는데, 이렇게 되면 우리가 일일이 표로 정리해야할 state들은 우주에 존재하는 원자 수 보다 많다고 합니다.

![state_space](/assets/images/CS285/lec-7/state_space.png){: width="50%"}
*Fig. Tabular 폼으로 나타낼 수 없는 경우*

![v_s](/assets/images/CS285/lec-7/v_s.png)
*Fig. Tabular 폼으로 나타낼 경우 모든 상태마다 가치를 부여하는게 가능하다.*


게다가 이것은 continuous action space에 대해 생각하면 차원이 더욱 말도 안되게 된다고 합니다.
이렇게 말도안되는 차원에 대해서 계산을 해야 하는 것을 `차원의 저주 (Curse of Dimensionality)`라고 하는데요, 머신러닝에도 쓰이는 말이지만 특히 강화학습에서는 이를 Tabular Reinforcement Learning을 할 경우 state가 굉장히 높은 차원을 가지고 있으면 entry 숫자는 그 차원의 지수승 (exponential) 이 된다는 것을 의미한다고 합니다. 


그러므로 우리는 이를 전부 tabular 형식으로 가지고 있는것이 아니라 `'state'를 'scalar value'로 매핑해주는` 어떤 function approximator가 필요한데요, 이는 6장에서 본것과 마찬가지로 뉴럴네트워크가 됩니다. 

![additive4](/assets/images/CS285/lec-6/additive4.png){: width="70%"}
*Fig. Neural Netowrk as Universal Function Approximator*

이제 우리는 Neural Net Value Function을 피팅해야되기 때문에 `Target Value`가 필요한데요, Target Value만 있으면 추로한 결과와 정답 데이터를 이용한 Least Square Error를 최적화 하는걸로 구할 수 있으며, 우리가 `Value Iteration`을 사용한다면 이 때의 Target Value는 바로 $$max_{a} Q^{\pi} (s,a)$$가 됩니다.

$$
\begin{aligned}
&
L(\phi) = \frac{1}{2} \parallel V_{\phi}(s) - {max}_a Q^{\pi} (s,a) \parallel^2 
& \\

&
Q^{\pi} (s,a) \leftarrow r(s,a) + \gamma \mathbb{s' \sim p(s' \vert s,a)} [V^{\pi} (s')]
& \\
\end{aligned}
$$

그니까 이게 성공적으로 잘 되면 우리는 어떤 상태 $$s$$에 대해서 가치, $$V_{\phi}(s)$$가 어떻다 라는 scalar 값을 바로 얻어낼 수 있는 근사 함수를 얻게되는거죠.  

이렇게 Value Iteration에 Value Function을 뉴럴네트워크로 근사한 `Fitted Value Iteration`은 Value Iteration과는 아래와 같은 차이가 있습니다. 

![fitted_value_iteration](/assets/images/CS285/lec-7/fitted_value_iteration.png){: width="80%"}
*Fig. Value Iteration VS Fitted Value Iteration*

보시면 Q 함수를 명시적으로 따로 두지 않았습니다.

다시 요약하자면

- 1. 모든 sample state에서 가능한 모든 action에 대해서 q function을 계산하고, 여기서 최대가 되는 action에 대한 Q값만 따로 취함.
- 2. 1번의 값을 타겟 ($$y$$)으로 $$s \rightarrow V(s) \approx y$$를 매핑해주는 Mapping Function을 학습함. 

입니다.


하지만 앞서 Value Iteration에서도 가지고 있던 제약이 여기에도 있는데요, 바로 1 step 을 계산하기 위해서는 `Transition Dynamics`, $$p(s' \vert s,a)$$를 알아야만 한다는 겁니다. 

```
Transition Dynamics를 모른다는 것은 "환경이 어떻게 구성되어있는지 모른다", 혹은 "Model을 모른다" 라고 할 수도 있는데요, 그러니까 우리가 어떻게 생겼는지 모르는 숫자가 6개인 주사위를 던질 때 1이나올 확률 6이나올 확률을 모른다는 겁니다. 그러니까 시뮬레이션을 통해서 Transition Dynamics을 알아내던지 아니면 Transition Dynamics없이 이상적인 정책을 찾아내야합니다.
```

![slide10](/assets/images/CS285/lec-7/slide10.png)
*Slide. 10.*

결론은 우리가 Transition Dynamics를 모르면 다이나믹 프로그래밍을 통해서 단순 반복으로 정책을 찾아내는 Value Iteration을 사용할 수 없다는 겁니다.


이를 해결하기 위해서 다시 Policy Iteration으로 돌아가보도록 하겠습니다. 다음 2 step을 반복함으로써 optimal policy을 찾는거였죠

- Policy Iteration
  - 1.evaluate $$Q^{\pi}(s,a)$$ : $$V^{\pi} \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, \pi(s))}[V^{\pi}(s)]$$를 통해서 Tabular의 모든 state에 대한 가치를 구한다.
  - 2.set $$\pi \leftarrow \pi'$$ : $$arg max_{a_t} Q^{\pi}(s_t,a_t)$$를 통해서 one hot policy를 찾아냅니다.


여기서 첫 번째 step을 주목해보면 

$$V^{\pi} \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, \pi(s))}[V^{\pi}(s)]$$

기대값에 transition probability가 들어가 있는게 문제임을 알 수 있습니다.


이를 어떻게 없앨 수 있을까요? 
Q함수와 V함수가 아래와 같은 관계를 가지고 있다는 것은 이제 익숙하실겁니다.
두 함수의 차이는 어떤 상태에서 모든 가능한 행동을 고려해서 가치를 생각하는가? 아니면 어떤 상태에서 특정 행동을 했을때의 가치를 생각하는가? 와
받는 input이 s뿐이냐 s,a 두개냐 입니다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

\end{aligned}
$$

이를 이용해서 아래를 업데이트하는 걸로 정책을 평가할 수 있는데요,

$$Q^{\pi} \leftarrow r(s,a) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, a)}[Q^{\pi}(s',\pi(s'))]$$

cf) $$V^{\pi} \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, \pi(s))}[V^{\pi}(s)]$$


이 두개의 차이가 굉장히 미묘해 보이지만 사실 중요한 점을 내포하고 있습니다.
바로 Q를 평가하는 수식에서는 $$(s,a,s')$$ 샘플을 통해서 



![slide11](/assets/images/CS285/lec-7/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-7/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-7/slide13.png)
*Slide. 13.*













## <mark style='background-color: #fff5b1'> From Q-Iteration to Q-Learning </mark>

![slide15](/assets/images/CS285/lec-7/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-7/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-7/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-7/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-7/slide19.png)
*Slide. 19.*










## <mark style='background-color: #fff5b1'> Value Functions in Theory </mark>

![slide21](/assets/images/CS285/lec-7/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-7/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-7/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-7/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-7/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-7/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-7/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-7/slide28.png)
*Slide. 28.*







## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/)

- [Deep Learning in a Nutshell: Reinforcement Learning By Tim Dettmers](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/)





