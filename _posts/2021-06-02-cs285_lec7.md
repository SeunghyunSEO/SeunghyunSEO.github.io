---
title: () Lecture 7 - Value Function Methods

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 7의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=pP_67mTJbGw&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=28)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 챕터에서 다룰 내용은 `Value Function based Method`입니다.

![slide1](/assets/images/CS285/lec-7/slide1.png)
*Slide. 1.*

Lecture 9과 그 이후에 Policy-based Method를 몇번 더 다룰 것 같지만 
앞으로 `7,8장`은 Value based로 policy를 `implicit`하게 배우는 Value Iteration 등과 더 나아가서는 (Deep) Q-Learning에 대해서 배우게 될겁니다. 

![rl_taxonomy_intellabs_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_intellabs_for_lec6.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_openai_for_lec6.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))




## <mark style='background-color: #fff5b1'> Recap </mark>

![slide2](/assets/images/CS285/lec-7/slide2.png)
*Slide. 2.*

지난 강의에서 `Critic`을 도입한 정책 경사 알고리즘인 `Actor-Critic Algorithm`에 대해서 알아봤습니다.
*Slide. 2.*에 나와있는 알고리즘은 Trajectory를 뽑아놓고 학습하는 `Batch-mode`로 2,3,4번 스텝이 중요했었죠.







## <mark style='background-color: #fff5b1'> Can we omit policy gradient entirely? </mark>

7장의 주제인 `Value Function based Method`은 "policy에 대한 요소를 빼고 Value Function을 잘 학습시켜서 
학습시킨 함수를 통해 `어떻게 행동할지?`를 결정할 순 없을까?" 라는 아이디어에서 출발합니다.


이게 가능한 이유는 우리가 어떤 상태에 놓여져 있을때, Policy를 통해서 가장 확률이 높은 행동을 취해 다음 스텝으로 나아가는건데,
가치 함수라는 것이 그러한 가능한 다음 상태들에 대한 가치를 알려주는 함수이고, 이를 통해서 나아가면 되기 때문입니다.
그러니까 `explicit policy neural network`는 더이상 필요가 없는거죠.




![slide3](/assets/images/CS285/lec-7/slide3.png)
*Slide. 3.*

앞서 많이 다뤘던 Advantage Function은 현재 상태에서 $$a_t$$ 라는 행동을 하는게 다른 옵션들을 골랐을때의 평균보다 얼마나 좋은가? 를 나타냈죠.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$

그렇다면 $$A^{\pi}$$에서 가장 높은 값을 나타내는 행동을 하나 뽑으면 어떨까요?

$$
arg max_{a_t} A^{\pi} (s_t,a_t) \scriptstyle{\text{ best action from } s_t \text{ , if we then follow } \pi } \\
$$

우리가 $$\pi$$ 정책을 계속 따른다는 가정하에, $$s_t$$상황에서 최선의 선택이 될겁니다 (그게 리턴하는 값이 제일 높으니까요).


이번 장에서부터는 한번 policy를 explicit하게 두지 않고 (네트워크를 따로 두지 않는다는 말) Advantage Function을 argmax해서 행동을 선택하는것으로 대체해서 강의를 진행을 하게 될겁니다.


그렇다면 우리는 새로운 policy를 아래와 같이 정의할 수 있습니다. (네트워크가 따로 있는건 아닙니다.)

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.

$$

이렇게 함으로써 매 iteration마다 새로운 policy를 얻을 수 있습니다.
우리는 이제 `Neural Netowrk for Advantage Function`만 학습하면 됩니다.

Anatomy를 보시면 아래와 같은 차이가 있습니다.

![policy_vs_value](/assets/images/CS285/lec-7/policy_vs_value.png)
*Fig. Blue Box가 다른것을 알 수 있다. 더이상 Blue Box에서 정책을 학습하지 않는다.*






*Slide. 4.*에 나와있는것은 `Policy Iteration` 알고리즘의 high-level idea 입니다. 

![slide4](/assets/images/CS285/lec-7/slide4.png)
*Slide. 4.*

`Policy Iteration`은 간단합니다. 정책을 평가하고, 정책을 업데이트하는 두 가지 스텝을 반복하는겁니다.
2번째 스텝은 discrete action space일 경우 다순히 argmax를 취하면 되는것이니 굉장히 간단한데요 (continuous일 경우 조금 복잡하지만 나중에 다룰것임),
그렇다면 1번은 어떻게 해야할까요? 과연 어떻게 $$\pi$$를 평가해야 하는걸까요?


$$
A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E} [ V^{\pi}(s') ] - V^{\pi}(s)
$$

이기 때문에 $$V^{\pi}(s)$$를 평가해봅시다.





![slide5](/assets/images/CS285/lec-7/slide5.png)
*Slide. 5.*

Policy iteration 을 위한 advantage를 추정하기 위해서 $$V^{\pi}(s)$$를 하는 방법에는 `Dynamic Programming, DP`가 있습니다. 


이에 대해 설명하기 위해서 일단 우리가 아래의 세가지를 안다고 합시다.

- State Transition Probability : $$p(s' \vert s,a)$$
- Very Small Discrete State Space : $$s$$ 
- Very Small Discrete State Space : $$a$$

이는 잘 알려진 DP 세팅인데요, 이러한 세팅은 `Model-Free RL`과는 다르지만 ($$p(s' \vert s,a)$$가 있으므로) 일단은 이 세팅에서 DP를 유도하고, 그 다음에 Model-Free로 넘어가겠다고 하네요.

```
여기서 동적 프로그래밍 (Dynamic Programming, DP)란 어떤 문제를 풀 때
이를 여러개의 작은 문제 (sub-problem)로 쪼개서 푸는 것을 말합니다.
```





![slide6](/assets/images/CS285/lec-7/slide6.png)
*Slide. 6.*





![slide7](/assets/images/CS285/lec-7/slide7.png)
*Slide. 7.*







## <mark style='background-color: #fff5b1'> Fitted Value Iteration & Q-Iteration </mark>

![slide9](/assets/images/CS285/lec-7/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-7/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-7/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-7/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-7/slide13.png)
*Slide. 13.*




## <mark style='background-color: #fff5b1'> From Q-Iteration to Q-Learning </mark>

![slide15](/assets/images/CS285/lec-7/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-7/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-7/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-7/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-7/slide19.png)
*Slide. 19.*




## <mark style='background-color: #fff5b1'> Value Functions in Theory </mark>

![slide21](/assets/images/CS285/lec-7/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-7/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-7/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-7/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-7/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-7/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-7/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-7/slide28.png)
*Slide. 28.*




### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/)

- [Deep Learning in a Nutshell: Reinforcement Learning By Tim Dettmers](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/)





