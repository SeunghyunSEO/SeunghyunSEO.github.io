---
title: () Lecture 7 - Value Function Methods

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 7의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=pP_67mTJbGw&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=28)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 챕터에서 다룰 내용은 `Value Function based Method`입니다.

![slide1](/assets/images/CS285/lec-7/slide1.png)
*Slide. 1.*

`Lecture 9`과 그 이후에 Policy-based Method를 몇번 더 다룰 것 같지만 
앞으로 7,8장은 Value based로 policy를 `implicit`하게 배우는 Value Iteration 등과 더 나아가서는 (Deep) Q-Learning에 대해서 배우게 될겁니다. 

![rl_taxonomy_intellabs_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_intellabs_for_lec6.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_openai_for_lec6.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))




## <mark style='background-color: #fff5b1'> Recap </mark>

![slide2](/assets/images/CS285/lec-7/slide2.png)
*Slide. 2.*

지난 강의에서 `Critic`을 도입한 정책 경사 알고리즘인 `Actor-Critic Algorithm`에 대해서 알아봤습니다.
*Slide. 2.*에 나와있는 알고리즘은 Trajectory를 뽑아놓고 학습하는 `Batch-mode`로 2,3,4번 스텝이 중요했었죠.







## <mark style='background-color: #fff5b1'> Can we omit policy gradient entirely? </mark>

7장의 주제인 `Value Function based Method`은 "policy에 대한 요소를 빼고 Value Function을 잘 학습시켜서 
학습시킨 함수를 통해 `어떻게 행동할지?`를 결정할 순 없을까?" 라는 아이디어에서 출발합니다.


이게 가능한 이유는 우리가 어떤 상태에 놓여져 있을때, Policy를 통해서 가장 확률이 높은 행동을 취해 다음 스텝으로 나아가는건데,
가치 함수라는 것이 그러한 가능한 다음 상태들에 대한 가치를 알려주는 함수이고, 이를 통해서 나아가면 되기 때문입니다.
그러니까 `explicit policy neural network`는 더이상 필요가 없는거죠.




![slide3](/assets/images/CS285/lec-7/slide3.png)
*Slide. 3.*

앞서 많이 다뤘던 Advantage Function은 현재 상태에서 $$a_t$$ 라는 행동을 하는게 다른 옵션들을 골랐을때의 평균보다 얼마나 좋은가? 를 나타냈죠.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$

그렇다면 $$A^{\pi}$$에서 가장 높은 값을 나타내는 행동을 하나 뽑으면 어떨까요?

$$
arg max_{a_t} A^{\pi} (s_t,a_t) \scriptstyle{\text{ best action from } s_t \text{ , if we then follow } \pi } \\
$$



![slide4](/assets/images/CS285/lec-7/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-7/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-7/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-7/slide7.png)
*Slide. 7.*







## <mark style='background-color: #fff5b1'> Fitted Value Iteration & Q-Iteration </mark>

![slide9](/assets/images/CS285/lec-7/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-7/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-7/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-7/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-7/slide13.png)
*Slide. 13.*




## <mark style='background-color: #fff5b1'> From Q-Iteration to Q-Learning </mark>

![slide15](/assets/images/CS285/lec-7/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-7/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-7/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-7/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-7/slide19.png)
*Slide. 19.*




## <mark style='background-color: #fff5b1'> Value Functions in Theory </mark>

![slide21](/assets/images/CS285/lec-7/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-7/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-7/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-7/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-7/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-7/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-7/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-7/slide28.png)
*Slide. 28.*




### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)










