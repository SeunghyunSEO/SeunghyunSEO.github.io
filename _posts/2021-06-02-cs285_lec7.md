---
title: (미완) Lecture 7 - Value Function Methods

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)


Lecture 7의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=pP_67mTJbGw&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=28)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 챕터에서 다룰 내용은 `Value Function based Method`입니다.

![slide1](/assets/images/CS285/lec-7/slide1.png)
*Slide. 1.*

Lecture 9과 그 이후에 Policy-based Method를 몇번 더 다룰 것 같지만 
앞으로 `7,8장`은 Value based로 `policy를 implicit 하게` 배우는 Value Iteration 같은 방법론과 더 나아가서는 (Deep) Q-Learning에 대해서 배우게 될겁니다. 

![rl_taxonomy_intellabs_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_intellabs_for_lec6.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_openai_for_lec6.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))


본격적으로 `Policy Iteration`과 `Value Iteration` 등에 대해서 본격적으로 알아보기 전에, 이는 테이블 형식으로 가능한 state나 (state, action) tuple에 대한 가치(Value) 를 Dynamic Programming을 이용해 빠짐없이 구해내는 과정을 반복함으로써, 이를 바탕으로 최적의 policy를 결국 찾아내는 오래된 방법론입니다. 이런 방법론은 일반적으로 학습할 요소가 없으므로 `강화 '학습'이 아닙니다`. 그럼에도 불구하고 "왜 이 오래된 알고리즘들을 알아야하는가?"에 대해서는 이렇게 Dynamic Programming을 통해서 솔루션을 구하는 아이디어가 현대의 강화학습에 지대한 영향을 끼쳤기 때문이라고 합니다.


이를 이해하고 쭉 강의와 글을 따라오시면서 같이 공부하면 될 것 같습니다.





## <mark style='background-color: #fff5b1'> Recap </mark>

먼저 지난 강의에서 다뤘던 내용을 복습하면서 강의를 시작합니다.

![slide2](/assets/images/CS285/lec-7/slide2.png)
*Slide. 2.*

지난 강의에서는 `Critic`을 도입한 정책 경사 알고리즘인 `Actor-Critic Algorithm`에 대해서 알아봤습니다.
*Slide. 2.*에 나와있는 알고리즘은 Trajectory를 뽑아놓고 학습하는 `Batch-mode`로 2,3,4번 스텝이 중요했었죠.
그리고 언제나 정책 경사 기반 알고리즘들의 목표는 Variance를 줄이되 unbiased함을 유지하는 것이었습니다.







## <mark style='background-color: #fff5b1'> Can we omit policy gradient entirely? </mark>

7장의 주제인 `Value Function based Method`은 "`policy에 대한 요소를 빼고` Value Function을 잘 학습시켜서 
학습시킨 함수를 통해 어떻게 행동할지?를 결정할 순 없을까?" 라는 아이디어에서 출발합니다.


이게 가능한 이유는 우리가 어떤 상태에 놓여져 있을때, Policy를 통해서 가장 확률이 높은 행동을 취해 다음 스텝으로 나아가는 게 기본적인 매커니즘인데,
모든 state마다 점수를 부여하고 (가치) 그 상태에서 취할 수 있는 여러 행동중에 어떤 행동을 통해서 다음 state로 한스텝 전진했을 때의 가치가 가장 큰 것을 고르면 이것이 policy를 통해서 가장 높은 확률을 고르는거나 다름 없기 때문입니다.
그러니까 `explicit policy neural network`는 더이상 필요가 없는거죠.


말로하려니 어려운데 이 아이디어가 지금 와닿지 않더라도 이번 글에서 계속 다루게 될 것이니 천천히 같이 알아가면 될 것 같습니다.



![slide3](/assets/images/CS285/lec-7/slide3.png)
*Slide. 3.*

앞서 많이 다뤘던 `Advantage Function`은 "현재 상태에서 $$a_t$$ 라는 행동을 하는게 다른 옵션들을 골랐을때의 평균보다 얼마나 좋은가?" 를 나타냈죠.

$$
\begin{aligned}
&
\color{red}{Q^{\pi}(s_t,a_t)} = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [ \color{red}{Q^{\pi}(s_t,a_t)} ]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$

이제 여기서 앞서말한 것 처럼 $$A^{\pi}$$의 `가장 높은 값 (argmax)`을 나타내는 행동을 하나 뽑으면 어떨까요?

$$
arg max_{a_t} A^{\pi} (s_t,a_t) \scriptstyle{\text{ best action from } s_t \text{ , if we then follow } \pi } \\
$$

우리가 $$\pi$$ 정책을 계속 따른다는 가정하에, $$s_t$$상황에서 `최선의 선택`이 될겁니다 (모든 가능한 행동을 고려해서 평균적인 가치를 계산한 것 보다 지금 행동이 얼마나 좋은지를 나타내는 $$A$$ 중에서도 최대값을 리턴하는 행동을 골랐으니까요).


즉, 이런 방식으로 policy를 암시적 (implicit)으로 정의하면 되니까 policy를 나타내는 명시적인 (explicit) 네트워크를 둘 필요가 없게 되는거죠.



하지만 policy가 명시적 (explicit)으로가 아니라 암시적 (implicit)으로 존재한다고 하지만 그래도 우리가 하고 싶은 것은 policy를 계속해서 업데이트 해 나가는 것입니다. 이 때 우리가 알고리즘을 진행하면서 얻게 될 `새로운 (업데이트 될) policy`는 방금 논한 것을 수식으로 표현해 아래와 같이 정의할 수 있습니다. (거듭 이야기하지만 네트워크가 따로 있는건 아닙니다.)

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.

$$

이렇게 함으로써 매 iteration마다 새로운 policy를 얻을 수 있습니다.
우리는 이제 `Neural Netowrk for Advantage Function`만 학습하면 됩니다.

Anatomy를 보시면 아래와 같은 차이가 있습니다.

![policy_vs_value](/assets/images/CS285/lec-7/policy_vs_value.png)
*Fig. Blue Box가 다른것을 알 수 있다. 정책 경사 알고리즘과 다르게 가치 기반 알고리즘은 더이상 Blue Box에서 정책을 학습하지 않는다.*








### <mark style='background-color: #dcffe4'> Policy Iteration </mark>


이런식으로 처음에는 어떤 state $$s_t$$에서 어떤 행동을 해야할지를 random하게 고르다가, 각 상태마다의 "Value를 평가"하고, 이 "Value를 기반"으로 state마다 최고의 Adavantage 값을 리턴하는 행동을 argmax를 함으로써 "policy를 개선해" 나가는 방식을 `Policy Iteration`이라고 합니다.
아래의 *Slide. 4.*에 나와있는것은 Policy Iteration 알고리즘의 high-level idea 입니다. 

![slide4](/assets/images/CS285/lec-7/slide4.png)
*Slide. 4.*

`Policy Iteration`은 간단합니다. 

- 1.현재 정책을 평가한다. 
- 2.정책을 업데이트한다. 
- 3.이를 정책이 수렴할 때 까지 반복한다. (더이상 정책을 업데이트해도 이전과 다를 것이 없을 때 까지)

위의 1,2번 두 가지 스텝을 반복하는겁니다. 정책을 평가하고 업데이트하는 것을 반복하기 때문에 `Policy Iteration`이란 이름이 붙은거죠.


2번째 스텝은 discrete action space일 경우 다순히 argmax를 취하면 되는것이니 굉장히 간단한데요 (continuous일 경우 조금 복잡하지만 나중에 다룰것임),
1번이 조금 번거롭습니다.


`과연 1번은 어떻게 해야할까요?`, 어떻게 매 상태마다의 행동이 야기하는 Advantage 값, $$A^{\pi$$}를 평가해야 하는걸까요?



이전에 배웠듯 Advantage는 아래와 같습니다.


$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
= r(s_t,a_t) + \gamma \mathbb{E} [ V^{\pi}(s') ]
& \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [ \color{red}{ Q^{\pi}(s_t,a_t) }]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
= \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [ \color{red}{ r(s_t,a_t) + \gamma \mathbb{E} [ V^{\pi}(s') ] } ]
& \\


&
A^{\pi} (s_t,a_t) = \color{red}{ Q^{\pi}(s_t,a_t) } - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

&
A^{\pi} (s_t,a_t) = \color{red}{ r(s_t,a_t) + \gamma \mathbb{E} [ V^{\pi}(s') ] } - V^{\pi}(s_t)
& \scriptstyle{s' \text{ is next state} } \\

\end{aligned}
$$

(여기서 $$s'$$는 다음 state이고, $$s$$는 현재 state)


결국 $$A^{\pi}(s,a)$$를 argmax 해서 정책을 업데이트해야 하므로, 이를 위해 `step 1`인 $$A^{\pi}(s,a)$$를 평가, 즉 $$V^{\pi}(s)$$를 `평가 (evaluate)` 해봅시다. ( $$A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E} [ V^{\pi}(s') ] - V^{\pi}(s)$$ 이므로 )





![slide5](/assets/images/CS285/lec-7/slide5.png)
*Slide. 5.*

$$V^{\pi}(s)$$를 구하는 방법에는 `동적 프로그래밍 (Dynamic Programming, DP)`가 있는데요,
이에 대해 설명하기 위해서 일단 우리가 아래의 세가지를 안다고 하겠습니다.

- Very Small Discrete State Space : $$s$$ - 미로 16칸 
- Very Small Discrete State Space : $$a$$ - 상,하,좌,우
- State Transition Probability : $$p(s' \vert s,a)$$ 

여기서 상태 천이 행렬은 $$16 \times 16 \times 4$$으로 3차원 Tensor가 됩니다.
또한 매 상태마다 가치가 매겨질테니 $$V^{\pi}(s)$$는 16개가 됩니다.


이는 잘 알려진 DP 세팅인데요, Transition Probability를 몰라도 되는 `Model-Free RL`과는 다르지만 ($$p(s' \vert s,a)$$가 있으므로) 일단은 이 세팅에서 DP를 유도하고, 그 다음에 Model-Free로 넘어가겠다고 하네요.

```
여기서 동적 프로그래밍 (Dynamic Programming, DP)란 어떤 문제를 풀 때
이를 여러개의 작은 문제 (sub-problem)로 쪼개서 푸는 것을 말합니다.

즉 Policy Iteration이나 Value Iteration같은 경우 모든 가능한 state에 가치를 부여하고, 이를 기반으로 업데이트하고, 또 부여하고 ... 를 반복함으로써 복잡한 문제를 풀게 됩니다.

DP는 완벽하게 환경을 알 경우 (상태 천이 행렬 등) optimal solution을 계산할 수 있게 해준다고 합니다.
```

사실 위와 같은 세팅은, action space 와 state space 모두 얼마 안돼서 방대한 state-action space를 뉴럴 네트워크로 근사해야만 했던 다른 방법론들과 다르게 `표 형태 (tabular form, table form)`로 표현할 수 있고, 매 state마다 가치를 직접적으로 다 부여할 수 있습니다.
이러한 세팅에서 lecture 6에서 봤던 가치 함수에 대한 `Bootstrapped update`는 다음과 같이 쓸 수 있습니다.

$$
\begin{aligned}
&
V^{\pi}(s) = \mathbb{E}_{a \sim \pi(a \vert s)} [Q^{\pi}(s,a)] 
& \\

&
V^{\pi}(s) \leftarrow \mathbb{E}_{a \sim \pi(a \vert s)} [r(s,a) + \gamma \mathbb{E}_{s' \sim p(s' \vert s,a)} [V^{\pi}(s')] ]
& \\
\end{aligned}
$$

여기서 우리가 앞으로 가지고 업데이트할 수식은 argmax를 통해서 가장 크게 리턴되는 값만 1이고 나머지는 0인 `one-hot policy`이기 때문에

$$
\pi(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.

$$

아래와 같이 수식을 간단히 해서 나타낼 수 있습니다.

$$
\begin{aligned}
&
V^{\pi}(s) \leftarrow \mathbb{E}_{a \sim \pi(a \vert s)} [r(s,a) + \gamma \mathbb{E}_{s' \sim p(s' \vert s,a)} [V^{\pi}(s')] ]
& \\

&
V^{\pi}(s) \leftarrow r( s,\color{red}{ \pi(s) } ) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, \color{red}{ \pi(s) } )} [V^{\pi}(s')] ]
& \\
\end{aligned}
$$

(모든 가능한 액션을 다 해본다는 기대값이 없어졌습니다. 더이상 policy가 `좌로 갈확률 0.21, 우로 갈 확률 0.37 ...` 처럼 확률적 (stochastic)으로 주어지지 않고 원핫 형태, $$[1,0,0,0]$$ 처럼 주어져 어떤 상태에서 취할 행동은 `정해져 (deterministic)` 있기 때문입니다.)


![slide6](/assets/images/CS285/lec-7/slide6.png)
*Slide. 6.*

Advantage를 추정하는건 결국 Value function $$V^{\pi}(s)$$를 평가하는 것 이었기 때문에 (모든 state마다 점수를 부여),

$$
A^{\pi} (s_t,a_t) = \color{red}{ r(s_t,a_t) + \gamma \mathbb{E} [ V^{\pi}(s') ] } - V^{\pi}(s_t)
$$

*Slide. 6.*에서 보시는 바와 같이 반복적으로 매 state마다 value를 부여하고 (반복적으로), 그리고 정책을 업데이트하고 ... 를 반복하고 최종적으로 다음 state가 될 수 있는 후보군들의 가치에 대한 기대값과 현재 state의 가치를 빼서 액션을 결정하면 그게 그 state에서 해야할 action을 알려주는 policy가 되는겁니다.


***

요약하자면 

```
1. 지금 상황은 Tabular를 만들 수 있을 정도로 작은 state와 action space이므로 tabular를 만든다. 
2. 4x4짜리 격자구조에 모두 가치를 매긴다. (계산 가능함)
3. 여기서 argmax를 통해 모든 상황별로 어떤 액션을 취하는게 베스트인지를 알 수 있으며, 이 policy로 과거 policy를 대체한다.
4. 2번과 3번을 반복한다.

5. policy가 더이상 변하지 않으면 수렴한 것이다.
```

![v_s](/assets/images/CS285/lec-7/v_s.png)
*Fig. 몇 번이나 evaluation을 반복했는지는 모르겠으나 매 state(격자)마다 가치 ($$V^{\pi}(s)$$)가 부여되어있다.*

***

```
여기서 반드시 알아야 할 것이 있는데요, Policy Iteration을 잘 보시면 학습해야할 파라메터가 아예 없습니다.
즉 이는 앞서 말씀드린 것 처럼 강화 '학습'을 한다고 볼 수는 없으며, 또한 환경에 대한 정보가 다 주어져있고 (상태 천이 확률이 다 주어져있으므로 기대값 정확히 계산 가능), state와 action space가 작은 table 에 다 표현할 수 있을만큼 작을 때,
더이상 policy가 변하지 않을 때까지 반복하면서 (DP) optimal policy를 결국 찾아내는 겁니다.

여기에는 Neural Network 사용되지 않았기에 (지금까지는) optimal policy을 찾을 수 있다는게 보장이 되어있습니다.
```









### <mark style='background-color: #dcffe4'> Value Iteration </mark>

Policy Iteration이 explicit한 정책 없이 잘 작동하는 것 처럼 보이지만 사실 이는 몇가지 문제점을 안고 있습니다.
강화학습 분야의 세계적인 석학 [Richard S. Sutton ](https://scholar.google.ca/citations?user=6m4wv6gAAAAJ&hl=en) 의 책이자 강화학습 입문서로 유명한 [Reinforcement Learning: An Introduction.](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)를 보면 이에 대한 내용이 나와 있는데요, 간단히 요약해서 `정책 반복이 정책 평가를 하는데 시간이 너무 오래걸리며, 이를 계속 반복적으로 수행해서 최적의 정책으로 수렴하는 데 너무 많은 시간이 걸린다`는 겁니다. 그래서 정책 반복의 두 가지 스텝을 합쳐서 보다 더 간단하고 빨리 수렴할 수 있게 만든 알고리즘이 아래의 *Slide. 7.*에 나와있는데요, 이것이 바로 `가치 반복 (Value Iteration`입니다.


![slide7](/assets/images/CS285/lec-7/slide7.png)
*Slide. 7.*


가치 반복 알고리즘의 key idea는 `policy iteration은 결국 policy가 필요하다는건데 아예 이것도 없애버리고, 내가 어떤 상태(s)에서 할 수 있는 행동들(a)이 있고, 이에 대응하는 Q(s,a) 값들을 다 구한 뒤에, 어떤 s에서 그냥 Q(s,a)가 최대인걸 찾으면 되지 않을까?` 입니다.

이렇게 함으로써 평가하고 업데이트하는 시간을 줄였다고 합니다.


우리가 아래의 `new policy`를 argmax를 통해서 구하는 과정에서

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.
$$

사용하는 것은 Advantage function 이었는데요, 이는 현재 reward + (다음 state의 기대값 - 현재 state의 가치) 였죠.

$$
A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E}[ V^{\pi} (s') ] - V^{\pi}(s)
$$

여기서 $$V^{\pi}(s)$$를 빼버리면 이는 Q Function과 다를 바 없다는 걸 알게됩니다.

$$
A^{\pi} (s,a) = Q^{\pi}(s,a) - \color{red}{ V^{\pi}(s) }
$$

이렇게 빼버려도 되는 이유는 $$a_t$$에 관해서 argmax 해버리는 것인데 $$V^{\pi}(s)$$ 에는 a가 없기 때문입니다.


그리고 Advantage function를 의 의미를 다시 곱씹어봐도 `어떤 상태에서 이 행동이 모든 행동을 고려해서 구한 '평균' 가치보다 얼마나 좋은가?` 였기 때문에 `평균`을 나타내는 $$V^{\pi}(s)$$를 빼도 상관 없는거죠.


$$
arg max_{a_t} A^{\pi} (s,a) = arg max_{a_t} Q^{\pi} (s_t,a_t)
$$

즉 $$Q^{\pi} (s_t,a_t)$$를 통해서도 new policy를 구할 수 있는 겁니다.


우리는 이를 이용해서 새로운 알고리즘을 생각해볼 수 있는데요, 이는 `Value Iteration`이라고 하며 아래와 같은 과정을 따릅니다.

- 1.$$Q(s,a)$$ 를 구한다. (마찬가지로 table을 만든다. 근데 여기서는 s에 대해서만 ($$s_1,s_2,\cdots,s_N$$) 만들지말고 (s,a)에 대해서 만드는것, 예를 들어 가능한 행동이 '상,하,좌,우' 면 (s1,상), (s1,하), ...이렇게 만드는 것임) 
- 2.$$Q(s,a)$$에서 action에 대해 max인 값을 구해서 $$V(s)$$를 구한다.
- 3.값을 다 구한 뒤에는 어떤 지점에서 시작하더라도 선택 가능한 다음 state들 중 $$V(s')$$값이 큰 state로 전진하면 그것이 곧 정책이다.

![s_a](/assets/images/CS285/lec-7/s_a.png){: width="50%"}
*Fig. (s,a)를 다 구해야 함. 예를 들어 state수가 16개인 미로이며, 행동이 '상,하,좌,우' 4개면, 4x16 = 64*


`Policy Iteration`는 Policy gradient와 다르게 explicit하게 policy를 네트워크로 정의하고 계산하는 과정이 빠졌죠. 거기에 추가로 `Value Iteration`은 step 2를 $$max_{a_t} Q$$으로 바껴서 아예 policy를 업데이트 하지도 않는 더욱 단순한 procedure가 되는 겁니다. 

***

하지만 여기서 프로세스가 한단계 더 간단해 질 수 있는데요, 왜냐면 Value Iteration의 step2를 step1에 넣을 수 있기 때문입니다. step 1에 Value Function이 들어가는 요소는 $$V^{\pi}(s')$$뿐이고 이를 $$max_{a} Q^{\pi}(s,a)$$로대체할 수 있는거죠. 그러면 우리는 Q 함수만으로 수식을 표현하면 되게 되는데요, 이는 나중에 배우게 될 Value-based Method의 핵심인 `Q-Learning`과 관련이 있습니다.

***


`Policy Iteration`과 `Value Iteration`을 비교하면 아래와 같습니다.


![policy_iteration_vs_value_iteration](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration.png){: width="80%"}
*Fig. Policy Iteration VS Value Iteration*


![policy_iteration_vs_value_iteration2](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration2.png)
*Fig. Policy Iteration VS Value Iteration with Deep RL Anatomy*







지금까지 `Policy Iteration`과 `Value Iteration`에 대해서 알아봤는데요, 수식만 가지고는 확실하게 와닿지 않을 것 같아 아래의 몇가지 예제를 준비해봤습니다.









### <mark style='background-color: #dcffe4'> Policy Iteration Example - 4x4 Grid World </mark>




[Richard S. Sutton ](https://scholar.google.ca/citations?user=6m4wv6gAAAAJ&hl=en) 의 책이자 강화학습 입문서로 유명한 [Reinforcement Learning: An Introduction.](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)를 보면 아래와 같이 `Policy Iteration`이 어떻게 동작하는지에 대한 figure가 있습니다.

![sutton_RLbook2020_policy_iteration1](/assets/images/CS285/lec-7/sutton_RLbook2020_policy_iteration1.png)
*Fig. 4x4 Grid World에서 음영 부분으로만 가면 되고, 매번 이동할 때 마다 -1점씩 점수를 까먹으므로 최단경로로 가는게 제일 좋은 정책일 것이다.*

`Policy Iteration`을 통해서 k번째 스텝의 $$V^k$$와 이를 argmax한 $$\pi$$를 보면 아래와 같습니다.

![sutton_RLbook2020_policy_iteration2](/assets/images/CS285/lec-7/sutton_RLbook2020_policy_iteration2.png)
*Fig. 반복하다보면 결국 어떤 상태에서 어떤 행동을 해야 하는지에 대한 deterministic optimal policy를 얻게 된다.*


***

이를 조금 더 디테일하게 나타내면 아래와 같이 나타낼 수 있습니다.
(출처는 $$\rightarrow$$ [Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/))

![im20](/assets/images/CS285/lec-7/im_20.png){: width="80%"}
*Fig. 서튼의 책과 같은 세팅, 같은 목표*

![im21](/assets/images/CS285/lec-7/im_21.png){: width="40%"}
*Fig. 맨 처음 아무것도 안했을 때의 $$V^{\pi}_0$$. 모든 state의 가치가 0으로 초기화되어 있다.*

![im22](/assets/images/CS285/lec-7/im_22.png)
*Fig. $$V^{\pi}_1$$를 모든 상태마다 계산하는 과정, 위의 figure에 적혀있는 수식은 6번째 타일 (모든 타일이 state인 것), $$V^{\pi}_1(6)$$에 대한 value를 계산하는 과정이다.*

위에서 맨 처음 policy는 '상,하,좌,우'로 움직일 확률이 모두 25%이기 때문에 0.25가 곱해지는 겁니다.


(위에서 가치를 계산하는데 쓰인 수식은 `Bellman Equation` 이라는 것으로, 강의 내내 다뤘던 것과 Form이 약간 다를 뿐 내용은 같은것이긴 한데, 왜인지 Sergey님이 강화학습에서 매우 중요하고 다른 서적들에서 맨 처음에 다루고 넘어가는 이 "Bellman Equation" 이라는 단어를 언급조차 안합니다. 의도적으로 그런건지, 다 안다고 생각하고 그런건지... 하긴 근데 이 강의는 다른 책들과 달리 DP를 이용한 정책, 가치 반복 알고리즘 (학습 아님) -> 정책 경사로 넘어가는게 아니라, 정책 경사부터 설명하기도 하니까 아마  다른 의도가 있을 것 같습니다...)


![im23](/assets/images/CS285/lec-7/im_23.png){: width="40%"}
*Fig. $$V^{\pi}_1$$ 를 격자구조에 나타낸 결과*

과연 첫 번째 평가를 (state마다 가치 부여) 마치고난 뒤의 정책은 어떻게 업데이트되어야 할까요? 2번째 타일을 기준으로 생각해보면 어떤 행동을 취해야 그 다음 state에서의 가치가 가장 높은지를 알 수 있고 (1번 타일로가야 0점으로 제일 높죠?) argmax를 취하니 당연히 정책은 '왼쪽=1' 이고 나머지는 0이게 될겁니다. 

![im24](/assets/images/CS285/lec-7/im_24.png){: width="80%"}
*Fig. $$V^{\pi}_2$$를 모든 상태마다 계산하는 과정*

![im25](/assets/images/CS285/lec-7/im_25.png){: width="80%"}
*Fig. $$V^{\pi}_1$$를 모든 상태마다 계산하는 과정 2*

![im26](/assets/images/CS285/lec-7/im_26.png)
*Fig. $$V^{\pi}_k$$가 매 스텝마다 어떻게 바뀌는지를 보여줌*

![im28](/assets/images/CS285/lec-7/im_28.png)
*Fig. 어떻게 argmax를 통해서 policy를 찾아내는지에 대한 과정 (random policy로 시작했지만 이번 스텝이 끝나면 결국 현재 가치 함수를 기반으로 선택된 policy가 리턴될 것임)*

![im30](/assets/images/CS285/lec-7/im_30.png)
*Fig. 이렇게 Evaluation (E), Improvement (I) 를 반복하다보면 optimal policy에 도달하게 됨*








### <mark style='background-color: #dcffe4'> Value Iteration Example - Grid World </mark>

아래는 `Value Iteration` 알고리즘을 통해 시간이 흐름에 따라서 각 state(격자)의 Value가 어떻게 변하는지를 알려줍니다. 

![nvidia_value_iteration](/assets/images/CS285/lec-7/nvidia_value_iteration.png)
*Fig. Value Iteration은 모든 time step에서 모든 state(여기서는 모든 격자)에 대해 가치를 매깁니다. S는 starting point이며 G는 goal state 이고, T는 함정이기 때문에 여기에 빠지면 감점이고, 검은색 박스는 막다른곳입니다. 알고리즘이 진행되지 않은 Iteration이 0일때를 보시면 reward가 trap과 goal에만 각각 음수,양수로 표현된 걸 알 수 있습니다. 그리고 시간이 지남에 따라서 value가 다른 격자들에도 번지게 되고, 결국 수렴하게 됩니다. 함정에 빠져서 얼마의 감점을 받는지를 결정하는 Penalty Value나 G에 도달했을 때의 보상값에 따라서 다른 솔루션을 얻을 수 있다고 합니다.*

(출처 : [Deep Learning in a Nutshell: Reinforcement Learning By Tim Dettmers](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/))

이것도 어떻게 계산이 됐는지 잠깐 보도록 하겠습니다. Value Iteration을 우리는 아래처럼 정의 했었는데요,

![policy_iteration_vs_value_iteration](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration.png){: width="80%"}
*Fig. Policy Iteration VS Value Iteration*

[Value Iteration from Pieter Abbeel, UC Berkeley ](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa11/slides/mdps-intro-value-iteration.pdf) 자료를 보고 설명하기 위해서 아래의 수식으로 생각해보겠습니다 (같은 의미임).

![pieter_figure1](/assets/images/CS285/lec-7/pieter_figure1.png)
*Fig. Equation for Value Iteration*

위의 수식을 이용하여 아래의 $$<3,3>$$ state의 Value를 처음으로 계산해보면 아래와 같은 수식을 얻을 수 있습니다.

![pieter_figure2](/assets/images/CS285/lec-7/pieter_figure2.png)

$$<3,3>$$ 입장에서 max가 되는 행동은 `right`이 되기 때문에 right이라는 행동을 취했을 때 우리가 다음 state가 될 수 있는 후보군은 `자기 자신, 오른쪽, 아래`가 되고, 그렇게 될 확률들이 각각 (0.1, 0.8, 0.1)이라고 할 때 생각할 때 ( $$\sum$$ `<3,3>에서 다른 state로 이동할 확률`=( 각각 $$0.1,0.8,0.1$$) $$\odot$$ (`현재 state 보상`($$ 각각 0, 0, 0$$) + `다음 state 가치`( 각각 $$0, 1, 0$$)) 가 되는겁니다. 여기서 $$\odot$$는 element-wise product 입니다. (그리고 여기서 0.9는 $$\gamma$$ 때문에 생긴거같은데, Pieter Abbeel의 슬라이드에 있는 수식에는 이게 없네요.)


그리고 이를 반복해서 아래처럼 Value를 모든 격자에 뿌리게 되는 겁니다.

![pieter_figure3](/assets/images/CS285/lec-7/pieter_figure3.png)

그리고 최종적으로 수렴이 된 솔루션을 통해서 어떤 격자에 랜덤하게 놓이더라도 그 주변에 이동할 수 있는 옵션들 중에서 가치가 가장 높은 곳을 타고 이동하기만 하면 그것이 최적의 policy가 되는겁니다.



여기서 이 수식을 계산 하려면 어떤 상태에서 어떤 행동을 했을때 어떤 상태로 움직일 것인가?를 나타내는 `State Transition probability`, $$p(s' \vert s,a)$$ 를 알고 있어야 합니다.


아무튼 이런식으로 솔루션을 구하게되면 stating point부터 이동 가능한 state들을 보고 점수가 높은 곳을 따라서 이동하면 그게 곧 policy가 됩니다.


***

추가적으로 Policy Iteration과 Value Iteration중 무엇이 좋은가?가 궁금하시다면 아래를 읽어보시길 바랍니다.

```
Policy iteration and value iteration, which is best? If you have many actions or you start from a fair policy then choose policy iteration. If you have few actions and the transition is acyclic then chose value iteration. If you want the best from the two world then give a look to the modified policy iteration algorithm.
```

(출처 : [Dissecting Reinforcement Learning-Part.1 from Massimiliano Patacchiola](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html))


***



### <mark style='background-color: #dcffe4'> Policy Iteration and Value Iteration Demo from Andrej Karpathy </mark>

Stanford의 딥러닝 강의인 cs231n으로 너무 유명한 [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/)의 데모 영상을 끝으로 기본적인 DP 기반 알고리즘을 정리하고 다음 스텝으로 넘어가려고 합니다.

![karpathy_policy](/assets/images/CS285/lec-7/karpathy_policy.gif)
*Policy Iteration Animation*

![karpathy_value](/assets/images/CS285/lec-7/karpathy_value.gif)
*Value Iteration Animation*

(출처 : [REINFORCEjs](https://cs.stanford.edu/people/karpathy/reinforcejs/index.html))







## <mark style='background-color: #fff5b1'> Fitted Value Iteration & Q-Iteration </mark>

다시 돌아와서, 지금까지 우리는 어떻게 `tabular 형식`으로 표현된 가치 함수를 배우는가 알아봤습니다.
이는 매우 작은 state-action space를 가졌기에 표 형식으로 표현이 가능했고, 그러므로 뉴럴네트워크를 통해서 함수 근사 (function approximation)를 할 필요가 없었습니다. 

```
다시 한 번 말하지만 Vanilla Policy Iteration과 Vanilla Value Iteration은 강화 '학습'이 아닙니다.
학습을 할 요소가 없습니다.
```


하지만 당연히 실제 문제에서 이런 tabular 형식으로 하는건 말이 안되기 때문에 뉴럴 네트워크를 사용해서 근사를 해야합니다.

![slide9](/assets/images/CS285/lec-7/slide9.png)
*Slide. 9.*

왜 이것이 문제가 되는지 생각을 해보자면 아래의 비디오 게임을 가지고 자율 주행 학습을 한다고 생각해볼 때
가능한 state의 수는, 이미지 사이즈가 $$200 \times 200$$ 이고 RGB컬러일 경우 모든 pixel마다 가질 수 있는 Value를 가져야 하므로

$$
\vert S \vert = (255^3)^{200 \times 200}
$$

이 되는데, 이렇게 되면 우리가 일일이 표로 정리해야할 state들은 우주에 존재하는 원자 수 보다 많다고 합니다.

![state_space](/assets/images/CS285/lec-7/state_space.png){: width="50%"}
*Fig. Tabular 폼으로 나타낼 수 없는 경우*

![v_s](/assets/images/CS285/lec-7/v_s.png)
*Fig. Tabular 폼으로 나타낼 경우 모든 상태마다 가치를 부여하는게 가능하다.*


게다가 이것은 continuous action space에 대해 생각하면 차원이 더욱 말도 안되게 된다고 합니다.
이렇게 말도안되는 차원에 대해서 계산을 해야 하는 것을 `차원의 저주 (Curse of Dimensionality)`라고 하는데요, 머신러닝에도 쓰이는 말이지만 특히 강화학습에서는 이를 Tabular Reinforcement Learning을 할 경우 state가 굉장히 높은 차원을 가지고 있으면 entry 숫자는 그 차원의 지수승 (exponential) 이 된다는 것을 의미한다고 합니다. 


그러므로 우리는 이를 전부 tabular 형식으로 가지며 매 번 계산할 것이 아니라 `'state'를 'scalar value'로 매핑해주는` 어떤 function approximator가 필요한데요, 이는 6장에서 본것과 마찬가지로 뉴럴네트워크가 됩니다. 

![additive4](/assets/images/CS285/lec-6/additive4.png){: width="70%"}
*Fig. Neural Netowrk as Universal Function Approximator*

이제 우리는 Neural Net Value Function을 피팅해야되기 때문에 `Target Value`가 필요한데요, Target Value만 있으면 추로한 결과와 정답 데이터를 이용한 Least Square Error를 최적화 하는걸로 구할 수 있습니다 (왜냐하면 연속적인 scalar value에 대한 학습이므로).


우리가 `Value Iteration`을 사용한다면 이 때의 Target Value는 바로 $$max_{a} Q^{\pi} (s,a)$$가 됩니다.

$$
\begin{aligned}
&
L(\phi) = \frac{1}{2} \parallel V_{\phi}(s) - {max}_a Q^{\pi} (s,a) \parallel^2 
& \\

\end{aligned}
$$

그니까 이게 성공적으로 잘 되면 우리는 "어떤 상태 $$s$$에 대해서 가치, $$V_{\phi}(s)$$가 어떻다" 라는 scalar 값을 바로 얻어낼 수 있는 근사 함수를 얻게되는거죠.  

이렇게 Value Iteration에 Value Function을 뉴럴네트워크로 근사한 `Fitted Value Iteration`은 Value Iteration과는 아래와 같은 차이가 있습니다. 

![fitted_value_iteration](/assets/images/CS285/lec-7/fitted_value_iteration.png){: width="80%"}
*Fig. Value Iteration VS Fitted Value Iteration*


policy 없이 direct로 가치 함수를 학습하는건 동일하지만, 
`fitted value iteration`은 Q 함수를 명시적으로 따로 두지 않고 표현했습니다.


아무튼 다시 요약하자면

- 1.모든 state에 대해서 계산할 순 없으니 `sample state`에서 가능한 모든 action에 대해서 q function을 계산하고 (별로 크지 않은 크기의 discrete action이므로), 여기서 최대가 되는 action에 대한 Q값만 따로 취함.
- 2.1번의 값을 타겟 ($$y$$)으로 $$s \rightarrow V(s) \approx y$$를 매핑해주는 Mapping Function을 학습함. 

입니다.


하지만 앞서 Value Iteration 와 마찬가지로 여기에는 크나큰 제약이 있는데요, 바로 1 step 을 계산하기 위해서는 `Transition Dynamics`, $$p(s' \vert s,a)$$를 알아야만 한다는 겁니다. 

```
Transition Dynamics를 모른다는 것은 "환경이 어떻게 구성되어있는지 모른다", 혹은 "Model을 모른다" 라고 할 수도 있는데요, 그러니까 우리가 어떻게 생겼는지 모르는 숫자가 6개인 주사위를 던질 때 1이나올 확률 6이나올 확률을 모른다는 겁니다. 

그러니까 무수히 많이 주사위를 던져봐서 Transition Dynamics을 알아내던지 아니면 Transition Dynamics없이 이상적인 정책을 찾아내야합니다.

이는 게임같은 시뮬레이션 환경을 사용해서 학습하는 경우에는 대부분 알고 문제를 풀 수도 있지만 
현실 세계에서는 주어지지도 않고, 이를 찾아내기도 힘들기 때문에 중요한 요소입니다.
```

![slide10](/assets/images/CS285/lec-7/slide10.png)
*Slide. 10.*

결론은 우리가 `Transition Dynamics를 모르면 다이나믹 프로그래밍을 통해서 단순 반복으로 정책을 찾아내는 Value Iteration을 사용할 수 없다`는 겁니다.
그리고 대부분의 경우에는 모든게 정해진 시뮬레이션 환경이 아니고서야 이를 정확히 알아낼 수 없습니다. 


이를 해결하기 위해서 다시 Policy Iteration으로 돌아가 생각을 해 보도록 하겠습니다. 
정책 반복은 다음 2 step을 반복함으로써 optimal policy을 찾는거였죠.

- Policy Iteration
  - 1.evaluate $$Q^{\pi}(s,a)$$ : $$V^{\pi} \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, \pi(s))}[V^{\pi}(s)]$$를 통해서 Tabular의 모든 state에 대한 가치를 구한다.
  - 2.set $$\pi \leftarrow \pi'$$ : $$arg max_{a_t} Q^{\pi}(s_t,a_t)$$를 통해서 one hot policy를 찾아냅니다.


여기서 첫 번째 step을 주목해보면 

$$V^{\pi} \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, \pi(s))}[V^{\pi}(s)]$$

기대값에 Transition Probability가 들어가 있는게 문제임을 알 수 있습니다.


이를 어떻게 없앨 수 있을까요? 
Q함수와 V함수가 아래와 같은 관계를 가지고 있다는 것은 이제 익숙하실겁니다.
두 함수의 차이는 `"어떤 상태에서 모든 가능한 행동을 고려해서 가치를 생각하는가?"` 아니면 어떤 상태에서 특정 행동을 했을때의 가치를 생각하는가? 와
`"받는 input이 s뿐이냐 s,a 두개냐"` 입니다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

\end{aligned}
$$

이를 이용해서 아래를 업데이트하는 걸로 정책을 평가를 한다고도 생각해 볼 수 있는데요,

$$
\begin{aligned}
&
Q^{\pi} \leftarrow r(s,a) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, a)}[Q^{\pi}(s',\pi(s'))]$$
& \\

&
\text{cf) }V^{\pi} \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s, \pi(s))}[V^{\pi}(s)]
& \\
\end{aligned}
$$

이 두개의 차이가 굉장히 미묘해 보이지만 사실 중요한 점을 내포하고 있다고 합니다.


그 이유는 Q함수가 (s,a)의 함수이기 때문에, 더이상 다음 상태 $$s'$$를 샘플링하기 위해서 필요한 수식이 $$\pi$$가 아니라 $$a$$라 바뀌었기 때문이라고 합니다. 
($$p(s' \vert s, \pi(s)) \rightarrow p(s' \vert s, a)$$)


그러니까 우리가 아래와 같이 정책을 통해서 샘플링을 여러번 하면

$$
(s_{1,t},a_{1,t},s_{1,t+1},a_{1,t+1},\cdots)
$$

$$
(s_{2,t},a_{2,t},s_{2,t+1},a_{2,t+1},\cdots)
$$

$$
\vdots
$$

어떤 시점 t에서 $$(s_t,a_t,s'_t)$$ 이라는 걸 알고있기 때문에, "어떤 상태 $$s$$에서 정책에따라 $$\pi(s)$$ 행동을 고르고, 이를 통해서 다음 state는 어떻게 될까?" 를 `Transition Dynamics 분포`를 사용한 Expectation로 계산해내지 않아도 된다는겁니다.

***

그리고 수식에서 보면 $$\pi$$가 들어간 곳이 기대값 속 $$Q^{\pi}(s',\pi(s'))$$ 밖에 없는데, 이는 만약 우리가 엄청나게 많은 $$(s,a,s')$$ 샘플을 가지고 있다면 Q함수를 fit하기 위해서 어떠한 policy도 쓸 수 있다는걸 의미한다고 합니다. 
즉 어떤 policy를 사용해서든 (기본적으로는 업데이트 되기전 가장 최근 policy) (s,a,s')을 샘플링하기만 하면 된다는 건데, 이 부분이 굉장히 중요한 부분이 될거라고 합니다. (`On Policy -> Off Policy`를 가능하게 해줌)

***

아무튼 V를 Q로 바꾸고 정책 반복을 하는 것, 그럼으로써 Transition Probability 없이도 학습이 가능하게 하는 것, 이것이 바로 `Value-based Model Free RL`의 근간이 되는 아주 중요한 요소임을 다시 한번 숙지하고 넘어가도록 하겠습니다.


다시, 여태까지 우리는 DP를 이용해 최적의 policy를 찾는 `Policy Iteration`을 우리는 policy를 제거하고 직접적으로 value를 계산하는 `Value Iteration`으로 발전시켰고 여기서 state 수가 너무 많은 경우를 고려해 뉴럴 네트워크로 디자인한 `Fitted Value Iteration`에 대해 알아봤습니다. 그리고 여기서 Transition Dynamics를 모르는 경우 (Unknown MDP)에 대한 대안으로 V대신 Q를 사용해서 최종적으로 `Fitted Q Iteration`을 도출해 낼 것입니다. (그리고 마지막으로 이를 발전시켜서 대망의 `Q-Learning`과 `Deep Q-Learning`으로 가는 것 입니다!)


![slide11](/assets/images/CS285/lec-7/slide11.png)
*Slide. 11.*

우리가 이전에 Policy-based Method를 탈피해 Policy Iteration에 대해서 알아보고, 이를 개선해서 Value Iteration으로 넘어갔습니다.
그리고 Transition Probability를 해결하기 위해서 다시 Policy Iteration으로 돌아와 이를 수정했으니 다시 Value Iteration으로 넘어가야겠죠?


우리가 Value Iteration으로 넘어갈 때 중요한 것은 `"max" trick`을 사용했었다는 겁니다.
이를 $$V \rightarrow Q$$로 바뀐 경우에도 사용 가능할까요?

![value_to_q](/assets/images/CS285/lec-7/value_to_q.png)

그렇습니다.
위의 수식에서 Transition Probability 분포가 들어가있는 기대값만 근사한다면 말이죠.

![fitted_q](/assets/images/CS285/lec-7/fitted_q.png)

기대값 수식을 $$max_{a'} Q_{\phi}(s'_i,a'_i)$$ 로 대체했고 이렇게 함으로써 이제 action들을 시뮬레이션 해 볼 필요가 없어졌습니다.
그냥 가장 "최근에 얻은 정책 (latest policy)"을 돌려서 (roll-out, run) 얻은 샘플들을 사용하면 되는것이죠. 

```
Sergey Levine이 여기서 중요한 점은  "최근에 얻은 정책" 대신에 다른 정책을 써도 된다고 한번 더 언급하는데요. 
아마 Off-Policy RL 이 현실세계에 RL을 적용하는 중요한 부분을 차지하기 떄문인 것 같습니다.

아마 나중에 자세히 다룰 듯 합니다. 
```

이렇게 구성된 `fitted Q iteration`은 네트워크가 Q를 근사한 $$Q_{\phi}$$ 뿐이며, 어떠한 정책 경사 (Policy Gradient) 과정도 필요 없습니다 (즉 Actor가 없음).

![q_network](/assets/images/CS285/lec-7/q_network.png){: width="60%"}

그림에서도 알 수 있듯 `Q-network`는 $$(s,a)$$를 받아서 scalar value를 내뱉는 네트워크고, 딥러닝 네트워크를 사용했으므로 `non-linearity` 가 존재합니다.
이는 다른말로 `비선형 함수 (non-linear function)`가 존재하기 때문에 Tabular Representation을 사용하던 이전의 Value Iteration과 다르게 무한히 반복해도 항상 수렴 (global optimal)이 보장되지 않는 다는 특징이 있다는 건데요, 이는 나중에 더 자세하게 다룬다고 합니다.

![slide12](/assets/images/CS285/lec-7/slide12.png)
*Slide. 12.*

어쨌든 이렇게 해서 우리는 `Full fitted Q iteration`에 대해 정의하게 되었고 이는 *Slide. 12.*에 잘 나와있습니다.
이를 조금 더 확대해서 보면 

![full_fitted_q](/assets/images/CS285/lec-7/full_fitted_q.png)

가장 최근의 (현재) policy를 따라서 (다시 강조하지만 이는 다른 policy를 써도 됩니다 : off-policy) 데이터를 뽑고,
Q 네트워크를 학습하는 과정을 k번 반복하고, 다시 정책을 업데이트하고 이를 바탕으로 샘플링을 하고, ... 를 반복하는 겁니다.

![slide13](/assets/images/CS285/lec-7/slide13.png)
*Slide. 13.*

리뷰는 넘어가겠습니다.











## <mark style='background-color: #fff5b1'> From Q-Iteration to Q-Learning </mark>

지금까지 우리는 policy를 explicit하게 정의하고 학습하는 정책 경사 알고리즘에서 벗어나 Policy Iteration 부터 Value Iteration 그리고 더 나아가 Q-Iteration의 general form까지 알아봤습니다.

이번에는 Q-Iteration이 Deep RL에 어떻게 적용되어 practical algorithm을 탄생시켰는지에 대해서 얘기해보려 합니다.

![slide15](/assets/images/CS285/lec-7/slide15.png)
*Slide. 15.*

Q-Iteration은 *Slide. 15.*에도 잘 나와있지만 다음과 같았습니다.

```
for 어떤 policy로 $$(s,a,s'r)$$ Trajectory를 샘플링한다.
  for 매 state, action 마다 가치를 책정한다 (y)
    Q 네트워크의 출력값과 y 사이의 loss를 계산해서 최적화한다. 
```

앞서 우리는 Q-Iteration이 `Off-Policy`로 발전할 가능성에 대해 얘기했었는데요, 왜 이게 가능할까요?
Off-Policy RL이란 현재 정책 $$\pi_k$$을 업데이트해서 $$\pi_{k+1}$$로 만드는 데 사용하기 위한 샘플들을 $$\pi_k$$가 아닌 $$\pi_{1, \cdots, k-1}$$ 이나 아예 쌩뚱맞은 정책을 사용할 수도 있다는 점을 시사합니다.

이렇게 함으로써 Sample Efficient하게 학습할 수 있게 되는거죠.
즉, 같은 set의 샘플들을 가지고 몇번이고 gradient descent를 해도 되는겁니다.

직관적으로 Q-Iteration에서 이게 되는 이유는 실제로 policy가 사용되는 부분은 수식에서

$$
y_i \leftarrow r(s_i,a_i) + \gamma \color{red}{ max_{a_{i'}} Q_{\phi} (s'_i,a'_i) }
$$

$$ max_{a_{i'}} Q_{\phi} (s'_i,a'_i) $$ 밖에 없기 때문입니다.
(왜냐면 우리가 샘플한 데이터는 현재 시점 $$s,a,r$$와 더불어 그 다음 상태 $$s'$$ 뿐이지만 수식에서는 $$s'$$에서 현재 "어떤 정책"에 따라서 선택 가능한 행동 $$a'$$중 가장 Q를 크게 할 행동을 골라야 하기 때문에 정책으로 한 수 둬봐야 하기 때문이죠.)

여기서 또 $$ max_{a_{i'}} Q_{\phi} (s'_i,a'_i) $$를 이렇게도 생각해 볼 수 있는데요, 우리가 $$a'$$를 argmax Q (=policy)에 따라 고르기 때문에 이는 아래와 같이 바꿔서도 표현 가능합니다.

$$
\begin{aligned}

&
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} Q^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.
& \\

&
max_{a_{i'}} Q_{\phi} (s'_i,a'_i) 
& \\

&
Q_{\phi} (s'_i,argmax_{a'} Q_{\phi}) 
& \\
\end{aligned}
$$


그러니까 우리가 policy를 업데이트해봐야 바뀌는 것은 $$ \color{red}{ max_{a_{i'}} Q_{\phi} (s'_i,a'_i) } $$ 부분 뿐입니다. 즉 알아서 policy가 변할수록 $$a'$$가 변하기 때문에 $$s,a,s'r$$은 따로 다시 샘플하지 않아도 되는겁니다.
기본적으로 Q Function을 사용한다는 것 자체가 모든 행동들을 고려해 $$s'$$ 상태에서의 가치를 근사 하는 것으로 시뮬레이션 해보는것과 다름 없기 때문입니다 (?).

그리고 여기서 $$s_i,a_i$$가 고정되어 있다고 생각하면, 아무리 policy가 변해도 $$s'$$ 또한 고정이 됩니다.


![slide16](/assets/images/CS285/lec-7/slide16.png)
*Slide. 16.*

Q-Iteration이 최적화 하는 것은 정확히 뭘까요?
`MSE Loss`를 통해 $$\epsilon = \frac{1}{2} \sum_i \parallel Q_{\phi}(s_i,a_i) - y_i \parallel^2$$ 을 최적화 하는 것은 아래와 같이 쓸 수 있는데요,

$$
\begin{aligned}
&
\epsilon = \frac{1}{2} \sum_i \parallel Q_{\phi}(s_i,a_i) - y_i \parallel^2
& \\

&
= \frac{1}{2} \mathbb{E}_{(s,a) \sim \beta} [ ( Q_{\phi}(s,a) - [ r(s,a) + \gamma max_{a'} Q_{\phi} (s'a') ] )^2 ]
& \\
\end{aligned}
$$

여기서 $$max_{a'}Q_{\phi}$$ 부분이 policy를 개선하는 부분이고,
`Fitted Q-Iteration`을 최적화한다는 것은 에러 $$\epsilon$$를 줄이는 겁니다. 
최적화를 할 수록 이 값이 줄어들기는 하겠지만 이 값이 작다고 해서 최적의 policy를 찾아낼 수 있다는걸 반영하지는 않습니다.
그냥 네트워크가 "현재 정책을 따를 때 어떤 상태에서 어떤 행동을 하는게 가치가 어떻다던데?"를 잘 따라할 뿐이죠.

하지만 $$\epsilon=0$$이라면 아래와 같이 되고,

$$
\text{if } \epsilon = 0, \text{ then } Q_{\phi}(s,a) = r(s,a) + \gamma max_{a'} Q_{\phi} (s'a') 
$$

이럴 때의 Q-Function을 `Optimal Q-Function`, $$Q^{\ast}$$라고 하며, 여기서 policy는 $$argmax_a Q$$ 이므로 똑같이 `Optimal Policy`, $$\pi^{\ast}$$가 됩니다. 
하지만 Tabular case를 벗어나 뉴럴 네트워크를 쓰거나 하는 경우 Error는 0이 될 수 없고 최적의 policy를 찾는다는 것이 보장되지는 않게 됩니다.


***


- Temporal Difference (TD) Learning

한편 여태까지 우리가 해온 방식처럼 현재 상태의 가치 $$V(s_t)$$를 그 다음 스텝의 가치 $$V(s_{t+1})$$를 사용해서 업데이트 하는, $$t$$와 $$t+1$$의 시간차이를 이용해 미래의 가치를 앞으로 가져와 학습하는 것을 시간차 학습, `Temporal Difference (TD) Learning`이라고 합니다.



$$
\begin{aligned}

&
\color{red}{ For \space Example) }
& \\

&
V(s_t) \leftarrow (1-\alpha) V(s_t) + \alpha G_t
& \\

&
V(s_t) \leftarrow V(s_t) + \alpha (G_t - V(s_t))
& \\

&
V(s_t) \leftarrow V(s_t) + \alpha (r_t + \gamma V(s_{t+1}) - V(s_t))
& \\

&
V(s_t) \leftarrow V(s_t) - \alpha ( V(s_t) - r_t + \gamma V(s_{t+1}) )
& \\

\end{aligned}
$$


한편 Q 함수를 사용한 TD Learning은 아래와 같이 쓸 수 있는데요, 이미 우리가 앞서 다 배운 내용들입니다.

- On-Policy TD Control : SARSA

$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha ( r_{t} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) )
$$

위의 수식을 SARSA라고 부르는 이유는 $$s_t,a_t,r_t,s_{t+1},a_{t+1}$$을 사용하기 때문입니다.

$$
Q(s_t,a_t) \leftarrow Q(\color{red}{ s }_t,\color{red}{ a }_t) + \alpha ( \color{red}{ r }_{t} + \gamma Q(\color{red}{ s }_{t+1},\color{red}{ a }_{t+1}) - Q(s_t,a_t) )
$$

- Off-Policy TD Control : Q-Learning

$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha ( r_{t} + \gamma \color{blue}{ max_{a_{t+1}} } Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) )
$$

***

![slide17](/assets/images/CS285/lec-7/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-7/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-7/slide19.png)
*Slide. 19.*










## <mark style='background-color: #fff5b1'> Value Functions in Theory </mark>

![slide21](/assets/images/CS285/lec-7/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-7/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-7/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-7/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-7/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-7/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-7/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-7/slide28.png)
*Slide. 28.*







## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/)

- [Deep Learning in a Nutshell: Reinforcement Learning By Tim Dettmers](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/)

- [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)




