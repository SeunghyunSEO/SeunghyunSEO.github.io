---
title: () Lecture 7 - Value Function Methods

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 7의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=pP_67mTJbGw&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=28)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 챕터에서 다룰 내용은 `Value Function based Method`입니다.

![slide1](/assets/images/CS285/lec-7/slide1.png)
*Slide. 1.*

Lecture 9과 그 이후에 Policy-based Method를 몇번 더 다룰 것 같지만 
앞으로 `7,8장`은 Value based로 policy를 `implicit`하게 배우는 Value Iteration 등과 더 나아가서는 (Deep) Q-Learning에 대해서 배우게 될겁니다. 

![rl_taxonomy_intellabs_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_intellabs_for_lec6.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec6](/assets/images/CS285/lec-7/rl_taxonomy_openai_for_lec6.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))




## <mark style='background-color: #fff5b1'> Recap </mark>

![slide2](/assets/images/CS285/lec-7/slide2.png)
*Slide. 2.*

지난 강의에서 `Critic`을 도입한 정책 경사 알고리즘인 `Actor-Critic Algorithm`에 대해서 알아봤습니다.
*Slide. 2.*에 나와있는 알고리즘은 Trajectory를 뽑아놓고 학습하는 `Batch-mode`로 2,3,4번 스텝이 중요했었죠.







## <mark style='background-color: #fff5b1'> Can we omit policy gradient entirely? </mark>

7장의 주제인 `Value Function based Method`은 "`policy에 대한 요소를 빼고` Value Function을 잘 학습시켜서 
학습시킨 함수를 통해 어떻게 행동할지?를 결정할 순 없을까?" 라는 아이디어에서 출발합니다.


이게 가능한 이유는 우리가 어떤 상태에 놓여져 있을때, Policy를 통해서 가장 확률이 높은 행동을 취해 다음 스텝으로 나아가는건데,
가치 함수라는 것이 그러한 가능한 다음 상태들에 대한 가치를 알려주는 함수이고, 이를 통해서 나아가면 되기 때문입니다.
그러니까 `explicit policy neural network`는 더이상 필요가 없는거죠.




![slide3](/assets/images/CS285/lec-7/slide3.png)
*Slide. 3.*

앞서 많이 다뤘던 `Advantage Function`은 현재 상태에서 $$a_t$$ 라는 행동을 하는게 다른 옵션들을 골랐을때의 평균보다 얼마나 좋은가? 를 나타냈죠.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$

그렇다면 $$A^{\pi}$$에서 가장 높은 값을 나타내는 행동을 하나 뽑으면 어떨까요?

$$
arg max_{a_t} A^{\pi} (s_t,a_t) \scriptstyle{\text{ best action from } s_t \text{ , if we then follow } \pi } \\
$$

우리가 $$\pi$$ 정책을 계속 따른다는 가정하에, $$s_t$$상황에서 최선의 선택이 될겁니다 (그게 리턴하는 값이 제일 높으니까요).


이번 장에서부터는 한번 policy를 explicit하게 두지 않고 (네트워크를 따로 두지 않는다는 말) Advantage Function을 argmax해서 행동을 선택하는것으로 대체해서 강의를 진행을 하게 될겁니다.


그렇다면 우리는 `새로운 (업데이트 될) policy`를 아래와 같이 정의할 수 있습니다. (네트워크가 따로 있는건 아닙니다.)

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.

$$

이렇게 함으로써 매 iteration마다 새로운 policy를 얻을 수 있습니다.
우리는 이제 `Neural Netowrk for Advantage Function`만 학습하면 됩니다.

Anatomy를 보시면 아래와 같은 차이가 있습니다.

![policy_vs_value](/assets/images/CS285/lec-7/policy_vs_value.png)
*Fig. Blue Box가 다른것을 알 수 있다. 더이상 Blue Box에서 정책을 학습하지 않는다.*






*Slide. 4.*에 나와있는것은 `Policy Iteration` 알고리즘의 high-level idea 입니다. 

![slide4](/assets/images/CS285/lec-7/slide4.png)
*Slide. 4.*

`Policy Iteration`은 간단합니다. 

- 1.현재 정책을 평가한다. 
- 2.정책을 업데이트한다. 
- 3.이를 정책이 수렴할 때 까지 반복한다.

위의 두 가지 스텝을 반복하는겁니다. 정책을 평가하고 업데이트하는 것을 반복하기 때문에 `Policy Iteration`이란 이름이 붙은거죠.


2번째 스텝은 discrete action space일 경우 다순히 argmax를 취하면 되는것이니 굉장히 간단한데요 (continuous일 경우 조금 복잡하지만 나중에 다룰것임),
그렇다면 1번은 어떻게 해야할까요? 과연 어떻게 $$\pi$$를 평가해야 하는걸까요?


이전에 배웠듯 Advantage는 아래와 같습니다.

$$
A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E} [ V^{\pi}(s') ] - V^{\pi}(s)
$$

(여기서 $$s'$$는 다음 state이고, $$s$$는 현재 state)


이제 $$V^{\pi}(s)$$를 평가 (evaluate) 해봅시다.





![slide5](/assets/images/CS285/lec-7/slide5.png)
*Slide. 5.*

Policy iteration 을 위한 advantage를 추정하기 위해서 $$V^{\pi}(s)$$를 구하는 방법에는 `Dynamic Programming, DP`가 있습니다. 


이에 대해 설명하기 위해서 일단 우리가 아래의 세가지를 안다고 합시다.

- Very Small Discrete State Space : $$s$$ - 미로 16칸 
- Very Small Discrete State Space : $$a$$ - 상,하,좌,우
- State Transition Probability : $$p(s' \vert s,a)$$ 

여기서 상태 천이 행렬은 $$16 \times 16 \times 4$$으로 3차원 텐서가 됩니다.
또한 매 상태마다 가치가 매겨질테니 $$V^{\pi}(s)$$는 16개가 됩니다.


이는 잘 알려진 DP 세팅인데요, 이러한 세팅은 `Model-Free RL`과는 다르지만 ($$p(s' \vert s,a)$$가 있으므로) 일단은 이 세팅에서 DP를 유도하고, 그 다음에 Model-Free로 넘어가겠다고 하네요.

```
여기서 동적 프로그래밍 (Dynamic Programming, DP)란 어떤 문제를 풀 때
이를 여러개의 작은 문제 (sub-problem)로 쪼개서 푸는 것을 말합니다.

DP는 완벽하게 환경을 알 경우 (상태 천이 행렬 등) optimal solution을 계산할 수 있게 해줍니다.
```

이러한 세팅에서 lecture 6에서 봤던 가치 함수에 대한 `Bootstrapped update`는 다음과 같이 쓸 수 있습니다.

$$
V^{\pi}(s) \leftarrow \mathbb{E}_{a \sim \pi(a \vert s)} [r(s,a) + \gamma \mathbb{E}_{s' \sim p(s' \vert s,a)} [V^{\pi}(s')] ]
$$

여기서 

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.

$$

이기 때문에 위의 수식을 간단히해서

$$
V^{\pi}(s) \leftarrow r(s,\pi(s)) + \gamma \mathbb{E}_{s' \sim p(s' \vert s,\pi(s))} [V^{\pi}(s')] ]
$$

위와 같이 나타낼 수 있습니다 (모든 가능한 액션을 다 해본다는 기대값이 없어졌습니다. 어차피 argmax로 어떤 상태에서 취할 행동은 `정해져 (deterministic)` 있기 때문에).


![slide6](/assets/images/CS285/lec-7/slide6.png)
*Slide. 6.*

Advantage를 추정하는건 결국 Value function $$V^{\pi}(s)$$를 평가하는것이기 때문에 (모든 state마다 점수를 부여),
*Slide. 6.*에서 보시는 바와 같이 반복적으로 매 state마다 value를 부여하고 (반복적으로), 그리고 정책을 업데이트하고 ... 를 반복하면 됩니다.


***

요약하자면 

```
1. 지금 상황은 Tabular를 만들 수 있을 정도로 작은 state와 action space이므로 tabular를 만든다. 
2. 4x4짜리 격자구조에 모두 가치를 매긴다. (계산 가능함)
3. 여기서 argmax를 통해 모든 상황별로 어떤 액션을 취하는게 베스트인지를 알 수 있으며, 이 policy로 과거 policy를 대체한다.
4. 2번과 3번을 반복한다.

5. policy가 더이상 변하지 않으면 수렴한 것이다.
```

![v_s](/assets/images/CS285/lec-7/v_s.png)
*Fig. 몇 번이나 evaluation을 반복했는지는 모르겠으나 매 state(격자)마다 가치 ($$V^{\pi}(s)$$)가 부여되어있다.*

***


Policy Iteration이 explicit한 정책 없이 잘 작동하는 것 처럼 보이지만 이보다 더 간단한 DP porcess가 있고, 이는 아래의 *Slide. 7.*에 나와있는데요,


![slide7](/assets/images/CS285/lec-7/slide7.png)
*Slide. 7.*

우리가 아래의 `new policy`를 argmax를 통해서 구하는 과정에서

$$
\pi'(a_t \vert s_t) = 

\left\{\begin{matrix}
1 \text{ if } a_t = arg max_{a_t} A^{\pi} (s_t,a_t)
\\ 
0 \text{ otherwise}
\end{matrix}\right.
$$

사용하는 것은 Advantage function 이었는데요, 이는 현재 reward + (다음 state의 기대값 - 현재 state의 가치) 였죠.

$$
A^{\pi} (s,a) = r(s,a) + \gamma \mathbb{E}[ V^{\pi} (s') ] - V^{\pi}(s)
$$

여기서 $$V^{\pi}(s)$$를 빼버리면 이는 Q Function과 다를 바 없다는 걸 알게됩니다.
왜냐하면 $$a_t$$에 관해서 argmax 해버리는 것이기 때문이죠.

$$
arg max_{a_t} A^{\pi} (s,a) = arg max_{a_t} Q^{\pi} (s_t,a_t)
$$

즉 $$Q^{\pi} (s_t,a_t)$$를 통해서도 new policy를 구할 수 있는 겁니다.


우리는 이를 이용해서 새로운 알고리즘을 생각해볼 수 있는데요, 이는 `Value Iteration`이라고 하며 아래와 같은 과정을 따릅니다.

- 1. $$Q(s,a)$$ 를 구한다. (table을 만든다.) (현재 reward, $$r(s,a)$$ + 다음 state의 기대값)
- 2. $$Q(s,a)$$에서 action에 대해 max인 값을 구해서 $$V(s)$$를 구한다.

![s_a](/assets/images/CS285/lec-7/s_a.png)

여기서는 Value를 direct로 구하지도 않고 policy를 계산하는 과정도 빠져있습니다.


`Policy Iteration`과 `Value Iteration`을 비교하면 아래와 같습니다.


![policy_iteration_vs_value_iteration](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration.png)
*Policy Iteration VS Value Iteration*


![policy_iteration_vs_value_iteration2](/assets/images/CS285/lec-7/policy_iteration_vs_value_iteration2.png)
*Policy Iteration VS Value Iteration 2*









### <mark style='background-color: #dcffe4'> Policy Iteration Example - 4x4 Grid World </mark>

강화학습 분야의 세계적인 석학 [Richard S. Sutton ](https://scholar.google.ca/citations?user=6m4wv6gAAAAJ&hl=en) 의 책이자 강화학습 입문서로 유명한 [Reinforcement Learning: An Introduction.](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)를 보면 아래와 같이 `Policy Iteration`이 어떻게 동작하는지에 대한 figure가 있습니다.

![sutton_RLbook2020_policy_iteration1](/assets/images/CS285/lec-7/sutton_RLbook2020_policy_iteration1.png)
*Fig. 4x4 Grid World에서 음영 부분으로만 가면 되고, 매번 이동할 때 마다 -1점씩 점수를 까먹으므로 최단경로로 가는게 제일 좋은 정책일 것이다.*

`Policy Iteration`을 통해서 k번째 스텝의 $$V^{\pi}$$와 이를 argmax한 $$\pi$$를 보면 아래와 같습니다.

![sutton_RLbook2020_policy_iteration2](/assets/images/CS285/lec-7/sutton_RLbook2020_policy_iteration2.png)
*Fig. 반복하다보면 결국 어떤 상태에서 어떤 행동을 해야 하는지에 대한 deterministic optimal policy를 얻게 된다.*


***

이를 조금 더 디테일하게 나타내면 아래와 같이 나타낼 수 있습니다.
(출처는 $$\rightarrow$$ [Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/))

![im20](/assets/images/CS285/lec-7/im_20.png)
*Fig. 서튼의 책과 같은 세팅, 같은 목표*

![im21](/assets/images/CS285/lec-7/im_21.png){: width="50%"}
*Fig. 맨 처음 아무것도 안했을 때의 $$V^{\pi}_0$$*

![im22](/assets/images/CS285/lec-7/im_22.png)
*Fig. $$V^{\pi}_1$$를 모든 상태마다 계산하는 과정*

![im23](/assets/images/CS285/lec-7/im_23.png){: width="50%"}
*Fig. $$V^{\pi}_1$$ 를 격자구조에 나타낸 결과*

![im24](/assets/images/CS285/lec-7/im_24.png)
*Fig. $$V^{\pi}_2$$를 모든 상태마다 계산하는 과정*

![im25](/assets/images/CS285/lec-7/im_25.png)
*Fig. $$V^{\pi}_1$$를 모든 상태마다 계산하는 과정 2*

![im26](/assets/images/CS285/lec-7/im_26.png)
*Fig. $$V^{\pi}_k$$가 매 스텝마다 어떻게 바뀌는지를 보여줌*

![im28](/assets/images/CS285/lec-7/im_28.png)
*Fig. 어떻게 argmax를 통해서 policy를 찾아내는지에 대한 과정 (random policy로 시작했지만 이번 스텝이 끝나면 결국 현재 가치 함수를 기반으로 선택된 policy가 리턴될 것임)*

![im30](/assets/images/CS285/lec-7/im_30.png)
*Fig. 이렇게 Evaluation (E), Improvement (I) 를 반복하다보면 optimal policy에 도달하게 됨*








### <mark style='background-color: #dcffe4'> Value Iteration Example - Grid World </mark>


![nvidia_value_iteration](/assets/images/CS285/lec-7/nvidia_value_iteration.png)
*Fig. Value iteration constructs the value function over all states over time. Here each square is a state: S is the start state, G the goal state, T squares are traps, and black squares cannot be entered. In value iteration we initialize the rewards (traps and goal state) and then these reward values spread over time until an equilibrium is reached. Depending on the penalty value on traps and the reward value for the goal different solution patterns might emerge; the last two grids show such solution states.*

(출처 : [Deep Learning in a Nutshell: Reinforcement Learning By Tim Dettmers](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/))










## <mark style='background-color: #fff5b1'> Fitted Value Iteration & Q-Iteration </mark>

![slide9](/assets/images/CS285/lec-7/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-7/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-7/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-7/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-7/slide13.png)
*Slide. 13.*













## <mark style='background-color: #fff5b1'> From Q-Iteration to Q-Learning </mark>

![slide15](/assets/images/CS285/lec-7/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-7/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-7/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-7/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-7/slide19.png)
*Slide. 19.*










## <mark style='background-color: #fff5b1'> Value Functions in Theory </mark>

![slide21](/assets/images/CS285/lec-7/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-7/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-7/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-7/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-7/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-7/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-7/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-7/slide28.png)
*Slide. 28.*







## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/)

- [Deep Learning in a Nutshell: Reinforcement Learning By Tim Dettmers](https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/)





