---
title: 5.1 - 5.2 Feed-Froward Netwrok and Network Training
categories: Brief_Review_for_PRML
tag: [PRML,MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---

***

시작하기에 앞서 이 글은 유명한 머신러닝 서적 중 하나인 [Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/) 과 이 책을 요약한 [홍기호 님](https://github.com/norman3)의 [PRML 요약 정리 post](http://norman3.github.io/prml/)를 
조금 더 간략하게 요약하고 추가설명을 덧붙힌 글 임을 밝힙니다.

(공개용은 아니고 혼자 공부하기 위한 용도)

***

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> 5.1 Feed-Forward Network </mark>

고정된 개수의 비선형 기저 함수를 사용하는 선형 결합 모델은 다음과 같이 기술될 수 있습니다.

$$y({\bf x}, {\bf w}) = f\;\left(\;\sum_{j=1}^{M}w_j\phi_j({\bf x}\;)\right) \qquad{(5.1)}$$

여기서 함수 \\( f(\cdot) \\) 는 활성 함수(activation function)이며, 
회귀 문제에서는 활성 함수가 *identity* 함수. 즉, 입력 값을 그대로 반환하는 함수이고, 
분류 문제에서는 시그모이드( *sigmoid* )나 소프트맥스( *softmax* ) 같은 비선형 함수가 사용되었. 

원래는 기저 함수 \\( \phi\_j({\bf x}) \\) 에 조정할 파라미터 계수 \\( {w\_j} \\) 를 내적하는 식을 사용하는데, 
신경망의 경우 기저 함수는 그냥 \\( \phi={\bf x} \\) 함수를 사용하게 됩니다.


아래는 가장 기본적인 형태의 신경망 모델입니다.

$$a_j=\sum_{i=1}^{D}w_{ji}^{(1)}x_i + w_{j0}^{(1)} \qquad{(5.2)}$$

여기서 \\( D \\) 는 입력 벡터 \\( {\bf x} \\) 의 차원으로 \\( {\bf x} = (x\_1,...,x\_D) \\) 라고 생각하면 됩니다.


\\( j=1,...M \\) 인 값으로 해당 레이어(layer)의 노드 개수로 생각하면 된다. 레이어는 첨자로 쓰여진 \\( (1) \\) 과 같은 형태로 나타내어지며,
현재 레이어(layer)가 \\( (1) \\) 이므로 첫번째 레이어에 속한 노드의 개수는 총 \\( M \\) 개 입니다.


우리는 \\( w_{ji}^{(1)} \\) 은 `가중치 (weight)` 라고 부르며, \\( w_{j0} \\) 는 `바이어스 (bias)` 라고 부르며,
\\( a_j \\) 는 3장에서와 마찬가지로 `활성자 (activations)` 라고 부릅니다.


위의 값들은 이제 서로 다른 비선형의 `활성 함수 (activation function)` \\( h(\cdot) \\) 에 의해서 변환되게 됩니다.
(이는 sigmoid 부터 relu까지 다양합니다.)

$$z_j=h(a_j) \qquad{(5.3)}$$

신경망에서는 위와 같은 작업을 처리하는 노드를 `히든 유닛(hidden units)`이라고 부르며,
보통 비선형 함수인 \\( h(\cdot) \\) 함수는 `시그모이드(sigmoid) 함수`나 `하이퍼볼릭 탄젠트(tanh)` 함수를 주로 사용하게 된다.

$$a_k=\sum_{j=1}^{M}w_{kj}^{(2)}z_j + w_{k0}^{(2)} \qquad{(5.4)}$$

이렇게 얻어진 최종 출력물은 위와 같습니다.

여기서는 \\( k=1,...K \\) 이고 \\( K \\)는 출력할 필드의 개수가 됩니다.

여기에 활성 함수(activation function)를 적용하면 최종 출력값인 \\( y\_k \\) 를 얻을 수 있는데, 한 번 시그모이드를 활용한 식으로 전개를 해보겠습니다.

$$y_k=\sigma(a_k) \qquad{(5.5)}$$

$$\sigma(a)=\dfrac{1}{1+\exp(-a)} \qquad{(5.6)}$$

위의 경우는 binary-class 문제를 해결할 때 사용되며, 앞서 배운 것 처럼 multi-class 문제에서는 `softmax`를 사용하면 됩니다.


이제 지금까지 설명했던 구조를 하나의 식으로 표현해 보겠습니다.

$$y_k({\bf x}, {\bf w})=\sigma\;\left(\sum_{j=1}^M w_{kj}^{(2)}\cdot h\;\left(\sum_{i=1}^{D}w_{ji}^{(1)}+w_{j0}^{(1)}\right)+w_{k0}^{(2)}\right) \qquad{(5.7)}$$


![Fig5.1](/assets/images/PRML_5.1_to_5.2/Fig5.1.png)
*Fig. 5.1. Network Diagram*


우리는 3.1 절에서 논의한 것처럼 바이어스는 \\( x_0=1 \\) 을 도입하여 좀 더 간단한 수식으로 정리할 수 있습니다.

$$a_j \sum_{i=0}^{M}w_{ji}^{(1)}x_i \qquad{(5.8)}$$

마찬가지로 2번째 레이어에서도 이 식을 적용하면, 아래처럼 됩니다.

$$y_k({\bf x}, {\bf w}) = \sigma\;\left(\sum_{j=0}^{M}w_{kj}^{(2)}\cdot h\;\left(\sum_{i=0}^{D}w_{ji}^{(1)}x_i\right)\right) \qquad{(5.9)}$$







### <mark style='background-color: #dcffe4'> MLP (*Multi-layer perceptrons*) </mark>

4.1.7 절에 설명한 퍼셉트론(perceptron) 모델과 신경망 모델은 사실 동일하므로 멀티 레이어(layer)인 경우 이를 *MLP* 라고 부를 수 있습니다.
(다만, 신경망은 활성 함수로 시그모이드 스타일의 함수를 쓰고 퍼셉트론은 (불연속이고 비선형인) Step 함수를 사용한다는 차이가 있습니다.)


만약 활성 함수에 비선형 함수가 아닌 선형 함수를 도입하면 어떻게 될까요?
활성 함수 자리에 선형 함수가 도입되면 최종적으로 \\( y_k({\bf x}, {\bf w}) \\) 함수는 비선형이 아니라 선형 함수가 된다.
왜냐하면 선형 함수의 결합은 다시 선형 함수가 되기 때문입니다. 
결국 히든유닛이 없는 함수와 동일한 상태가 되고, 이러한 히든 유닛이 없는 신경망은 단일 퍼셉트론과 차이가 없고, 따라서 그냥 선형 분류기가 됩니다.


따라서 MLP를 사용하려 했지만 퍼셉트론을 쓰는 것과 동일한 삽질을 하게 되는 것인데, 이는 예를 들어 비선형 함수를 이용해 분류 시 결정 경계의 표현력 등을 증가시키려는 이득을 없애는 것과 같습니다.


위의 2-layer 모델보다 좀 더 일반화된 모델 생성도 가능하며, 추가로, `skip-layer` 를 도입할 수도 있.


#### < Skip Layer and Sparse Network > 
예를 들어 2-layer 모델을 사용하되, 일부 노드는 입력 변수에서 출력 변수로 바로 전달되는 엣지가 생성될 수도 있습니다.
레이어간 모든 연결이 존재하는 Dense 네트워크가 일반적이지만, 일부 노드만 연결되어 있는 모델도 있습니다.
이런 모델을 Convolutional-NN 이라고 합니다(?).(5.5.6절에 참고)
하지만 특별한 언급이 없다면 신경망은 그냥 Dense 신경망이라 생각하면 된다.

![Fig5.2](/assets/images/PRML_5.1_to_5.2/Fig5.2.png)
*Fig. 5.2. Sparse Network*


`skip-layer` 또는 `sparse-network` 등의 신경망은 좀 더 일반화된 신경망이라 볼 수 있습니다.


하지만 여기서도 feed-forward 라는 제한은 남아있으며, 이로 인해 출력값은 입력에 대한 결정(deterministic) 함수가 됨을 보장한다.
그냥 최종 결과가 항상 도출될 수 있다는 것으로 간주하면 된다는 말인데,
이를 일반화한 식을 기술해보면 다음과 같습니다.

$$z_k=h\;\left(\sum_j w_{kj}z_j\right) \qquad{(5.10)}$$

이는 한 노드로 들어오는 모든 노드들의 합을 표현한 식입니다.


(additional)

![skip1](/assets/images/PRML_5.1_to_5.2/skip1.png)
*Fig. Effectness of Skip Connection in Optimization*


![skip2](/assets/images/PRML_5.1_to_5.2/skip2.png)
*Fig. Effectness of Skip Connection in Optimization 2*

(이미지 출처 : [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/pdf/1712.09913.pdf))


### <mark style='background-color: #dcffe4'> Universal Approximation </mark>

사실 feed-forward 네트워크를 활용한 근사법은 오랜 기간을 거쳐 광범위하게 연구되어 왔습니다.
신경망은 범용적인 근사법을 제공하는데, 예를 들어 2-layer 신경망일지라도, 충분한 히든 유닛을 사용하기만 한다면 어떠한 연속 함수일지라도 근사해 낼 수 있습니다. (수학적으로 증명되어 있음.)


임의의 활성 함수를 사용해도 근사할 수 있으나 물론 다 되는건 아님. 예를 들어 다항함수(polynomial)는 사용하면 안된다고 합니다. 
물론 히든 유닛을 어마어마하게 넣게 되면 학습 시간이 너무 길어지거나, 현실적으로는 계산이 불가능해질 수 있습니다.


2-layer 신경망으로 근사한 함수의 결과는 다음과 같습니다.

![Fig5.3](/assets/images/PRML_5.1_to_5.2/Fig5.3.png)
*Fig. 5.3. Universal Function Approximator*



위의 그림에서 각 점들은 각각 (a) \\( x^2 \\) , (b) \\( sin(x) \\) , (c) \\( \|x\| \\) , (d) \\( H(x) \\) 로부터 생성된 점들입니다. ( \\)H \\) 는 스텝함수)
    - 각각 50개의 샘플을 만들었으며, 히든 유닛으로 3개를 이용하여 결과를 만들어냈다.
    - 활성 함수는 \\( tanh(\cdot) \\) 를 사용하였다.
    - 근사한 식은 붉은색 선으로 실제 결과와 매우 유사함을 알 수 있다.
    - 물론 초기 \\( {\bf w} \\) 값을 어떻게 선정하느냐에 따라 결과가 많이 달라질 것이므로 어떻게 선정하느냐가 중요한데, 책에는 안 나와있다.


![Fig5.4](/assets/images/PRML_5.1_to_5.2/Fig5.4.png)
*Fig. 5.4.*


위의 그림은 2-layer 신경망을 간단하게 도식화한 그림입니다.
    - 파란색 점선이 히든 레이어로 인해 구분되는 선형식을 의미한다. ( \\)z=0.5 \\) 일 때의 값이다.)
    - 붉은 색 선이 최종적으로 얻은 분류식이다.
    - 녹색 선이 실제 최적화된 분류선이 된다. (근데 이게 어떤 기준에 의해 선정되었는지는 모름.)





### <mark style='background-color: #dcffe4'> 5.1.1. 공간의 대칭성 (Weight-space symmetries) </mark>

베이즈 모델과 비교하여 feed-forward 네트워크가 가지는 독특한 특성은,
서로 다른 \\( {\bf w} \\) 에 대해서도 동일한 입력에 대해 동일한 출력 결과를 만들어낼 수 있다는 것입니다.
이러한 특성을 `Symmetry` 라고 하는데, 다음과 같은 `Symmetry` 가 있습니다.

- `Sign-flip symmetry`
    - 예를 들어 일반적인 2-layer 네트워크에서 히든 유닛에 대한 활성 함수가 \\( \tanh \\) 라고 할 때,
    - 수학적으로 \\( \tanh(-a) = - \tanh(a) \\) 가 성립하는 것을 이미 알고 있다. (odd function)
    - 따라서 어떤 히든 유닛 \\( j \\) 에 대해 연결되어 입력되는 \\( w\_{ji} \\) 벡터의 부호를 바꾸면 출력 값은 기존 값에 부호만 바뀐 값이 된다.
    - 이 때 그 다음 레이어에서는 다시 \\( w\_{kj} \\) 의 부호가 바뀌게 되면 이전과 동일한 결과를 얻게 된다. ( \\)w_{kj} \cdot o_j \\) 이므로)
    - 즉, 부호만 반대인 경우가 두 단계를 거치면 동일한 결과가 만들어지는 현상을 만들 수 있다.
    - 2-layer에서 히든 유닛이 \\( M \\) 개의 경우 이러한 경우를 총 \\( 2^M \\) 개 만들어 낼 수 있다. (조합)
    - 사실 \\( \tanh(\cdot) \\) 가 예를 들기 쉬워서 이야기한 것이지만 다른 활성 함수에서도 이런 현상을 만들어 낼 수 있다.

- `Interchange symmetry`
    - 같은 레이어 내 임의의 히든 유닛 2개에 대해 서로 위치를 바꾸게 되는 경우에도 최종 출력 값은 변함이 없다. (당연하다.)
    - 하지만 이런 경우 인덱스 번호는 바뀌게 되므로 \\( {\bf w} \\) 벡터 자체는 서로 다른 벡터로 취급할 수 있다.
    - 따라서 2-layer에서는 이런 경우를 총 \\( M! \\) 개 만들 수 있다.
    
종합하자면, 2-layer 환경에서는 간단하게 생각해봐도 동일한 결과를 만들어 낼 수 있는 \\( {\bf w} \\) 경우가 대략 \\( M!\times 2^M \\) 개나 됩니다.
(다행인지 불행인지 \\( {\bf w} \\) 에 대한 symmetry 현상은 이 두 가지가 전부이다.)


결론은 동일한 결과를 가지는 \\( {\bf w} \\) 가 무궁무진 할 수 있으므로 잘 찾아야 한다는 것 입니다.
(전역 최소점을 찾을 수 없는 환경에서 동일한 에러 값을 가지는 \\( {\bf w} \\) 벡터가 다수 존재한다는 것은 구현자 입장에서 부담일 수 밖에 없다.)








***






## <mark style='background-color: #fff5b1'> 5.2 Network Training </mark>

- 지금까지 입력 벡터 \\( {\bf x} \\) 로부터 파라미터를 가지는 비선형 함수를 적용하여 출력 벡터 \\( {\bf y} \\) 를 만드는 신경망에 대해 살펴보았다.
- 앞서 최소 제곱법을 이용하여 다항식 커브 피팅 문제를 해결한 것처럼(1.1절) 여기서도 이런 형태로 에러 함수를 정의할 수 있다.

$$E({\bf w}) = \dfrac{1}{2}\sum_{n=1}^{N}\|{\bf y}({\bf x}_n, {\bf w})-{\bf t}_n\|^2 \qquad{(5.11)}$$

- 이제 이걸 확률 모델을 이용해서 식을 전개해보자.
    - 이러한 확률적 해석은 네트워크 학습에 대한 일반적인 관점을 제공해주고,
    - 풀려고하는 문제에 맞는 출력 함수와 에러 함수 선정 방식을 적절하게 설명할 수 있다.





### <mark style='background-color: #dcffe4'>  회귀(Regression) </mark>

- 이제 신경망을 이용해서 3장에서 다루었던 회귀(regression) 문제를 풀어보도록 하자.
    - 출력 값은 1차원의 실수 값으로 \\( t \\) 라고 표기했었다.
    - 우리가 가정했던 사실은 \\( t \\) 가 \\( x \\) 에 종속적인 평균 값을 가지는 가우시안 분포를 따른다는 것이었다.
    - 이를 수식으로 표현하면 다음과 같다.
    
$$p(t|{\bf x}, {\bf w}) = N(t\;|\;y({\bf x}, {\bf w}), \beta^{-1}) \qquad{(5.12)}$$

- 여기서 \\( \beta^{-1} \\) 은 가우시안 노이즈의 정확도(precision)를 의미한다. (분산의 역수이다.)
    - 지금까지 다 봤던 내용들이므로 후딱 후딱 넘기도록 하자.
- 가능도 함수(likelihood)를 구하기 위해 식을 설정하도록 한다.
    - 입력값 : \\( {\bf X}=({\bf x}\_1,...,{\bf x}\_N) \\) 이고 서로 독립적으로 발현된다. ( *i.i.d* )
    - 출력값 : \\( {\bf t}={t\_1,...,t\_N} \\) 이고 1차원 실수 값

$$p({\bf t}\;|\;{\bf X}, {\bf w}, {\bf \beta}) = \prod_{n=1}^{N}p(t_n\;|\;{\bf x}_n, {\bf w}, \beta)$$

- 음의 로그항을 이용하여 식을 전개하면 다음과 같은 에러 함수를 얻을 수 있다.

$$\frac{\beta}{2}\sum_{n=1}^{N}\{y(x_n, {\bf w})-t_n\}^2 - \frac{N}{2}\ln\beta + \frac{N}{2}\ln(2\pi) \qquad{(5.13)}$$

- 여기서 사용될 \\( y \\) 함수는 앞서 설명했다. (식 5.7 참고)
    - 즉 2-layer 모델을 기준으로 식을 전개해 보도록 하자.
- 이후에 이 식을 이용하여 베이지언 방식을 이용한 풀이법을 설명할 것이지만, 우선은 *MLE* 방식을 설명한다.
- 사실 신경망에서는 로그 가능도 함수를 최대화하는 문제로 처리하는 방식보다 에러 함수를 최소화하는 문제로 처리하는 것이 일반적이다.
- 물론 가능도 함수를 최대화하는 문제나 에러 함수를 최소화하는 문제나 모두 동일한 식을 만들어내는 것은 이전 장에서도 다 보았던 것들이다.
- 따라서 회귀 문제를 푸는데 있어 가능도 함수가 아닌 에러 함수를 정의하고, 정의된 에러 함수로부터 가장 적합한 \\( {\bf w} \\) 를 구하는 문제로 진행하도록 해보자.

$$E({\bf w}) = \frac{1}{2}\sum_{n=1}^{N}\{y({\bf x}_n, {\bf w})-t_n\}^2 \qquad{(5.14)}$$

- 원래 하던대로 *MLE* 를 구하면 된다. 
    - 다만 신경망에서는 \\( y({\bf x}, {\bf w}) \\) 가 비선형이기 때문에 \\( E({\bf w}) \\) 가 `nonconvex` 이다.
    - 이 말은 미분을 해도 전역 최소점을 찾을 수 없다는 의미이다.
- 따라서 이를 구하는 문제는 에러 함수의 로컬 최소값(local minima) 형태로 구해지게 되는데 이건 5.2.1 절에서 설명하도록 하겠다.
- 우선 \\( {\bf w}\_{ML} \\) 은 찾았다고 생각하고 \\( \beta \\) 를 구하는 식부터 좀 보자.
    - 예전에 했던 방식대로 음로그 가능도 함수를 이용해서 구하면 된다.
    
$$\dfrac{1}{\beta_{ML}} = \frac{1}{N}\sum_{n=1}^{N}\{y({\bf x}_n, {\bf w}_{ML})-t_n\}^2 \qquad{(5.15)}$$

- 예전에 구한 식과 차이가 없다. 
- 만약 \\( t \\) 의 결과가 단일 차원이 아니라 벡터가 되면 어떻게 될까?
    - 이것도 예전과 마찬가지로 노이즈는 각 차원에 독립적이고 동일한 값을 가진다고 가정하고 식을 전개하면 된다.

$$p({\bf t}|{\bf x}, {\bf w}) = N({\bf t}\;|y({\bf x}, {\bf w}), \beta^{-1}{\bf I}) \qquad{(5.16)}$$

- 이 경우 최종 식도 거의 유사한 형태로 얻을 수 있게 된다.

$$\dfrac{1}{\beta_{ML} } = \frac{1}{NK}\sum_{n=1}^{N}\|{\bf y}({\bf x}_n, {\bf w}_{ML})-{\bf t}_n\|^2 \qquad{(5.17)}$$

- 여기서 \\( K \\) 는 타겟 값 벡터의 개수이다.
- 앞서 이야기한대로 회귀(regression) 문제에서 활성 함수는 identity 함수를 사용했으므로 \\( y_k=a_k \\) 가 성립한다.
     - 즉, 회귀 문제는 함수 값 자체를 예측하는 문제이므로 활성 함수 \\( f(\cdot)={\bf x} \\) 였었다.
- 따라서 다음과 같은 식이 성립함을 알 수 있다.

$$\dfrac{\partial E}{\partial a_k} = y_k-t_k \qquad{(5.18)}$$

- 조금 뜬금없다 싶지만 이후에 Backpropergation 문제를 다룰 때 등장하므로 여기서 한번 눈에 익혀두면 좋다. 




### <mark style='background-color: #dcffe4'>  이진 분류 (Binary Classification) </mark>

- 이제 **이진 분류**의 문제를 살펴보도록 하자. (즉, classification 문제이다.)
- 앞 장에서 보았듯이 이진 분류 문제에서는 활성 함수로 시그모이드와 같은 함수들이 사용된다.

$$y=\sigma\;(a)\equiv\dfrac{1}{1+\exp(-a)} \qquad{(5.19)}$$

- 위의 식을 사용하면 \\( 0\le y({\bf x}, {\bf w}) \le 1 \\) 의 결과를 얻게 된다.
- 이진 문제이므로 \\( p(C\_1\|{\bf x}) \\) 와 \\( p(C\_2\|{\bf x}) \\) 를 표현해야 하는데 당연히 \\( y \\) 와 \\( 1-y \\) 로 사용한다.
- 이 식은 베르누이 프로세스와 동일하므로 다음과 같이 하나의 식으로 표현 가능하다.

$$p(t|{\bf x}, {\bf w}) = y({\bf x}, {\bf w})^t\{1-y({\bf x}, {\bf w})\}^{1-t} \qquad{(5.20)}$$

- 모든 관찰 데이터는 독립적으로 발생한다고 가정하므로 에러 함수를 최소 제곱법이 아닌 음의 로그 가능도 함수로 정의하면 된다.
- 이를 크로스 엔트로피 (*cross-entropy*) 라고 한다. 3장에서 소개한 내용이다. 

$$E({\bf w}) = - \sum_{n=1}^{N}\{t_n\ln{y_n} + (1-t_n)\ln(1-y_n)\} \qquad{(5.21)}$$

- 분류 문제에서는 노이즈 정확도인 \\( \beta \\) 와 같은 요소는 없다.
- 왜냐하면 관찰 데이터에 이미 명확한 레이블(label) 타겟 값이 결정되어 제공되기 때문이다.
    - 물론 레이블(lebel)이 잘못 표기된 데이터가 존재할 수도 있는 법이다.
    - 뭐, 사실 이런 경우에도 쉽게 확장 가능하기는 하다. 더 이상 깊게 다루지는 않는다.
- 위의 수식에서는 cross-entropy 를 이용했는데, 사실 sum-of-squares 를 사용해도 상관은 없다고 한다.
    - 다만 *Simard* 가 쓴 논문에 따르면 cross-entropy 가 학습 속도도 더 빠르고 일반화 성능도 더 좋다고 한다.

- 3장에서도 살펴보았지만 최종 출력 결과를 \\( K \\) 개의 이진 분류기를 이용해서 다분류기를 만들 수 있다.
    - 이 경우는 최종 출력 결과가 각각의 시그모이드 활성 함수를 가진 \\( K \\) 개의 출력 노드로 생각할 수 있다.
- 이 때의 확률 모델은 다음과 같이 기술할 수 있다.

$$p({\bf t}|{\bf x}, {\bf w}) = \prod_{k=1}^{K}y_k({\bf x}, {\bf w})^{t_k}[1-y_k({\bf x}, {\bf w})]^{1-t_k} \qquad{(5.22)}$$

- 이제 cross-entropy 모델을 적용한다. 따라서 음로그 가능도 함수를 에러 함수로 정의한다.

$$E({\bf w}) = - \sum_{n=1}^{N}\sum_{k=1}^{K} \{t_{nk}\ln{y_{nk} }+(1-t_{nk})\ln(1-y_{nk})\} \qquad{(5.23)}$$

- 여기서 \\( y_{nk} \\) 는 \\( y_{nk}=y_k({\bf x}_n, {\bf w}) \\) 이다.
- 마찬가지로 이 함수를 출력값으로 미분한 식은 어떻게 될까?

$$\dfrac{\partial E}{\partial a_k} = y_k-t_k$$

- 오, 회귀 문제나 분류 문제나 에러 함수를 출력값으로 미분한 값은 동일한 결과가 나온다는 것을 알 수 있다.
- 마찬가지로 여기서도 간단하게 이런 사실만 알고 넘어가도록 하자. 뒤에서 다시 나온다.

- 신경망이 일반적인 선형 분류식과 다른점이 뭘까?
    - 지금까지 살펴본 신경망에서 첫번째 레이어에 속한 가중치 \\( {\bf w} \\) 가 여러 출력값(말단)들 사이에서 다양하게 공유된다는 것을 알 수 있다.
        - 즉, 수식 내에서 보면 하나의 히든 노드가 최종 출력 값에 다양한 가중치를 통해 영향을 미치고 있음을 알 수 있다.
        - 선형 분류기의 경우 이러한 요소들이 모두 독립적으로 적용되고 있는 것과 마찬가지.
    - 첫번째 레이어에서 이루어지는 작업은 사실 비선형 속성 선택(feature selection)을 의미하게 된다.
        - 따라서 좋은 자질을 가지고 있는 속성을 취하는 것이 된다.
    - 서로 다른 출력에 대해 Feature가 공유되어 일반화 측면에 유리하다. (그런데 왜인지는 딱히 감이 오지를 않네.)






### <mark style='background-color: #dcffe4'>  다중 분류 (Multi Class Classification) </mark>

- 이제 최종적으로 다중 분류식을 보자.
- 입력 데이터는 \\( K \\) 개의 서로 다른 클래스 중 하나에 속하게 된다.
- 이전에 다루었던 1-of-K 방식을 사용한다.
- 이를 이용한 일반화된 에러 함수는 다음과 같다.
    - 즉 \\( K \\) 개의 출력 값에 대한 에러 값을 모두 합산하는 형태
    
$$E({\bf w}) = - \sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln y_k({\bf x}_n, {\bf w}) \qquad{(5.24)}$$

- 이 때 \\( y\_k({\bf x}, {\bf w}) = p(t_k=1\|{\bf x}) \\) 이다.
- 사실 4장에서 다루었던 방식으로, 최종적으로 소프트맥스(soft-max) 함수를 얻게 된다.

$$y_k({ {\bf x}, {\bf w} })=\dfrac{\exp(a_k({\bf x}, {\bf w}))}{\sum_j\exp(a_j({\bf x}, {\bf w}))} \qquad{(5.25)}$$

- 이제 재미난 성질을 좀 보자. 위의 식에서 \\( a_k \\) 영역에 어떤 상수를 더해도 이 식은 항상 성립하게 되는데,

$$\exp(a+b)=\exp(a)\exp(b)$$

- 위와 같은 성질이 있기 때문이다.
    - 따라서 위의 식에 대입하여 \\( a\_k \\) 항에 특정 상수를 더하더라도 분자, 분모가 서로 약분되어 상수항은 사라지게 된다.
    - 결국 \\( a\_k \\) 에 어떤 상수를 더해도 최종 식 \\( y\_k \\) 는 변하지 않기 때문에 에러 함수는 \\( {\bf w} \\) 공간에서 특정 방향으로는 상수가 된다.
        - 이게 무슨말이가 싶지만 다음을 기악해보자.
    - 이를 막기위해 정칙화(regularaition)를 도입해야 한다. (당연히 뒤에 나온다.)

- 마지막으로 에러 함수를 \\( a\_k \\) 로 미분한 값을 알아보자. 

$$\dfrac{\partial E}{\partial a_k} = y_k-t_k$$

- 역시나 처음에 봤던 식이 나왔다.
    - 이번 절이 주는 교훈은 아주 명확하다. 
    - 풀고 싶은 문제(즉, 회귀, 이진, 다중분류)에 따라 에러 함수를 다르게 정의하지만,
    - \\( a\_k \\) 에 대한 에러 함수의 미분 값은 \\( y\_k-t\_k \\) 로 모두 동일하다.
    - 이후에 중요하게 사용되므로 꼭 기억하자.
    
    
    
    
    
    
    
### <mark style='background-color: #dcffe4'>  5.2.1 모수 최적화 (Parameter optimization) </mark>

![Fig5.5](/assets/images/PRML_5.1_to_5.2/Fig5.5.png)
*Fig. 5.5.*

- 다음 단계로 어떻게 에러 함수 \\( E({\bf w}) \\) 를 최소화하는 벡터 \\( {\bf w} \\) 를 찾을 수 있는지 확인해보자.
- 위의 그림을 보면 임의의 한 점 \\( {\bf w}_C \\) 에서 미분한 벡터의 방향이 \\( \nabla E \\) 임을 알 수 있다.
    - 임의의 한 점 \\( {\bf w} \\) 에서 약간 떨어진 곳을 \\( {\bf w}+\delta{\bf w} \\) 라고 하면,
    - \\( \delta E \simeq \delta{\bf w}^T\nabla E({\bf w}) \\) 라고 할 수 있다. 
    - 에러 함수는 미분 가능하므로 최소값은 당연히 Gradient 가 없어지는 지점에서 나타나게 된다. ( \\( \nabla E=0 \\) )
    - 그 외에는 \\( -\nabla E({\bf w}) \\) 방향으로 조금씩 이동하면 된다.

$$\nabla E({\bf w})=0 \qquad{(5.26)}$$

- 에러 함수는 비선형 함수에 종속되기 때문에 Gradient 값이 0이 되는 지점이 무척이나 많을 수 있다.
- 이런 지점을 지역 최소점(Local Minimum) 이라고 하고, 이 중 가장 작은 값을 가지는 지점을 전역 최소점(Global Minimum) 이라고 한다.
- 학습의 최종 목적은 전역 최소점(Global Minimum)을 찾는 것이지만 현실적으로 정확한 값을 찾기 어렵기 때문에 몇 개의 지역 최소점(Local Miminum)을 찾아 이 중 가장 작은 값을 가지는 지점을 정답으로 간주한다.
- 비선형 연속 함수의 최적화 문제에서 가중치를 구하기 위한 방법은 다음과 같다.
    - (1) 초기값 \\( {\bf w} \\) 를 정하고,
    - (2) 가중치 공간 내에서 특정 형태의 반복 식을 통해 찾고자 하는 위치를 계속 변경한다.
    - (3) 목적함수에 좀 더 부합되는 \\( {\bf w} \\) 를 업데이트하고 이를 계속 반복함.

$${\bf w}^{(\tau+1)}={\bf w}^{(\tau)}+\nabla{\bf w}^{(\tau)} \qquad{(5.27)}$$

- 여기서 \\( \tau \\) 는 반복 스텝을 의미한다.
- 많은 경우에 Gradient 방식을 이용하여 값을 갱신하게 된다.
- Gradient 방식의 중요성을 이해하기 위해 테일러 급수라를 이용한 에러 함수 근사 방법을 확인해 보는 것도 좋다.






### <mark style='background-color: #dcffe4'>  5.2.2. 지역 이차형식 근사 (local quadratic approximation) </mark>

- 임의의 한 점에서의 함수를 근사하기 위한 여러 방법이 존재하는데,
    - 이 중 테일러 급수를 이용한 함수 근사는 매우 유명함.
    - 간단하게 테일러 급수를 이용한 함수 근사식을 확인해보자. 아래가 테일러 급수이다.

$$f(x) \simeq f(a) + \dfrac{f'(a)}{1!}(x-a) + \dfrac{f''(a)}{2!}(x-a)^2 + \dfrac{f'''(a)}{3!}(x-a)^3 + \cdots $$

- 물론 간단하게 표기할 수도 있다.

$$\sum_{n=0}^{\infty}\dfrac{f^{(n)}(a)}{n!}(x-a)^n$$

- 보시다시피 한 점 \\( a \\) 에서 함수 식을 근사하게 된다.
- 차수가 계속 증가할수록 한 점에서의 근사 값이 더 정확해지지만, 계산량이 너무 많아진다.
- 이제 이 식을 이용해서 에러 함수를 근사해보자.
    - 물론 차수를 무한히 늘릴 수 없으므로 2차까지만 확장한다. (좀 심한 가정 아닌가?)
    
$$E({\bf w}) \simeq E({\bf \widehat{w} }) + ({\bf w}-{\bf \widehat{w} })^T{\bf b} + \frac{1}{2}({\bf w}-{\bf \widehat{w} })^T{\bf H}({\bf w}-{\bf \widehat{w} }) \qquad{(5.28)}$$

- 여기서 못 보던 값인 \\( {\bf b} \\) 와 \\( {\bf H} \\) 가 등장했다.

- \\( {\bf b} \\) 는 \\( {\bf \widehat{w} } \\) 일 때의 \\( E \\) 의 Gradient 값이다.
$${\bf b}\equiv\nabla \left.E\right|_{w=\hat{w} } \qquad{(5.29)}$$

- \\( {\bf H} \\) 는 헤시안(Hessian) 행렬이라고 부른다. ( \\( {\bf H}=\nabla\nabla E \\))

$$({\bf H})_{ij}=\left.\dfrac{\partial E}{\partial w_i \partial w_j}\right|_{w={\hat{w}} } \qquad{(5.30)}$$

- 이걸 최초의 식에 대입하며 풀면 다음의 식을 얻을 수 있다.

$$\nabla E \simeq {\bf b} + {\bf H}({\bf w}-{\bf \widehat{w} }) \qquad{(5.31)}$$

- 만약 \\( {\bf w} \\) 가 \\( {\bf \widehat{w} } \\) 에 충분히 가깝게 접근하면 위의 식은 꽤나 적절한 근사 식으로 생각할 수 있다.
- 자, 이제 특정 지점에서 이차 형식의 근사 식이 에러 함수를 최소화 하는 지역 최적점이라고 가정해보자.
- 그럼 \\( \nabla E=0 \\) 을 만족할 것이고 이 때의 \\( {\bf w} \\) 를 \\( {\bf w}^* \\) 라고 하면,

$$E({\bf w})\simeq E({\bf w }^*) + \frac{1}{2}({\bf w}-{\bf w}^*)^T{\bf H}({\bf w}-{\bf w}^*) \qquad{(5.32)}$$


- 이 때 헤시안 행렬 \\( {\bf H} \\) 는 \\( {\bf w}^* \\) 에 대해 얻어진 행렬이 된다.
- 이 함수가 기하학적으로 어떠한 모양을 가지게 되는지 분석하기 위해 헤시안 행렬에 대한 식을 잠시 전개해 보자.

$${\bf H}{\bf u}_i=\lambda{\bf u}_i \qquad{(5.33)}$$

- 헤시안 행렬은 대칭 행렬이다. (선형대수 책 참고)
    - 따라서 대칭 행렬이 가지는 특징을 그대로 가지게 되는데 이는 고유 벡터와 많은 관련이 있다.
    - 위의 식은 정방 대칭 행렬에서의 고유 벡터 식을 표현한 것이다.
     - 여기서 \\( {\bf u}_i \\) 가 고유 벡터가 된다. 대칭 행렬에 대해 생성되는 고유 벡터끼리는 서로 직교(orthonormal)하므로,

$${\bf u}_i^T{\bf u}_j=\delta_{ij} \qquad{(5.34)}$$ 

- 위와 같은 형태로 기술할 수 있다.
    - 근데 사실 위의 \\( \delta \\) 가 정확하게 뭘 의미하는지 모르겠다.
    - \\( i \neq j \\) 인 경우 0, \\( i = j \\) 인 경우 1인 식을 의미하는 건가? (이거 같다.)
- \\( {\bf x}^T{\bf H}{\bf x} \\) 와 같은 형태는 선형 대수 책에 많이도 나오는 형태이다.
- 여기서는 \\( ({\bf w}-{\bf w}^*) \\) 가 헤시안 행렬 좌, 우로 오는 것을 알 수 있다. 이 벡터는 기저 벡터로 표현이 가능한데,
    - 고유 벡터가 orthonormal 하므로 고유 벡터를 기저 벡터로 사용 가능하다.
    
$${\bf w}-{\bf w}^*=\sum_i \alpha_i{\bf u}_i \qquad{(5.35)}$$

- 위의 식이 가능한 이유는 \\( {\bf u}_i \\) 가 단위 기저 벡터가 되기 때문에 표현 공간 내에서 임의의 한 벡터를 벡터를 기저 벡터의 선형 합으로 표현 가능하기 때문.
- 여기서 \\( \alpha \\) 는 일정한 크기의 상수이다.
- 이와 관련된 내용은 선형 대수 책을 참고하면 대부분의 교재에서 나오는 내용이다.
- 최종 식은 다음과 같이 기술할 수 있다.

$$E({\bf w})=E({\bf w}^*) + \frac{1}{2}\sum_i\lambda_i\alpha_i^2 \qquad{(5.36)}$$

- 헤시안 행렬 \\( {\bf H} \\) 이 양의 정부호 행렬(positive definite) 이 되기 위한 필요 충분 조건(i.f.f)은 다음과 같다.

$${\bf v}^T{\bf H}{\bf v}>0, \;for\;all\;v\neq 0 \qquad{(5.37)}$$

- 왜 헤시안 행렬이 양의 정부호 행렬이 되어야 하는 것일까?
    - 양의 정부호 행렬이어야만 극소값을 가지게 된다.
    - 그 외에는 새들 포인트가 되거나 극대값이 되어버림.
- 이 때 고유 벡터는 기저 벡터가 되기 때문에 고유 벡터로 임의의 벡터를 표현 가능하다.
- 따라서 임의의 벡터 \\( {\bf v} \\) 를 다음과 같이 표기 가능하다.

$${\bf v}=\sum_{i}c_i{\bf u}_i \qquad{(5.38)}$$

- 따라서 다음의 식을 얻게 된다.

$${\bf v}^T{\bf H}{\bf v}=\sum_{i}c_i^2\lambda_i \qquad{(5.39)}$$

- 그리고 \\( {\bf H} \\) 가 양의 정부호 행렬이 되기 위한 필요 충분조건으로 고유 값이 모두 양수( \\)>0 \\)) 여야 한다.
    - 이건 양의 정부호 행렬의 필요 충분 조건이 된다.
    
- 새로운 좌표 축에서 고유 벡터를 이용해 에러 함수를 표현한 그림은 다음과 같다.

![Fig5.6](/assets/images/PRML_5.1_to_5.2/Fig5.6.png)
*Fig. 5.6.*

- 사실 이거 많이 본 그림이다.
- 양의 정부호 행렬을 가져야만 타원형의 결과를 얻을 수 있다.
- 에러 함수는 최소 값인 \\( {\bf w}^* \\) 을 중심으로 가지는 이차 형식의 함수로 정의될 수 있다. 이는 결국 타원이 된다.
- 붉은색 선은 동일한 에러 값을 가지는 위치이며, 타원체를 기준으로 고유 벡터로는 타원의 방향을, 고유값으로는 타원의 퍼짐을 알 수 있다.
- \\( {\bf w}^* \\) 근처의 어떠한 값도 모두 \\( E({\bf w}^*) \\) 보다는 큰 값을 가지게 된다. 
    - \\( \lambda \\) 값이 양수이므로 양의 상수가 에러 값에 추가되게 된다. 
    - \\( \lambda \\) 는 아까 말했듯 타원을 만드려면 양수가 되어야 함.
- 어쨌거나 극소값을 가지기 위한 조건으로 1차식인 경우 아래 식을 만족해야 한다.

$$\left.\dfrac{\partial^2 E}{\partial w^2}\right|_{w^*}>0 \qquad{(5.40)}$$

- 이를 확장하여 \\( D \\) 차원의 경우에는 헤시안 행렬이 양의 정부호 행렬이 되어야 한다. (극소값을 가지기 위해)







### <mark style='background-color: #dcffe4'>  5.2.3 그라디언트 정보 사용하기 (Use of gradient information) </mark>

- 우리는 이후에 역전파 알고리즘(backpropagation)을 사용하기 위해 에러 함수의 경사도(gradient)를 구하는 식을 사용하는 것을 보게 될 것이다.
- 경사도라는 말은 좀 어색하므로 앞으로는 그냥 그라디언트라고 표기할 것이다.
- 이후에 역전파 알고리즘으로 에러 함수의 그라디언트를 효율적으로 평가 가능하다. (뒤에서 보자.)
- 에러 함수를 이차 형식(quadratic)의 근사 식으로 표현한 식을 다시 한번 생각해보자.

$$E({\bf w}) \simeq E({\bf \widehat{w} }) + ({\bf w}-{\bf \widehat{w} })^T{\bf b} + \frac{1}{2}({\bf w}-{\bf \widehat{w} })^T{\bf H}({\bf w}-{\bf \widehat{w} })$$

- 여기서 얻어야할 요소들은 \\( {\bf b} \\) 와 \\( {\bf H} \\) 이고 이들은 총 \\( W(W+3)/2 \\) 개의 파라미터로 이루어져 있다.
    - 헤시안 행렬은 대칭 행렬이므로 위와 같은 개수가 나온다.
- 여기서 \\( W \\) 는 벡터 \\( {\bf w} \\) 의 차수를 의미한다.
- 여기서 지역 최소점은 이 이차 형식의 최소점이 되고 따라서 이 때의 연산량은 \\( O(W^2) \\) 에 영향을 받게 된다. (이차식이기 때문)
    - 에러 값을 구할 때 \\( O(W^2) \\) 만큼의 연산이 들어간다고 생각하면 된다.
    - 따라서 함수를 평가하기 위해서 \\( O(W^2) \\) 만큼의 비용이 들고,
    - 이러한 평가를 각각의 \\( {\bf w} \\) 요소에 대해 변경을 해보면서 수행해야 하므로 \\( O(W) \\) 의 비용이 든다.
    - 따라서 전체 필요한 비용은 \\( O(W^3) \\) 이 된다.
- 하지만 그라디언트 정보를 사용하게 되면 어떻게 될까?
    - \\( W \\) 개의 가중치에 대해 이미 \\( \nabla E \\) 값이 존재한다고 하면 그냥 \\( O(W) \\) 로 데이터를 갱신할 수 있다.
    - 마찬가지로 backpropergation 알고리즘을 사용하게 되면 \\( \nabla E \\) 를 구하는데 \\( O(W) \\) 의 비용이 들어간다.
    - 총 \\( O(W^2) \\) 으로 계산을 끝마칠 수 있다.
        







### <mark style='background-color: #dcffe4'>  5.2.4 그라디언트 감소 최적화 (Gradient descent optimication) </mark>




#### < Batch Method >

- 가장 손쉬운 접근법은 가중치를 업데이트할 때 에러 함수의 그라디언트를 사용하는 것.

$${\bf w}^{(\tau+1)} = {\bf w}^{(\tau)} - \eta \nabla E({\bf w}^{(\tau)}) \qquad{(5.41)}$$

- 이 때 \\( \eta > 0 \\) 을 만족해야 하며 \\( \eta \\) 는 학습률(learning rate) 이라고 한다.
- 관찰 데이터를 한번에 사용해서 가중치를 갱신하는 방식을 **batch** 방식이라고 하며 가중치 갱신 작업이 진행되는 동안 에러 함수 값을 가장 작게 만드는 방향으로 가중치 벡터가 갱신되게 된다.
    - 이를 *gradient descent*  또는 *steepest descent*  라고 부른다.
    - 하지만 실제로 성능은 그리 좋지 못하다.
- 많은 경우 *conjugate gradients*  와 같은 방식이나 *quasi-Newton*  방법을 이용하여 *batch*  방식을 사용한다.
    - 실제 이러한 방식이 더 좋은 성능을 내는 것으로 알려져 있다. (빠른 속도와 더 간단한 그라디언트 함수 사용)

- 앞서 설명한데로 에러 함수를 그라디언트 방식으로 접근하는 경우에 전역 최소점이 아닌 지역 최소점을 찾게 된다.
    - 따라서 랜덤하게 초기값을 지정한 뒤에 가급적 여러 번 작업을 반복하여 가장 최소가 되는 지점을 선택하여 사용하면 된다.
    
    
    
    
#### < On-line Method >

- 배치 모드가 아닌 데이터를 하나씩 업데이트하는 방식도 있다.
- 이를 *on-line*  방식이라고 한다.
    - 실제로 성능도 좋고 많은 데이터를 처리하기에 적합한 방식이다.
- 이제 각각의 독립된 관찰 데이터를 이용하여 에러 함수를 정의하여 보자.

$$E({\bf w}) = \sum_{n=1}^{N}E_n({\bf w}) \qquad{(5.42)}$$

- *on-line gradient descent*  방식을 *sequential gradient descent*  또는 *stochastic gradient descent* 라고도 부른다.
- 가중치에 대한 갱신은 다음과 같은 방식을 사용한다.

$${\bf w}^{(\tau+1)} = {\bf w}^{(\tau)} - \eta \nabla E_n({\bf w}^{(\tau)}) \qquad{(5.43)}$$

- 입력 데이터를 순차적으로 입력하면서 반복을 수행할 수도 있고 임의로 데이터를 선택해가며 반복을 수행할 수도 있다.
- 일정 크기의 부분 집합 데이터를 기본 단위로 하여 처리하는 *mini-batch*  방식을 사용할 수도 있다.
- 온라인 업데이트 방식의 장점은 redundancy 에 있다.
    - 만약 입력 데이터를 그대로 복사해서 2배의 크기로 만들어 학습을 한다고 해보자.
    - 이럴 경우 배치 모드는 2배의 연산 비용이 들어간다.
    - 하지만 온라인 업데이트 방식은 동일한 비용이 들어간다.
- 또 다른 장점은 지역 최소점을 탈출할 가능성에 있다.
    -  \\( E_n({\bf w}) \\) 의 stationary 지점은 \\( \nabla E({\bf w}) \\) 의 stationary 지점과 다르기 때문에 지역 최소점을 벗어날 수 있는 여지가 생긴다

