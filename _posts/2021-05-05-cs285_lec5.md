---
title: (미완) Lecture 5 - Policy Gradients

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 5의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=17)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 배울 내용은 "정책 경사 알고리즘 (Policy Gradient Algorithm)" 입니다. 정책 경사 알고리즘은 수 많은 강화학습 알고리즘들 중에서도 가장 단순한 방법론이고, 강화 학습의 Objective Function 자체를, 정책의 파라메터 $$\theta$$에 대해서 직접 미분해 업데이트하는 방법론 입니다. 

![slide1](/assets/images/CS285/lec-5/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap </mark>

Recap이기 때문에 이전부터 강의를 잘 따라오신 분들은 가볍게 듣고 넘기셔도 될 것 같습니다.

![slide2](/assets/images/CS285/lec-5/slide2.png)
*Slide. 2.*

Policy는 입출력을 서로 연결해주는 Mapping Function이기 때문에, 네트워크의 파라메터가 곧 정책의 파라메터가 됩니다.

일반적인 딥러닝과 다를 바 없이 입력을 given으로 출력을 만들어 내는 것 처럼, 강화학습에서는 상태 $$s$$를 givne, 행동 $$a$$의 분포를 출력합니다. 
분포 또한 딥러닝과 같이 연속적 (일반적으로 ML에선 회귀) 이거나 이산적 (분류) 일 수 있죠.


그리고 그림에서 알 수 있듯, Mapping Function을 통해 어떤 행동 $$a$$를 산출해 내고, $$s,a$$를 given으로 다음 상태를 예측하게 되는데,
이 상태가 어떻게 되는지를 내포하고 있는 Transition Operator(Probability Matrix), $$T$$는 우리가 처음부터 알고 있을 수도 아닐 수도 있습니다.
이 때 이 Operator를 다른 Network를 만들어 예측하면 Model을 만드는 것이 됩니다. (Model-based RL)


궤적 (Trajectory)이란 한 에피소드 내의 일련의 상태,행동들 $$s_1,a_1,\cdotst,_s_T,a_T$$의 모음이며, Trajectory Distribution은 Chain Rule에 따라서  *Slide. 2.*의 중간에 있는 수식 처럼 나타낼 수 있습니다 (Initial State Distribution와 Transition Operator 그리고 Policy의 곱). Trajecctory의 수식에는 $$\pi_{\theta}$$라는 항이 있는데, 이 의미는 "현재 내가 가지고 있는 정책을 따라서, $$s_1,a_1,\cdotst,_s_T,a_T$$을 샘플링 한 것이 Trajectory다" 라는 것 입니다.


한편, Model-based RL과 다르게 Transition Probability를 모르는 상태에서 샘플링을 통해 학습을 진행하는 것을 `Model-free RL`라고 하며,


마지막으로, 우리가 강화학습에서 원하는 것은 어떠한 Trajectory Distribution를 따르는 Trajectory하에서의 보상 값들의 합의 기대값 (즉 Trajectory Distribution에서 샘플링한 여러 Trajectory들에 대해서 각각의 $$1~T$$까지의 보상의 합을 더한 것이라고 할 수 있음)을 Objective Function이라고 정의하고
이 보상에 대한 기대값을 최대로 하는 `정책의 파라메터를 찾는 것 (최적화 하는 것)` 이 강화학습의 목표라고 할 수 있었습니다.

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$


![slide3](/assets/images/CS285/lec-5/slide3.png)
*Slide. 3.*


*Slide. 3.*는 강화학습의 Obejective Function을 State-Action Marginal을 사용해서 표현하면, 에피소드의 끝이 없는 Infinite Horizon Case와 끝이 정해진 Fintie Horizon Case 둘 다에 대해서 Objective Function을 정의할 수 있음을 보여주는 내용을 Recap한 것입니다.


![slide4](/assets/images/CS285/lec-5/slide4.png)
*Slide. 4.*

어떻게 Obejective Function을 최적화 (optimize)했는지에 대해서도 한번 되짚어 보겠습니다.
Objective Function은 다시 아래와 같은데,

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

만약 우리가 $$p(s_1)$$, $$p(s_{t+1} \vert s_t)$$같은 정보를 모른다고 하면 어떻게 $$J(\theta)$$를 추정해야 할까요?

우리는 real-world에 대해서 현재 가지고 있는 Policy를 직접 돌려서 이들을 샘플링 (sampling) 할 수 있을 겁니다.

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] \approx \frac{1}{N} \sum_i \sum_t r(s_{i,t},a_{i,t})
$$

$$N$$번 Policy를 돌려서 (roll-out) $$N$$개의 Trajectory들을 얻게 된거죠.

이렇게 얻어진 Trajectory들 각각에 대해서 보상의 합을 평균 내어 $$J(\theta)$$로 쓰게 되면서 우리는 Unbiased Estimate를 할 수 있게 됩니다. 
(N이 커질수록 더 정확해집니다.)




![slide5](/assets/images/CS285/lec-5/slide5.png)
*Slide. 5.*

그리고 우리는 단순히 현재 Policy가 좋은지 나쁜지, 그러니까 이 현재의 Policy를 가지고 샘플링한 Trajectory들이 과연 좋은지? (좋은 점수를 내는 것들인지?)를 단순히 평가할 뿐만 아니라, 이를 바탕으로 `Policy를 개선` 시키고 싶은게 목적이기 때문에 미분값을 계산해야 합니다.


위의 Objective Fucntion, 즉 기대값은 아래와 같이 적분식으로도 표현이 가능하고,

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_{t=1}^T r(s_t,a_t) ]
r(\tau) = \sum_{t=1}^T r(s_t,a_t)
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ r(\tau) ]
J(\theta) = \int p_{\theta}(\tau) r(\tau) d\tau
$$

우리는 이를 통해 정책을 업데이트하고 싶기 때문에, 정책의 파라메터 $$\theta$$에 대해서 Objective를 미분합니다.
미분 연산자, $$\bigtriangledown$$는 선형적이기 때문에 이를 적분식 안으로 넣으면 아래와 같이 표현할 수 있는데요,

$$
\bigtriangledown_{\theta} J(\theta) = \int \bigtriangledown_{\theta} p_{\theta} (\tau) r(\tau) d\tau
$$

여기서 



![slide6](/assets/images/CS285/lec-5/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-5/slide7.png)
*Slide. 7.*







## <mark style='background-color: #fff5b1'> Understanding Policy Gradients </mark>

![slide9](/assets/images/CS285/lec-5/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-5/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-5/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-5/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-5/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-5/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-5/slide15.png)
*Slide. 15.*




## <mark style='background-color: #fff5b1'> Reducing Variance </mark>

![slide17](/assets/images/CS285/lec-5/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-5/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-5/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-5/slide20.png)
*Slide. 20.*


## <mark style='background-color: #fff5b1'> Off-Policy Policy Gradients </mark>

![slide22](/assets/images/CS285/lec-5/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-5/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-5/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-5/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-5/slide26.png)
*Slide. 26.*




## <mark style='background-color: #fff5b1'> Implementing Policy Gradients </mark>

![slide28](/assets/images/CS285/lec-5/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-5/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-5/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-5/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-5/slide32.png)
*Slide. 32.*





## <mark style='background-color: #fff5b1'> Advanced Policy Gradients </mark>

![slide34](/assets/images/CS285/lec-5/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-5/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-5/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-5/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-5/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-5/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-5/slide40.png)
*Slide. 40.*



### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








