---
title: (미완) Lecture 5 - Policy Gradients

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 5의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=17)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 배울 내용은 "정책 경사 알고리즘 (Policy Gradient Algorithm)" 입니다. 정책 경사 알고리즘은 수 많은 강화학습 알고리즘들 중에서도 가장 단순한 방법론이고, 강화 학습의 Objective Function 자체를, 정책의 파라메터 $$\theta$$에 대해서 직접 미분해 업데이트하는 방법론 입니다. 

![slide1](/assets/images/CS285/lec-5/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap </mark>

Recap이기 때문에 이전부터 강의를 잘 따라오신 분들은 가볍게 듣고 넘기셔도 될 것 같습니다.

![slide2](/assets/images/CS285/lec-5/slide2.png)
*Slide. 2.*

Policy는 입출력을 서로 연결해주는 Mapping Function이기 때문에, 네트워크의 파라메터가 곧 정책의 파라메터가 됩니다.

일반적인 딥러닝과 다를 바 없이 입력을 given으로 출력을 만들어 내는 것 처럼, 강화학습에서는 상태 $$s$$를 givne, 행동 $$a$$의 분포를 출력합니다. 
분포 또한 딥러닝과 같이 연속적 (일반적으로 ML에선 회귀) 이거나 이산적 (분류) 일 수 있죠.


그리고 그림에서 알 수 있듯, Mapping Function을 통해 어떤 행동 $$a$$를 산출해 내고, $$s,a$$를 given으로 다음 상태를 예측하게 되는데,
이 상태가 어떻게 되는지를 내포하고 있는 Transition Operator(Probability Matrix), $$T$$는 우리가 처음부터 알고 있을 수도 아닐 수도 있습니다.
이 때 이 Operator를 다른 Network를 만들어 예측하면 Model을 만드는 것이 됩니다. (Model-based RL)


궤적 (Trajectory)이란 한 에피소드 내의 일련의 상태,행동들 $$s_1,a_1,\cdots,s_T,a_T$$ 의 모음이며, Trajectory Distribution은 Chain Rule에 따라서  *Slide. 2.*의 중간에 있는 수식 처럼 나타낼 수 있습니다 (Initial State Distribution와 Transition Operator 그리고 Policy의 곱). Trajecctory의 수식에는 $$\pi_{\theta}$$라는 항이 있는데, 이 의미는 "현재 내가 가지고 있는 정책을 따라서, $$s_1,a_1,\cdots,s_T,a_T$$ 을 샘플링 한 것이 Trajectory다" 라는 것 입니다.


한편, Model-based RL과 다르게 Transition Probability를 모르는 상태에서 샘플링을 통해 학습을 진행하는 것을 `Model-free RL`라고 하며,


마지막으로, 우리가 강화학습에서 원하는 것은 어떠한 Trajectory Distribution를 따르는 Trajectory하에서의 보상 값들의 합의 기대값 (즉 Trajectory Distribution에서 샘플링한 여러 Trajectory들에 대해서 각각의 $$1~T$$까지의 보상의 합을 더한 것이라고 할 수 있음)을 Objective Function이라고 정의하고
이 보상에 대한 기대값을 최대로 하는 `정책의 파라메터를 찾는 것 (최적화 하는 것)` 이 강화학습의 목표라고 할 수 있었습니다.

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$


![slide3](/assets/images/CS285/lec-5/slide3.png)
*Slide. 3.*


*Slide. 3.*는 강화학습의 Obejective Function을 State-Action Marginal을 사용해서 표현하면, 에피소드의 끝이 없는 Infinite Horizon Case와 끝이 정해진 Fintie Horizon Case 둘 다에 대해서 Objective Function을 정의할 수 있음을 보여주는 내용을 Recap한 것입니다.






## <mark style='background-color: #fff5b1'> Direct Policy Differentiation </mark>

이제 어떻게 Obejective Function을 최적화 (optimize)했는지에 대해서 조금 디테일 하게 알아보도록 하겠습니다.

![slide4](/assets/images/CS285/lec-5/slide4.png)
*Slide. 4.*

Objective Function은 다시 아래와 같은데,


$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

만약 우리가 $$p(s_1)$$, $$p(s_{t+1} \vert s_t)$$같은 정보를 모른다고 하면 어떻게 $$J(\theta)$$를 추정해야 할까요?

우리는 real-world에 대해서 현재 가지고 있는 Policy를 직접 돌려서 이들을 샘플링 (sampling) 할 수 있을 겁니다.

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] \approx \frac{1}{N} \sum_i \sum_t r(s_{i,t},a_{i,t})
$$

$$N$$번 Policy를 돌려서 (roll-out) $$N$$개의 Trajectory들을 얻게 된거죠.

이렇게 얻어진 Trajectory들 각각에 대해서 보상의 합을 평균 내어 $$J(\theta)$$로 쓰게 되면서 우리는 Unbiased Estimate를 할 수 있게 됩니다. 
(N이 커질수록 더 정확해집니다.)





![slide5](/assets/images/CS285/lec-5/slide5.png)
*Slide. 5.*

그리고 우리는 단순히 현재 Policy가 좋은지 나쁜지, 그러니까 이 현재의 Policy를 가지고 샘플링한 Trajectory들이 과연 좋은지? (좋은 점수를 내는 것들인지?)를 단순히 평가할 뿐만 아니라, 이를 바탕으로 `Policy를 개선` 시키고 싶은게 목적이기 때문에 미분값을 계산해야 합니다.


위의 Objective Fucntion, 즉 기대값은 아래와 같이 적분식으로도 표현이 가능하고,

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_{t=1}^T r(s_t,a_t) ]
$$

$$
r(\tau) = \sum_{t=1}^T r(s_t,a_t)
$$

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ r(\tau) ]
$$

$$
J(\theta) = \int p_{\theta}(\tau) r(\tau) d\tau
$$

우리는 이를 통해 정책을 업데이트하고 싶기 때문에, 정책의 파라메터 $$\theta$$에 대해서 Objective를 미분합니다.
미분 연산자, $$\bigtriangledown$$는 선형적이기 때문에 이를 적분식 안으로 넣으면 아래와 같이 표현할 수 있는데요,

$$
\bigtriangledown_{\theta} J(\theta) = \int \bigtriangledown_{\theta} p_{\theta} (\tau) r(\tau) d\tau
$$

여기서 우리는

$$
p_{\theta}(\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) = p_{\theta}(\tau) \frac{ \bigtriangledown_{\theta} p_{\theta} (\tau) }{ p_{\theta} (\tau) }
$$

$$
p_{\theta}(\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) = \cancel{p_{\theta}(\tau)} \frac{ \bigtriangledown_{\theta} p_{\theta} (\tau) }{ \cancel{p_{\theta} (\tau)} }
$$

와 같은 항등식을 이용하면, (기억이 안나시는 분은 log의 미분에 대해서 찾아보시면 됩니다.)


$$
\bigtriangledown_{\theta} J(\theta) = \int \bigtriangledown_{\theta} p_{\theta} (\tau) r(\tau) d\tau = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau
$$

가 되고, 마지막으로 기대값의 정의를 이용해서 이를 다시 기대값의 형태로 바꾸면

$$
\bigtriangledown_{\theta} J(\theta) = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

최종적으로 Objective를 Policy의 파라메터 $$\theta$$에 대해 미분한 수식을 얻을 수 있습니다.
(모든 Trajectory Sample들에 대해서 그 Trajectory가 나타날 확률에 "로그"를 취한 값을 미분한 값과, 그 Trajectory를 따랐을 때의 보상값이 곱해져있죠)



하지만 아직 끝난 것이 아닙니다, 바로 $$\bigtriangledown_{\theta} log p_{\theta} (\tau)$$ 텀 때문인데요, 좀 더 수식을 진행해보도록 하겠습니다.

![slide6](/assets/images/CS285/lec-5/slide6.png)
*Slide. 6.*

우리가 여태까지 유도한 수식은 아래와 같고,

$$
J(\theta) = \int p_{\theta}(\tau) r(\tau) d\tau
$$

$$
\theta^{\ast} = argmax_{\theta} J(\theta)
$$

$$
\bigtriangledown_{\theta} J(\theta) = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

위의 수식에서 우변에 존재하는 $$log p_{\theta}(\tau)$$ 텀을 다루기 위해서,
여기에 우리가 Transition Probability는 모른다고 했으나 이를 이용해서 Trajectory Distribution을 아래처럼 표현해 보도록 하겠습니다.

$$ 
p_{\theta} (\tau) = p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t ) 
$$

여기서 양변에 $$log$$를 취하면 아래와 같은 식이 되는데요,

$$ 
log p_{\theta} (\tau) = log p(s_1) \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + log p(s_{t+1} \vert s_t, a_t ) 
$$


우리가 원하는 것은 $$\bigtriangledown_{\theta} log p_{\theta}(\tau)$$이기 때문에 양변에 미분연산자를 붙혀주면 아래의 식처럼 나타낼 수 있습니다.

$$
\bigtriangledown_{\theta} log p_{\theta}(\tau) = \bigtriangledown_{\theta} [log p(s_1) + \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + log p(s_{t+1} \vert s_t, a_t )]
$$

여기서 우리가 $$\theta$$에 대해서 미분을 했기 때문에, 이와 관련없는 텀들은 전부 없앨 수 있는데요, 

$$
\bigtriangledown_{\theta} log p_{\theta}(\tau) = \bigtriangledown_{\theta} [\cancel{log p(s_1)} + \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + \cancel{log p(s_{t+1} \vert s_t, a_t )}]
$$

결국 최종적으로 우리는 아래의 $$\bigtriangledown_{\theta} log p_{\theta}(\tau)$$를 대체해서

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

아래와 같은 미분 식을 얻어낼 수 있습니다.

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ ( \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) ) r(\tau) ]
$$

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ ( \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) ) (\sum_{t=1}^T r(s_t,a_t)) ]
$$


여기서 우리가 얻을 수 있는 의미는 뭘까요? 그것은 바로 Transition Probability, $$p(s_{t+1} \vert s_t,a_t)$$가 사라졌다는 겁니다. 
우리가 Transition Probability (혹은 Model) 을 모르고 출발하기도 했지만, 이를 따로 정의해주지 않아도, 그러니까 Model이라는 것을 따로 추정하면서 학습하지 않아도 된다는 것이죠.







![slide7](/assets/images/CS285/lec-5/slide7.png)
*Slide. 7.*







## <mark style='background-color: #fff5b1'> Understanding Policy Gradients </mark>

![slide9](/assets/images/CS285/lec-5/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-5/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-5/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-5/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-5/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-5/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-5/slide15.png)
*Slide. 15.*




## <mark style='background-color: #fff5b1'> Reducing Variance </mark>

![slide17](/assets/images/CS285/lec-5/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-5/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-5/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-5/slide20.png)
*Slide. 20.*


## <mark style='background-color: #fff5b1'> Off-Policy Policy Gradients </mark>

![slide22](/assets/images/CS285/lec-5/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-5/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-5/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-5/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-5/slide26.png)
*Slide. 26.*




## <mark style='background-color: #fff5b1'> Implementing Policy Gradients </mark>

![slide28](/assets/images/CS285/lec-5/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-5/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-5/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-5/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-5/slide32.png)
*Slide. 32.*





## <mark style='background-color: #fff5b1'> Advanced Policy Gradients </mark>

![slide34](/assets/images/CS285/lec-5/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-5/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-5/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-5/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-5/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-5/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-5/slide40.png)
*Slide. 40.*



### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








