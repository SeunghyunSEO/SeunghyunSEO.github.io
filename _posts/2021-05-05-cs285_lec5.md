---
title: (미완) Lecture 5 - Policy Gradients

categories: CS285
tag: [RL]
ㅋㄱ
toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 5의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=17)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 배울 내용은 "정책 경사 알고리즘 (Policy Gradient Algorithm)" 입니다. 정책 경사 알고리즘은 수 많은 강화학습 알고리즘들 중에서도 가장 단순한 방법론이고, 강화 학습의 Objective Function 자체를, 정책의 파라메터 $$\theta$$에 대해서 직접 미분해 업데이트하는 방법론 입니다. 

![slide1](/assets/images/CS285/lec-5/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap </mark>

Recap이기 때문에 이전부터 강의를 잘 따라오신 분들은 가볍게 듣고 넘기셔도 될 것 같습니다.

![slide2](/assets/images/CS285/lec-5/slide2.png)
*Slide. 2.*

Policy는 입출력을 서로 연결해주는 Mapping Function이기 때문에, 네트워크의 파라메터가 곧 정책의 파라메터가 됩니다.

일반적인 딥러닝과 다를 바 없이 입력을 given으로 출력을 만들어 내는 것 처럼, 강화학습에서는 상태 $$s$$를 givne, 행동 $$a$$의 분포를 출력합니다. 
분포 또한 딥러닝과 같이 연속적 (일반적으로 ML에선 회귀) 이거나 이산적 (분류) 일 수 있죠.


그리고 그림에서 알 수 있듯, Mapping Function을 통해 어떤 행동 $$a$$를 산출해 내고, $$s,a$$를 given으로 다음 상태를 예측하게 되는데,
이 상태가 어떻게 되는지를 내포하고 있는 Transition Operator(Probability Matrix), $$T$$는 우리가 처음부터 알고 있을 수도 아닐 수도 있습니다.
이 때 이 Operator를 다른 Network를 만들어 예측하면 Model을 만드는 것이 됩니다. (Model-based RL)


궤적 (Trajectory)이란 한 에피소드 내의 일련의 상태,행동들 $$s_1,a_1,\cdots,s_T,a_T$$ 의 모음이며, Trajectory Distribution은 Chain Rule에 따라서  *Slide. 2.*의 중간에 있는 수식 처럼 나타낼 수 있습니다 (Initial State Distribution와 Transition Operator 그리고 Policy의 곱). Trajecctory의 수식에는 $$\pi_{\theta}$$라는 항이 있는데, 이 의미는 "현재 내가 가지고 있는 정책을 따라서, $$s_1,a_1,\cdots,s_T,a_T$$ 을 샘플링 한 것이 Trajectory다" 라는 것 입니다.


한편, Model-based RL과 다르게 Transition Probability를 모르는 상태에서 샘플링을 통해 학습을 진행하는 것을 `Model-free RL`라고 하며,


마지막으로, 우리가 강화학습에서 원하는 것은 어떠한 Trajectory Distribution를 따르는 Trajectory하에서의 보상 값들의 합의 기대값 (즉 Trajectory Distribution에서 샘플링한 여러 Trajectory들에 대해서 각각의 $$1~T$$까지의 보상의 합을 더한 것이라고 할 수 있음)을 Objective Function이라고 정의하고
이 보상에 대한 기대값을 최대로 하는 `정책의 파라메터를 찾는 것 (최적화 하는 것)` 이 강화학습의 목표라고 할 수 있었습니다.

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$


![slide3](/assets/images/CS285/lec-5/slide3.png)
*Slide. 3.*


*Slide. 3.*는 강화학습의 Obejective Function을 State-Action Marginal을 사용해서 표현하면, 에피소드의 끝이 없는 Infinite Horizon Case와 끝이 정해진 Fintie Horizon Case 둘 다에 대해서 Objective Function을 정의할 수 있음을 보여주는 내용을 Recap한 것입니다.






## <mark style='background-color: #fff5b1'> Direct Policy Differentiation </mark>

이제 어떻게 Obejective Function을 최적화 (optimize)했는지에 대해서 조금 디테일 하게 알아보도록 하겠습니다.

![slide4](/assets/images/CS285/lec-5/slide4.png)
*Slide. 4.*

Objective Function은 다시 아래와 같은데,


$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

만약 우리가 $$p(s_1)$$, $$p(s_{t+1} \vert s_t)$$같은 정보를 모른다고 하면 어떻게 $$J(\theta)$$를 추정해야 할까요?

우리는 real-world에 대해서 현재 가지고 있는 Policy를 직접 돌려서 이들을 샘플링 (sampling) 할 수 있을 겁니다.

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] \approx \frac{1}{N} \sum_i \sum_t r(s_{i,t},a_{i,t})
$$

$$N$$번 Policy를 돌려서 (roll-out) $$N$$개의 Trajectory들을 얻게 된거죠.

이렇게 얻어진 Trajectory들 각각에 대해서 보상의 합을 평균 내어 $$J(\theta)$$로 쓰게 되면서 우리는 Unbiased Estimate를 할 수 있게 됩니다. 
(N이 커질수록 더 정확해집니다.)





![slide5](/assets/images/CS285/lec-5/slide5.png)
*Slide. 5.*

그리고 우리는 단순히 현재 Policy가 좋은지 나쁜지, 그러니까 이 현재의 Policy를 가지고 샘플링한 Trajectory들이 과연 좋은지? (좋은 점수를 내는 것들인지?)를 단순히 평가할 뿐만 아니라, 이를 바탕으로 `Policy를 개선` 시키고 싶은게 목적이기 때문에 미분값을 계산해야 합니다.


위의 Objective Fucntion, 즉 기대값은 아래와 같이 적분식으로도 표현이 가능하고,

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_{t=1}^T r(s_t,a_t) ]
$$

$$
r(\tau) = \sum_{t=1}^T r(s_t,a_t)
$$

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ r(\tau) ]
$$

$$
J(\theta) = \int p_{\theta}(\tau) r(\tau) d\tau
$$

우리는 이를 통해 정책을 업데이트하고 싶기 때문에, 정책의 파라메터 $$\theta$$에 대해서 Objective를 미분합니다.
미분 연산자, $$\bigtriangledown$$는 선형적이기 때문에 이를 적분식 안으로 넣으면 아래와 같이 표현할 수 있는데요,

$$
\bigtriangledown_{\theta} J(\theta) = \int \bigtriangledown_{\theta} p_{\theta} (\tau) r(\tau) d\tau
$$

여기서 우리는

$$
p_{\theta}(\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) = p_{\theta}(\tau) \frac{ \bigtriangledown_{\theta} p_{\theta} (\tau) }{ p_{\theta} (\tau) }
$$

$$
p_{\theta}(\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) = \cancel{p_{\theta}(\tau)} \frac{ \bigtriangledown_{\theta} p_{\theta} (\tau) }{ \cancel{p_{\theta} (\tau)} }
$$

와 같은 항등식을 이용하면, (기억이 안나시는 분은 log의 미분에 대해서 찾아보시면 됩니다.)


$$
\bigtriangledown_{\theta} J(\theta) = \int \bigtriangledown_{\theta} p_{\theta} (\tau) r(\tau) d\tau = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau
$$

가 되고, 마지막으로 기대값의 정의를 이용해서 이를 다시 기대값의 형태로 바꾸면

$$
\bigtriangledown_{\theta} J(\theta) = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

최종적으로 Objective를 Policy의 파라메터 $$\theta$$에 대해 미분한 수식을 얻을 수 있습니다.
(모든 Trajectory Sample들에 대해서 그 Trajectory가 나타날 확률에 "로그"를 취한 값을 미분한 값과, 그 Trajectory를 따랐을 때의 보상값이 곱해져있죠)



하지만 아직 끝난 것이 아닙니다, 바로 $$\bigtriangledown_{\theta} log p_{\theta} (\tau)$$ 텀 때문인데요, 좀 더 수식을 진행해보도록 하겠습니다.

![slide6](/assets/images/CS285/lec-5/slide6.png)
*Slide. 6.*

우리가 여태까지 유도한 수식은 아래와 같고,

$$
J(\theta) = \int p_{\theta}(\tau) r(\tau) d\tau
$$

$$
\theta^{\ast} = argmax_{\theta} J(\theta)
$$

$$
\bigtriangledown_{\theta} J(\theta) = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

위의 수식에서 우변에 존재하는 $$log p_{\theta}(\tau)$$ 텀을 다루기 위해서,
여기에 우리가 Transition Probability는 모른다고 했으나 이를 이용해서 Trajectory Distribution을 아래처럼 표현해 보도록 하겠습니다.

$$ 
p_{\theta} (\tau) = p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t ) 
$$

여기서 양변에 $$log$$를 취하면 아래와 같은 식이 되는데요,

$$ 
log p_{\theta} (\tau) = log p(s_1) \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + log p(s_{t+1} \vert s_t, a_t ) 
$$


우리가 원하는 것은 $$\bigtriangledown_{\theta} log p_{\theta}(\tau)$$이기 때문에 양변에 미분연산자를 붙혀주면 아래의 식처럼 나타낼 수 있습니다.

$$
\bigtriangledown_{\theta} log p_{\theta}(\tau) = \bigtriangledown_{\theta} [log p(s_1) + \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + log p(s_{t+1} \vert s_t, a_t )]
$$

여기서 우리가 $$\theta$$에 대해서 미분을 했기 때문에, 이와 관련없는 텀들은 전부 없앨 수 있는데요, 

$$
\bigtriangledown_{\theta} log p_{\theta}(\tau) = \bigtriangledown_{\theta} [\cancel{log p(s_1)} + \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + \cancel{log p(s_{t+1} \vert s_t, a_t )}]
$$

결국 최종적으로 우리는 아래의 $$\bigtriangledown_{\theta} log p_{\theta}(\tau)$$를 대체해서

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

아래와 같은 미분 식을 얻어낼 수 있습니다.

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ ( \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) ) r(\tau) ]
$$

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ ( \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) ) (\sum_{t=1}^T r(s_t,a_t)) ]
$$


이렇게 함으로써 우리가 얻을 수 있는 의미는 뭘까요? 

우선 Expectation, $$\mathbb{E}$$속의 모든 term들이 알려져 있다는 겁니다. 
당연히 현재 Policy로 접근 (access)할 수 있고, 우리의 샘플들에 대해서 보상 (reward)이 얼마인지도 평가 (evaluate) 가능하니까요.

하지만 여기서 더 중요한 부분이 있는데, 바로 알려지지 않았던 term들인 Initial State Probability $$p(s_1)$$과 Transition Probability, $$p(s_{t+1} \vert s_t,a_t)$$들이 미분을 하는 과정에서 사라졌다는 겁니다. 
즉, 우리가 Transition Probability (혹은 Model) 을 모르고 출발하기도 했지만, 이를 따로 정의해주지 않아도, 그러니까 Model이라는 것을 따로 추정하면서 학습하지 않아도 된다는 것이죠.



Objective는 다시 아래의 *Slide. 7.*에서와 같이 $$\mathbb{E}$$을 $$\sum$$로 대체할 수 있고 (sampling), 이렇게 얻어낸 Objective의 미분체를 통해 경사 하강법 (Gradient Descent) 기법을 통해 정책을 개선시킬 수 있습니다. 

![slide7](/assets/images/CS285/lec-5/slide7.png)
*Slide. 7.*


*Slide. 7.*의 중간 부분에서는 여태까지 전개한 수식을 이전에 살펴봤던 RL 알고리즘의 Anatomy와 각각 대응시켜 표현한 것입니다.
이러한 기본적인 Policy Gradient Algorithm은 `REINFORCE`라고 부르기도 합니다. 
[REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)는 1990년대에 [Williams](https://en.wikipedia.org/wiki/Ronald_J._Williams)가 제안한 첫 번째 정책 경사 알고리즘이며, 
*Slide. 7.*의 하단에 나와있는 세가지 스텝으로 이루어져 있는 기본적인 정책 경사 알고리즘입니다.




하지만 안타깝게도 지금까지 살펴본 기본적인 방법론은 현실에서 잘 작용하지 않는다고 하는데요, 
앞으로 남은 파트에서 정책 경사 알고리즘 실제로 어떤 일을 하는것인지에 대한 직관적인 이해와 더불어 실제로 잘 작동하는 정책 경사 알고리즘들에 대해서 알아본다고 합니다.









## <mark style='background-color: #fff5b1'> Understanding Policy Gradients </mark>

"직관적으로 정책 경사 알고리즘을 하면 어떻게 정책이 학습되는 걸까?"

![slide9](/assets/images/CS285/lec-5/slide9.png)
*Slide. 9.*

우리가 여태까지 수학적으로 정책 경사 알고리즘을 유도해 봤고, 그 결과 아래와 같은 근사 식을 얻어냈습니다.

$$
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ) ( \sum_{t=1}^T r(s_{i,t},a_{i,t}) ) 
$$

여기서 Lecturer Sergey는 $$ \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) $$ 가 하는 일이 뭔지에 대해서 한번 생각해 보자고 하는데요,

그렇게 하기 위해 우선 우리가 가진 정책 $$\pi_{\theta}$$가 이미지를 입력으로 이산적인 행동 (Discrete Action)을 출력한다고 생각해 보도록 하겠습니다.
(당연히 정책은 뉴럴 네트워크 (Neural Network, NN)로 모델링 되어있으니, NN이 가지는 파라메터가 곧 $$\theta$$입니다.





![slide10](/assets/images/CS285/lec-5/slide10.png)
*Slide. 10.*



이미 눈치를 채신 분들도 계시겠지만 수식에서

$$
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ) ( \sum_{t=1}^T r(s_{i,t},a_{i,t}) ) 
$$

reward 부분만 빼면 이는 Log-Likelihood만 남게 되는데요, 

$$
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) )) 
$$

이렇게 되면 Objective는 딥러닝의 지도 학습 (Supervised Learning)에서 일반적으로 사용하는 최대 우도 측정 (Maximum Likelihood Estimation, MLE)나 다름없게 됩니다. 

즉 정답에 해당하는 log probability를 최대화 하는 방향으로 학습하는거죠.


하지만 우리는 Imitation Learning이 아니기 때문에 정답 레이블이 따로 없으며, 뒤에 $$( \sum_{t=1}^T r(s_{i,t},a_{i,t})$$가 붙어있죠, 
이 말의 의미는 보상 값 (Reward Value)에 따라서 좋았던 Trajectory의 log probability는 증가시키고, 나빴던 Trajectory에 대해서는 log probability를 줄이는 방향으로 학습을 하겠다는 겁니다.


***

```
조금 더 설명을 보태보겠습니다. 
```

우리가 지금 가정한 문제는 이산 확률 분포를 나타내는 (즉 이진 분류 문제를 푸는 것) 경우이므로, 크로스 엔트로피 (Cross-Entropy, CE) Loss를 줄이는 것이며, 수식을 CE와 똑같은 폼으로 나타내기 위해서 *Slide. 10.*의 수식에는 없지만 $$t_{i,t}$$를 추가해줘야 할 것 같습니다.


$$t_{i,t}$$는 당연히 정답을 의미하는 원핫 벡터이며, 예를들어 현재 자동차가 "좌,우" 두 가지 선택지만 고를 수 있다면, 
어떤 상태에서 "좌" 가 정답일때는 $$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$이 되고 "우"가 정답일 때는 $$\begin{bmatrix} 0 \\ 1 \end{bmatrix}$$이 되는 값이 곱해져, 정답에 대해서 잘 맞추지 못한것에 대해서만 패널티를 부과하는게 CE Objective Function에 대한 해석이겠죠. 
(수식에 직접 확률을 대입해 보시면 아시겠지만, $$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$이 곱해져 정답이라는 것이 Activation 됐을 때 그 정답을 출력으로 낼 확률이 적으면 손실함수의 값이 크기 때문에, 이를 줄이는 방향으로 학습하면, 정답을 출력으로 뱉을 확률이 높아지는게 수식이 가지는 의미입니다.)

$$
\bigtriangledown_{\theta}J_{ML}(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} t_{i,t} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) )) 
$$

즉 이를 최적화 한다는 것은 각각의 상태마다 정답이라고 레이블링 된 행동들이 나올 확률을 1에 가깝게 계속해서 높히는 것이 됩니다. 


하지만 우리는 Imitation Learning을 하는 것이 아니기 때문에 정답 레이블도 없습니다.
게다가 우리가 원래 가지고 있는 수식은 $$t_{i,t}$$가 없죠.
대신에 어떤 행동을 했을 때 그 행동에 대한 댓가를 나타내는 $$ ( \sum_{t=1}^T r(s_{i,t},a_{i,t})$$를 수식에 곱했죠.


즉 우리는 정답이라고 알려주는 텀 $$t_{i,t}$$이 $$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$ 되어서 정답 행동과 관련 없는것들은 모두 죽이는 것 대신에,
모든 가능한 행동에 대한 각각의 보상 값 $$ r(s_{i,t},a_{i,t}$$을 해당 행동을 할 확률과 곱해서 나타낸다는 겁니다. 
$$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$가 곱해지는 것이 아니기 때문에, 정답 행동이 아닌 (정답이랄것도 없죠) 모든 행동에 대해서 보상값들을 다 곱하는 상황에서 Objective가 최종적으로 가장 큰 보상값을 리턴하기 위해서는 가장 큰 보상을 산출하는 행동의 확률을 크게해야 각각의 행동과 보상을 곱해 더했을 때 최종적으로 가장 큰 값을 리턴하니, 가장 큰 보상을 산출하는 행동의 확률을 크게하고, 적거나 혹은 음수의 보상을 리턴하는 행동은 확률을 줄여야 합니다.


결국 강화학습의 Objective를 최대화 하는 것은 곧 현재 정책을 가지고 여러 행동을 해 보면서 좋았던 행동은 더 자주 나오게 (높은 확률로), 아닌 행동은 덜 나오게 (낮은 확률로)끔 확률을 조정하는 의미를 갖게 되는겁니다.

***







![slide11](/assets/images/CS285/lec-5/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-5/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-5/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-5/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-5/slide15.png)
*Slide. 15.*









## <mark style='background-color: #fff5b1'> Reducing Variance </mark>

![slide17](/assets/images/CS285/lec-5/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-5/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-5/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-5/slide20.png)
*Slide. 20.*








## <mark style='background-color: #fff5b1'> Off-Policy Policy Gradients </mark>

![slide22](/assets/images/CS285/lec-5/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-5/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-5/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-5/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-5/slide26.png)
*Slide. 26.*








## <mark style='background-color: #fff5b1'> Implementing Policy Gradients </mark>

![slide28](/assets/images/CS285/lec-5/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-5/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-5/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-5/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-5/slide32.png)
*Slide. 32.*





## <mark style='background-color: #fff5b1'> Advanced Policy Gradients </mark>

![slide34](/assets/images/CS285/lec-5/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-5/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-5/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-5/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-5/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-5/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-5/slide40.png)
*Slide. 40.*








### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








