---
title: (미완)Dropout
categories: DeepLearning
tag: [tmp]

toc: true
toc_sticky: true
---

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> Normalization </mark>

## <mark style='background-color: #fff5b1'> References </mark>

- 1.[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)

- 2.[Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://proceedings.mlr.press/v48/gal16.pdf)

- 3.[Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference](https://arxiv.org/pdf/1506.02158)

- 4.[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/pdf/1512.05287)

- 5.[12 Main Dropout Methods: Mathematical and Visual Explanation for DNNs, CNNs, and RNNs from Axel Thevenot](https://towardsdatascience.com/12-main-dropout-methods-mathematical-and-visual-explanation-58cdc2112293)

- 6.[What My Deep Model Doesn't Know... from Yarin Gal](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)
