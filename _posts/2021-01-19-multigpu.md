---
title: Multi GPU with DP and DDP
categories: gpu
tag: [gpu,multigpu,pytorch]

toc: true
toc_sticky: true
---

딥러닝 학습을 진행할 때, GPU를 여러 개 사용하여 학습하고 싶을 때가 있을 것입니다.<br>
Data-driven approach인 딥러닝의 특성상, 일반적으로 학습 데이터가 무수히 많기 때문에, 그리고 데이터의 입력 차원이 크기 때문에 대부분의 딥러닝 네트워크는 batch 단위로 학습을 하기 마련입니다.
그리고 모델의 크기가 클 수록 일반적으로 네트워크의 성능이 좋아지기 때문에 최근 논문들에서 제안되는 네트워크들은 엄청나게 모델 사이즈가 크기 때문에 큰 batch size를 사용하기 힘듭니다.<br>
위와 같은 이유로 딥러닝 네트워크의 학습이 수렴하는데는 상당히 많은 시간이 걸리기 때문에 GPU가 넉넉하다면 누구나 여러개의 GPU를 사용해서 batch size를 늘려 학습하고 싶을 것입니다.<br><br> 

pytorch 프레임워크를 사용하면 굉장히 쉽게 여러개의 gpu를 사용해 학습할 수 있는데, 이는 ```torch.nn.DataParallel``` 를 사용하거나 ```torch.nn.parallel.DistributedDataParallel``` 를 사용하는 방식으로 간단하게 구현할 수 있습니다.<br>
```torch.nn.DataParallel```(DP)를 사용해 간단히 multi-gpu를 사용하는 코드는 다음과 같습니다.<br>
  
  
{% gist SeunghyunSEO/dc8bb539c4d8655254e48010fcff1192 %}

이번에는 ```torch.nn.parallel.DistributedDataParallel```, 즉 DDP를 사용할 경우에 대해서 알아보겠습니다.<br>


{% gist SeunghyunSEO/2a03baf56c0d6a96d220269804521318 %}

gist 코드 테스트중<br>




