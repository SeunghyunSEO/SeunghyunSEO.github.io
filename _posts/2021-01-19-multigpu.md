---
title: Multi GPU Training with DP and DDP
categories: pytorch
tag: [gpu,multigpu,pytorch]

date: 2021-01-19 01:00:00

toc: true
toc_sticky: true

comments: true
---

Data-driven approach인 딥러닝의 특성상, 일반적으로 학습 데이터가 무수히 많기 때문에, 그리고 데이터의 입력 차원이 크기 때문에 대부분의 딥러닝 네트워크는 batch 단위로 학습을 하기 마련입니다.
그리고 모델의 크기가 클 수록 일반적으로 네트워크의 성능이 좋아지기 때문에 최근 논문들에서 제안되는 네트워크들은 엄청나게 모델 사이즈가 크기 때문에 큰 batch size를 사용하기 힘듭니다.<br>
위와 같은 이유로 딥러닝 네트워크의 학습이 수렴하는데는 상당히 많은 시간이 걸리기 때문에 GPU가 넉넉하다면 누구나 여러개의 GPU를 사용해서 batch size를 늘려 학습하고 싶을 것입니다.<br><br> 

이럴 경우 pytorch 프레임워크를 예로 들면 내장 함수인 ```torch.nn.DataParallel``` 혹은 ```torch.nn.parallel.DistributedDataParallel``` 를 사용하거나 nvidia의 ```apex.parallel.DistributedDataParallel```나 ```hovorod```를 사용하는 등 다양한 방법을 사용할 수 있습니다.

이 때, 하나의 pc에서 여러개의 gpu를 사용하는경우 (예를 들어, single pc, [0,1,2,3] 총 4개 gpu), 아니면 여러개의 pc 여러개의 gpu를 사용하는 경우(예를 들어, multiple pc, 하나의 pc에 [0,1,2,3] 4개씩 총 8개 gpu)를 사용하는 경우에 따라 조금 다르게 코딩을 해줘야 하는데, (보통 전자의경우 DP를 사용하고 후자의 경우 DDP를 사용함) 이번에는 전자의 경우에 대해서만 다뤄보도록 하겠습니다. 

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> torch.nn.DataParallel (DP) </mark>

앞서 말한것처럼 pytorch 에서는 굉장히 쉽게 여러개의 gpu를 사용해 학습할 수 있는데, ```torch.nn.DataParallel```(DP)를 사용해 간단히 multi-gpu를 사용하는 코드는 다음과 같습니다.<br>
  
  
{% gist SeunghyunSEO/dc8bb539c4d8655254e48010fcff1192 %}

사실상 네트워크를 선언한 뒤 ```torch.nn.DataParallel```로 감싸주기만 하면 돼서 아주 쉽다고 볼 수 있습니다.
1개의 pc에서 2개의 gpu를 사용한다고 생각해봅시다. 이럴 경우 torch가 알아서 model을 replica 해서 각각의 모델 파라메터를 ```cuda:0```, ```cuda:1```로 할당하고 네트워크에 들어갈 인풋 instance들도 batch_size의 절반은 ```cuda:0```으로 나머지는 ```cuda:1```로 할당해서 처리를 합니다.

하지만 이렇게 할 경우 몇가지 문제점이 발생할 수 있는데요, 여러가지 이유로 ```cuda:0```에 memory가 몰린다거나, gpu를 여러개 쓴 효율이 잘 안나온다거나 하는 문제가 있을 수 있습니다.
또한 다음과 같은 이유로, 인풋 instance는 여러 gpu로 할당이 되었으나 model이 할당되지 않는 버그가 생겨 제대로 학습을 할 수 없을 수도 있습니다.

참조 : [DP issue](https://github.com/pytorch/pytorch/issues/8637)

마지막으로 파이토치의 공식 문서에서 single machine에서 multi gpu 학습을 할 경우에도 DP 보다는 DDP를 사용하기를 권장하기 때문에 DP를 사용하는것은 별로 좋지 않은 선택일 수 있습니다. 

```
It is recommended to use DistributedDataParallel, instead of this class, to do multi-GPU training, even if there is only a single node. See: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel and Distributed Data Parallel.
```

참조 : [pytorch DataParallel Docs](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)

## <mark style='background-color: #fff5b1'> torch.nn.DistributedDataParallel (DDP) </mark>

이번에는 ```torch.nn.parallel.DistributedDataParallel```, 즉 DDP를 사용할 경우에 대해서 알아보겠습니다.<br>
아래의 코드는 [pytorch imagenet example](https://github.com/pytorch/examples/blob/792d336019a28a679e29cf174e10cee80ead8722/imagenet/main.py)에서 DDP와 관련된 부분만 남겨 재구성 한 코드입니다.

{% gist SeunghyunSEO/2a03baf56c0d6a96d220269804521318 %}

gist 코드 테스트중<br>


## <mark style='background-color: #fff5b1'> References </mark>

[아직 미완성]

- [pytorch tutorial DDP 번역](https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/intermediate/dist_tuto.html#id2)
- [tutorial DDP 2](https://tutorials.pytorch.kr/intermediate/dist_tuto.html) 

- [GETTING STARTED WITH DISTRIBUTED DATA PARALLEL](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [DP and DDP](https://tutorials.pytorch.kr/beginner/dist_overview.html)

- [단일 머신 병렬화 예제](https://tutorials.pytorch.kr/intermediate/model_parallel_tutorial.html)
- [torch.distributed Docs](https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization)

- [torch.mp Docs](https://pytorch.org/docs/stable/multiprocessing.html)

- [find_unused_param에 대한 qna](https://discuss.pytorch.org/t/how-to-create-multiple-distributeddataparallel-tasks-on-a-single-node/42687/2)

- [일반인 Distributed data parallel training in Pytorch](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)
- [일반인 당근마켓](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b)

