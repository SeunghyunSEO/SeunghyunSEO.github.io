---
title: Multi GPU Training with DP and DDP
categories: gpu
tag: [gpu,multigpu,pytorch]

toc: true
toc_sticky: true
---

Data-driven approach인 딥러닝의 특성상, 일반적으로 학습 데이터가 무수히 많기 때문에, 그리고 데이터의 입력 차원이 크기 때문에 대부분의 딥러닝 네트워크는 batch 단위로 학습을 하기 마련입니다.
그리고 모델의 크기가 클 수록 일반적으로 네트워크의 성능이 좋아지기 때문에 최근 논문들에서 제안되는 네트워크들은 엄청나게 모델 사이즈가 크기 때문에 큰 batch size를 사용하기 힘듭니다.<br>
위와 같은 이유로 딥러닝 네트워크의 학습이 수렴하는데는 상당히 많은 시간이 걸리기 때문에 GPU가 넉넉하다면 누구나 여러개의 GPU를 사용해서 batch size를 늘려 학습하고 싶을 것입니다.<br><br> 

이럴 경우 pytorch 프레임워크를 예로 들면 내장 함수인 ```torch.nn.DataParallel``` 혹은 ```torch.nn.parallel.DistributedDataParallel``` 를 사용하거나 nvidia의 ```apex.parallel.DistributedDataParallel```나 ```hovorod```를 사용하는 등 다양한 방법을 사용할 수 있습니다.

이 때, 하나의 pc에서 여러개의 gpu를 사용하는경우 (예를 들어, single pc, [0,1,2,3] 총 4개 gpu), 아니면 여러개의 pc 여러개의 gpu를 사용하는 경우(예를 들어, multiple pc, 하나의 pc에 [0,1,2,3] 4개씩 총 8개 gpu)를 사용하는 경우에 따라 조금 다르게 코딩을 해줘야 하는데, (보통 전자의경우 DP를 사용하고 후자의 경우 DDP를 사용함) 이번에는 전자의 경우에 대해서만 다뤄보도록 하겠습니다. 


- torch.nn.DataParallel (DP)

앞서 말한것처럼 pytorch 에서는 굉장히 쉽게 여러개의 gpu를 사용해 학습할 수 있는데, ```torch.nn.DataParallel```(DP)를 사용해 간단히 multi-gpu를 사용하는 코드는 다음과 같습니다.<br>
  
  
{% gist SeunghyunSEO/dc8bb539c4d8655254e48010fcff1192 %}

사실상 네트워크를 선언한 뒤 ```torch.nn.DataParallel```로 감싸주기만 하면 돼서 아주 쉽다고 볼 수 있습니다.
1개의 pc에서 2개의 gpu를 사용한다고 생각해봅시다. 이럴 경우 torch가 알아서 model을 replica 해서 각각의 모델 파라메터를 ```cuda:0```, ```cuda:1```로 할당하고 네트워크에 들어갈 인풋 instance들도 batch_size의 절반은 ```cuda:0```으로 나머지는 ```cuda:1```로 할당해서 처리를 합니다.

하지만 이렇게 할 경우 몇가지 문제점이 발생할 수 있는데요, 여러가지 이유로 ```cuda:0```에 memory가 몰린다거나, gpu를 여러개 쓴 효율이 잘 안나온다거나 하는 문제가 있을 수 있습니다.
또한 다음과 같은 이유로, 인풋 instance는 여러 gpu로 할당이 되었으나 model이 할당되지 않는 버그가 생겨 제대로 학습을 할 수 없을 수도 있습니다.
[link](https://github.com/pytorch/pytorch/issues/8637)

- torch.nn.DistributedDataParallel (DDP)

이번에는 ```torch.nn.parallel.DistributedDataParallel```, 즉 DDP를 사용할 경우에 대해서 알아보겠습니다.<br>
아래의 코드는 [pytorch imagenet example](https://github.com/pytorch/examples/blob/792d336019a28a679e29cf174e10cee80ead8722/imagenet/main.py)에서 DDP와 관련된 부분만 남겨 재구성 한 코드입니다.

{% gist SeunghyunSEO/2a03baf56c0d6a96d220269804521318 %}

gist 코드 테스트중<br>




