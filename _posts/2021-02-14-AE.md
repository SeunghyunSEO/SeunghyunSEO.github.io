---
title: From AutoEncoder(AE) to Variational AutoEncoder(VAE)
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

본 포스트는 [lillog의 'from AutoEncoder to beta VAE' 블로그 post](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)와 [이활석(전 Clova leader)님의 '오토 인코더의 모든 것 (1~3)'](https://www.youtube.com/watch?v=o_peo6U7IRM) [+(presentation slide)](https://www.slideshare.net/NaverEngineering/ss-96581209) 등의 자료들을 참고하여 만들었습니다.

- <mark style='background-color: #fff5b1'> Dimensionality Reduction </mark>

![pcavslda](https://user-images.githubusercontent.com/48202736/107734574-122a5c80-6d41-11eb-85fe-aa05b23df9f4.png)
*Fig. 차원 축소 알고리즘의 대표적인 예인 PCA, LDA 출처 : [lecture slide from Haesun Park](https://project.inria.fr/siamsummerschool/files/2019/06/Lec2LRA.pdf)*


- <mark style='background-color: #fff5b1'> AutoEncoder (AE) </mark>

<img width="850" alt="ae1" src="https://user-images.githubusercontent.com/48202736/107733559-a7782180-6d3e-11eb-8cfc-35be14f7d4eb.png">
*Fig. 오토인코더 (AutoEncoder, AE) 모델 아키텍쳐*


- <mark style='background-color: #dcffe4'> Principal Components Analysis (PCA) vs AE </mark>

![pca3](https://user-images.githubusercontent.com/48202736/107734572-10f92f80-6d41-11eb-857e-18388f3dbe9c.png)
*Fig. 선형 차원 축소 알고리즘인 PCA (Kernel PCA아님) vs 비선형 차원 축소 알고리즘인 AE, 이미지 출처 : [link](https://www.researchgate.net/figure/Comparison-between-PCA-and-Autoencoder-15_fig1_340049776)*


![pca1](https://user-images.githubusercontent.com/48202736/107734566-0fc80280-6d41-11eb-9fd5-f461f94fc255.png)
![pca2](https://user-images.githubusercontent.com/48202736/107734571-10609900-6d41-11eb-91c7-c533d738b9a5.png)
*Fig. PCA vs AE 의 embedding space representation, 이미지 출처 : [link](https://stats.stackexchange.com/questions/190148/building-an-autoencoder-in-tensorflow-to-surpass-pca)*


- <mark style='background-color: #dcffe4'> Restricted Boltzman Machine (RBM) vs AE </mark>

- <mark style='background-color: #dcffe4'> Denoising AutoEncoder (DAE) </mark>

<img width="1035" alt="dae1" src="https://user-images.githubusercontent.com/48202736/107733564-aa731200-6d3e-11eb-9c43-5af450a2c0fb.png">

- <mark style='background-color: #dcffe4'> Contractive AutoEncoder (CAE) </mark>





- <mark style='background-color: #fff5b1'> Variational AutoEncoder (VAE) </mark>

<img width="961" alt="vae1" src="https://user-images.githubusercontent.com/48202736/107733571-ac3cd580-6d3e-11eb-99b9-92cd42df5d65.png">

- <mark style='background-color: #dcffe4'> AE vs VAE </mark>

<img width="850" alt="ae1" src="https://user-images.githubusercontent.com/48202736/107733559-a7782180-6d3e-11eb-8cfc-35be14f7d4eb.png">

<img width="1183" alt="vae2" src="https://user-images.githubusercontent.com/48202736/107733574-acd56c00-6d3e-11eb-8f4a-2c65d331c84d.png">


- <mark style='background-color: #dcffe4'> Objective Function of VAE : ELBO </mark>

- <mark style='background-color: #dcffe4'> Reparamaterization Trick </mark>

![reparam1](https://user-images.githubusercontent.com/48202736/107733569-aba43f00-6d3e-11eb-8f4d-4994745f83eb.png)






- <mark style='background-color: #dcffe4'> Conditional Variational AutoEncoder (CVAE) </mark>

- <mark style='background-color: #dcffe4'> Vector Quantized - Variational AutoEncoder (VQ-VAE) </mark>

<img width="1617" alt="vq-vae" src="https://user-images.githubusercontent.com/48202736/107733581-ae9f2f80-6d3e-11eb-94cf-f5b9e70f1812.png">







- <mark style='background-color: #dcffe4'> + Generative Adversarial Networks (GAN) </mark>
