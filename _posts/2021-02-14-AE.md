---
title: (미완)From AutoEncoder(AE) to Variational AutoEncoder(VAE)
categories: DeepLearning
tag: [DeepLearning]

toc: true
toc_sticky: true
---

본 포스트는 [CS W182 / 282A at UC Berkeley - Designing, Visualizing and Understanding Deep Neural Networks](https://cs182sp21.github.io/), [lillog의 'from AutoEncoder to beta VAE'](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html) 그리고 [이활석(전 Clova leader)님의 '오토 인코더의 모든 것 (1~3)'](https://www.youtube.com/watch?v=o_peo6U7IRM) [+(presentation slide)](https://www.slideshare.net/NaverEngineering/ss-96581209) 등의 자료들을 참고하여 만들었습니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> Dimensionality Reduction </mark>

`오토인코더(AutoEncoder, AE)`는 비지도 학습(Unsupervised Learning)을 통해서 "어떻게하면 큰 차원의 입력 데이터를 작은 차원의 데이터로 줄일까? 근데 또 막무가내로 줄이는건 아니고 의미있는 정보는 최대한 가지면서(혹은 더 대단한 성분(more efficient and compressed representation)을 추출하면서) 줄일 수 있을까?" 라는 생각에서 디자인된 뉴럴 네트워크(Neural Network, NN) 입니다.  

물론 `차원 축소(Dimensionality Reduction)` 알고리즘에는 오토인코더만 있는게 아니고, non-parametric한 방법인 주성분 분석(Principal Components Analysis)이나 선형 판별 분석(Linear Discriminant Analysis, LDA) 등 다양한 방법이 존재합니다. 

|Dimensionality Reduction|
|---|
|1.	Principal component analysis (PCA)|
|2.	Non-negative matrix factorization (NMF)|
|3.	Kernel PCA|
|4.	Graph-based kernel PCA|
|5.	Linear discriminant analysis (LDA)|
|6.	Generalized discriminant analysis (GDA)|
|7.	Autoencoder|
|8.	t-SNE|
|9.	UMAP|
|...|

*출처 : [Wikipidea 문서](https://en.wikipedia.org/wiki/Dimensionality_reduction), 물론 위키에 있는 방법이 전부가 아니며, 다른 방법들도 많이 있습니다.*

![lda](/assets/images/ae_to_vae/pca_vs_lda.png)
*Fig. 차원 축소 알고리즘의 대표적인 예인 PCA, LDA 출처 : [lecture slide from Haesun Park](https://project.inria.fr/siamsummerschool/files/2019/06/Lec2LRA.pdf)*

하지만 이번 글에서는 AE에 대해서만 알아볼 것이고, 더 나아가 다양한 목적을 위한 AE의 Variation들에 대해서도 알아보도록 하겠습니다.





## <mark style='background-color: #fff5b1'> AutoEncoder (AE) </mark>

오토 인코더는 아래와 같이 생겼습니다. 

우리가 앞서 말했던 것 처럼 오토 인코더라는 차원을 축소하는 것이 목적입니다. (물론 그렇게 줄어든 차원 속에 더욱 중요하고 효율적인 정보가 담겨있다는 믿음도 있습니다.)
사실 우리가 정말 원하는 것은 오토 인코더의 `인코더(Encoder)` 부분 그러니까 고차원의 입력 데이터를 저차원으로, 정보를 압축(Encoding)하는 부분입니다.

입력 데이터 x를 네트워크에 통과시켜 다시 x를 만들어내게끔 하는 비지도 학습 방법으로 학습 시키기 위해서 다시 저차원의 압축된 정보를 원래의 차원으로 복원시켜줄 `디코더(Decoder)`를 덧붙혀서 학습 시키는 겁니다.


![cs285_lec17_ae1](/assets/images/ae_to_vae/cs285_lec17_ae1.png)
*Fig. 오토인코더 (AutoEncoder, AE) 모델 아키텍쳐, AE가 원하는 것은 Encoder를 학습하는 것이다.*

그래서 오토인코더는 위와같이 네트워크가 좁아졌다가 다시 넓어지는 형태로 생겼습니다. 가운데 차원이 확 좁아지는 곳은 물병의 목 같다고 해서 `Bottle Neck` 이라고 합니다.   

이제 수식적인 term들을 추가해서 글을 전개하기 위해 Notation을 아래와 같이 정의하고 이야기하도록 하겠습니다.  





### Notation

{: class="info"}
| Symbol | Mean(Kor) | Mean(Eng) |
| ---------- | ---------- ||
| $$\mathcal{D}$$ | 데이터 셋 집합, 크기는 n | The dataset, $$\mathcal{D} = \{ \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(n)} \}$$, contains $$n$$ data samples; $$\vert\mathcal{D}\vert =n $$. |
| $$\mathbf{x}^{(i)}$$ | 각 데이터 포인트는 d차원으로 이루어져 있음. | Each data point is a vector of $$d$$ dimensions, $$\mathbf{x}^{(i)} = [x^{(i)}_1, x^{(i)}_2, \dots, x^{(i)}_d]$$. |
| $$\mathbf{x}$$ | 데이터 셋 집합으로 부터 샘플링한 데이터 1개 | One data sample from the dataset, $$\mathbf{x} \in \mathcal{D}$$. |
| $$\mathbf{x}’$$| 축소했다가 디코더로부터 복원된 데이터 x | The reconstructed version of $$\mathbf{x}$$. |
| $$\tilde{\mathbf{x}}$$ | 노이즈가 섞인 데이터 x | The corrupted version of $$\mathbf{x}$$. |
| $$\mathbf{z}$$ | 인코더가 뱉은 표현 벡터 | The compressed code learned in the bottleneck layer. |
| $$a_j^{(l)}$$ | l번째 레이어의 j번째 뉴런의 활성 함수  | The activation function for the $$j$$-th neuron in the $$l$$-th hidden layer. |
| $$g_{\phi}(.)$$ | $$\phi$$로 매개변수 화 되어있는 인코더 (*What AE want!*) | The **encoding** function parameterized by $$\phi$$. |
| $$f_{\theta}(.)$$ | $$\theta$$로 매개변수 화 되어있는 디코더 | The **decoding** function parameterized by $$\theta$$. |
| $$q_{\phi}(\mathbf{z}\vert\mathbf{x})$$ | 사후 확률 분포를 뱉는 '확률적' 인코더 (위의 인코더와 다름) |Estimated posterior probability function, also known as **probabilistic encoder**.  |
| $$p_{\theta}(\mathbf{x}\vert\mathbf{z})$$ | 마찬가지로 인코더가 예측한 확률 분포에서 샘플링을 통해 추출한 잠재 변수를 입력으로 하는 '확률적' 디코더 (*What VAE want!*) | Likelihood of generating true data sample given the latent code, also known as **probabilistic decoder**. |
| ---------- | ---------- |

수식을 포함해서 다시 얘기하자면,
기본적인 오토인코더 모델은 (추후에 기술할 예정인 VAE는 조금 다릅니다) 파라메터 $$\phi$$로 모델링 된 인코더 함수 $$g(.)$$와 파라메터 $$\theta$$로 모델링 된 디코더 $$f(.)$$로 이루어져있습니다.
병목 층(bottle neck layer)에서 학습이 된 representation은 $$\mathbf{z} = g_\phi(\mathbf{x})$$ 이며, 이를 입력받아 디코더를 통해 얻은 복원된 데이터는 $$\mathbf{x}' = f_\theta(g_\phi(\mathbf{x}))$$ 라고 합니다.

인코더와 디코더의 파라메터인 $$(\theta, \phi)$$는 각각 따로 학습되는것이 아니라 $$x$$를 넣어서 원래 입력값인 $$\mathbf{x} \approx f_\theta(g_\phi(\mathbf{x}))$$를 복원하는 과정에서 함께 학습이 됩니다.

이렇게 학습을 시키는 것은 `Cross Entropy Loss`를 사용하는 등 다양한 방법이 있지만, 일반적으로 간단하게 `Mean Squared Error Loss`를 사용해서 학습하게 됩니다.

$$
L_\text{AE}(\theta, \phi) = \frac{1}{n}\sum_{i=1}^n (\mathbf{x}^{(i)} - f_\theta(g_\phi(\mathbf{x}^{(i)})))^2
$$








### <mark style='background-color: #dcffe4'> Principal Components Analysis (PCA) vs AE </mark>

사실 차원 축소 방법론에는 PCA와 같은 방법론도 있는데요, AE가 비선형 활성함수들을 사용한 신경망 (Neural Network, NN)이 아니라, 선형적인 특성만 이용한다면 이는 PCA와 크게 다르지 않은 결과를 만들어 냅니다.


(증명은 하지 않겠습니다.)


![pca_vs_ae](/assets/images/ae_to_vae/pca_vs_ae.png){: width="50%"}
*Fig. 선형 차원 축소 알고리즘인 PCA (Kernel PCA아님) vs 비선형 차원 축소 알고리즘인 AE, 이미지 출처 : [link](https://www.researchgate.net/figure/Comparison-between-PCA-and-Autoencoder-15_fig1_340049776)*

위의 그림에서는 원 데이터들이 일반적인 AE를 사용할 경우 `비선형 매니폴드 (Non-linear Manifold)`에 매핑된 것을 볼 수 있고, PCA를 사용할 경우 `선형 매니폴드 (Linear Manifold)`에 매핑된 것을 볼 수 있습니다.


또한 각각의 차원 축소 방법론을 사용해 (인코더를 사용해) MNIST 분류를 하는 경우 아래처럼 얼마나 같은 Class의 숫자끼리 뭉치는지 차이가 많이 나는 것을 확인할 수 있습니다.

![pca_vs_ae_embedding](/assets/images/ae_to_vae/pca_vs_ae_embedding.png)

![pca_vs_ae_embedding2](/assets/images/ae_to_vae/pca_vs_ae_embedding2.png)
*Fig. PCA vs AE 의 embedding space representation, 이미지 출처 : [link](https://stats.stackexchange.com/questions/190148/building-an-autoencoder-in-tensorflow-to-surpass-pca)*










### <mark style='background-color: #dcffe4'> Denoising AutoEncoder (DAE) </mark>

AE는 $$x$$를 given으로 다시 $$x$$를 예측하는 방법론으로, 오버피팅을 하게 되는 문제가 필연적으로 발생하는데,
`디노이징 오토인코더 (Denoisig Autoencoder, DAE)`는 이를 해결하기 위해 제안되었습니다.

의도적으로 입력 데이터에 노이즈 (Noise) 를 섞어서 (corrupt) 이를 다시 없애는 말 그대로 "De-Noise"하게 끔 인코더를 학습시키는 겁니다.
가장 간단하게는 단순히 입력값의 벡터 요소들을 "0" 으로 마스킹한다고 생각할 수 있습니다.
이렇게 함으로써 데이터를 증강 (Augmentation) 시키는 효과를 본다고 할 수도 있겠죠.

![cs285_lec17_dae1](/assets/images/ae_to_vae/cs285_lec17_dae1.png)
*Fig. Denoising AutoEncoder (DAE)는 더럽힌 이미지를 원본으로 복원시키는 방식으로 학습해 AE보다 더욱 강건한 (robust) Encoder를 얻는게 목적이다.*

DAE는 수식적으로 아래처럼 간단히 나타낼 수 있습니다.

$$
\begin{aligned}
\tilde{\mathbf{x}}^{(i)} &\sim \mathcal{M}_\mathcal{D}(\tilde{\mathbf{x}}^{(i)} \vert \mathbf{x}^{(i)})\\
L_\text{DAE}(\theta, \phi) &= \frac{1}{n} \sum_{i=1}^n (\mathbf{x}^{(i)} - f_\theta(g_\phi(\tilde{\mathbf{x}}^{(i)})))^2
\end{aligned}
$$

여기서 원본 이미지를 마스킹 하는 것은 확률적인 분포로부터 샘플링 하여 진행합니다.


이러한 `Denoising Auto-Encoding` 방법론은 간단하지만 현대의 강력한 딥러닝 모델들에서도 계속해서 쓰이는 방법론인데요,
그 대표적인 예로 `BERT`를 들 수 있습니다. 

![jay_bert](/assets/images/ae_to_vae/jay_bert.png)
*Fig. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)의 모식도. 버트는 DAE와 마찬가지로 sequence의 토큰 일부를 랜덤하게 마스크 토큰으로 바꾼 뒤, 해당 부분을 예측하는 비지도 학습 방식으로 학습한다.*

(이미지 출처 : [The Illustrated BERT, ELMo, and co. from Jay Alammar](http://jalammar.github.io/illustrated-bert/))


![xlnet_bert](/assets/images/ae_to_vae/xlnet_bert.png)
*Fig. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237)에서 언급된 버트. 버트는 Denoising Auto-Encoding 방법론의 일종이다.*












## <mark style='background-color: #fff5b1'> Variational AutoEncoder (VAE) </mark>

`Variational AutoEncoder (VAE)`는 Kingma라는 연구자에 의해서 2014년에 처음 제안되었습니다. [Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes."](https://arxiv.org/pdf/1312.6114)

사실 보기에는 이름 자체가 (Variational) `AutoEncoder`이기 때문에 오토인코더와 다른점이 거의 없어 보이지만, 그렇지 않습니다.

VAE는 논문에서도 저자들이 이야기하듯, variational bayesian 방법과 graphical model과 관련이 있는 모델이며, 
목적 자체도 오토인코더와 같이 차원축소를 목적으로 '저차원의 유의미한 representation을 추출한다' 가 아니라, `'데이터로부터 단순히 고정된 벡터 (fixed vector)가 아닌 잠재 분포(latent distribution)를 찾아내고, 이로부터 샘플링을 통해 데이터셋에 없는 새로운 데이터를 만들어내는 생성 모델(generative model)을 만들자'` 입니다. (VAE는 생성모델)


물론 $$x$$를 넣어서 원래 입력값인 $$\mathbf{x} \approx f_\theta(g_\phi(\mathbf{x}))$$를 복원해내는 비지도 학습 방식으로 학습되는 것은 AE와 똑같습니다, 하지만 학습을 위한 목적 함수도 다르고, 학습이 된 후 네트워크의 어떤 부분을 쓰느냐도 다릅니다.

![lee_ae1](/assets/images/ae_to_vae/lee_ae1.png){: width="80%"}
*Fig. AutoEncoder는 유의미한 공간으로의 차원 축소를 해줄 Encoder를 학습하는게 목적이며, 이를 비지도 학습 방법론으로 학습하기 위해 Decoder를 붙힌 것이다.*

일반적인 오토인코더는 인코더를 앞서 말한 방식과 같이 학습하기 위해서 디코더를 붙힐 수 밖에 없었던 것이고, VAE는 디코더를 학습하기 위해서 인코더를 붙힌것으로 목적 자체가 다릅니다.

![lee_vae1](/assets/images/ae_to_vae/lee_vae1.png){: width="60%"}
*Fig. Variational AutoEncoder는 우리가 어떠한 유의미한 데이터 분포를 알고 있을 때, 이를 입력으로 유의미한 데이터를 만들어내기 위한 (i.e. 이미지) Decoder를 학습하는것이 목적이며, 이를 AE와 마찬가지로 비지도 학습 방법론으로 달성하기 위해서 Encoder를 붙힌 것이다.*

![lee_vae2](/assets/images/ae_to_vae/lee_vae2.png){: width="100%"}
*Fig. Encoder를 붙힌 VAE의 모습은 AE와 유사하다. Sample을 한다는 것의 의미는 후술하기로 한다.*

(+ `Variational` AutoEncoder의 'Variational'이란 앞서 말한 posterior라고 할 수 있는 latent distribution $$z$$를 direct로 학습할 수 없기 때문에 사용하는 근사방법인, 변분 추론(Variational Inference)에서 따온 이름입니다.)  


이제 VAE에 대해서 차분히 알아보도록 할건데, 그 전에 우리는 `잠재 변수 모델 (Latent Variable Model)`이 무엇이며 어떤 의미를 가지는지에 대해 알 필요가 있기 때문에 이에 대해 먼저 알아보도록 하겠습니다.









### <mark style='background-color: #dcffe4'> What is Latent Variable Model ? </mark>

잠재 변수 모델은 과연 뭘까요?


머신러닝은 일반적으로 $$p(x)$$이나 $$p(y \vert x)$$에 대한 분포를 모델링하고 (연속적이거나 이산적인 분포 i.e. 가우시안, 카테고리컬 분포) 이 분포를 통해 구한 데이터 셋 전체에 대한 likelihood를 최대화 하는 분포에 대한 파라메터를 찾는것이 목적입니다.
여기서 $$p(x)$$는 일반적으로 `확률적 생성 모델 (Probabilistic Generative Model)`, $$p(y \vert x)$$는 `확률적 판별 모델 (Probabilistic Discriminative Model)`이라고 합니다.

![cs285_lec17_generative](/assets/images/ae_to_vae/cs285_lec17_generative.png){: width="40%"}
*Fig. 확률적 생성 모델, $$ p(x) $$*

![cs285_lec17_discriminative](/assets/images/ae_to_vae/cs285_lec17_discriminative.png){: width="80%"}
*Fig. 확률적 판별 모델, $$ p(y \vert x) $$*

하지만 확률적이라고 해도 이러한 모델들에서 랜덤 변수 (random varialbes)라고 할 만한 것들은 데이터의 입출력 $$x,y$$ 밖에 없는데요, 
이러한 확률적 분포 모델 $$p(x)$$, $$p(y \vert x)$$에 랜덤 변수 $$z$$를 추가해서 모델링 하는 것이 바로 잠재 변수 모델이며, 이 때 $$z$$를 잠재 변수라고 합니다.

직관적으로 이해하기 위해 우리가 다음의 2차원 데이터를 가지고 있다고 생각하도록 하겠습니다.

![cs285_lec17_latent1](/assets/images/ae_to_vae/cs285_lec17_latent1.png){: width="40%"}
*Fig. 여기서 색을 입힌건 편의를 위해서 이지 정답이 주어진게 아니며, 검은색 원 또한 잠재 변수 모델로 얻어낸 다변량 다봉 정규 분포 (Multi-modal Multivariate Gaussian Distribution)를 나타내는 것이지, 실제로 우리가 알고 있는 것은 데이터 $$x$$ 정보 밖에 없습니다.*

당연히 위의 데이터를 가지고 있을 때 우리가 모델링 하고 싶은 $$p(x)$$ 분포는 봉우리가 하나 뿐인 다변량 정규 분포가 아니라, 데이터를 잘 분리해서 표현하는 다변량 정규 분포 일겁니다.

하지만 우리가 가지고 있는 정보는 입력 변수 밖에 없으며, 어떤식으로 다변량 정규 분포를 표현해야 할 지 모르죠.

여기서 실제 데이터로 주어지지는 않았지만, 입력 데이터에 무언가 내재되어 있으며, 이 내재되어있는 숨겨진 변수 (hidden variable)이 그러한 정보를 가지고 있을 것이다 라고 생각을 해 보면 어떨까요?

$$
p(x) = \sum_z p(x \vert z) p(z)
$$

그러한 잠재 변수는 위의 수식처럼 `주변화 (marginalization)`를 통해 새로 표현할 수 있고, 우리는 이런 모델을 통해서 더욱 복잡한 분포를 모델링 할 수 있습니다.

![cs285_lec17_latent1](/assets/images/ae_to_vae/cs285_lec17_latent1.png){: width="40%"}
*Fig. Gaussian Mixture Model*

위의 예제에서는 $$p(z)$$는 데이터를 여러 클래스로 나누는 카테고리컬 분포 (Categorical Distribution)입니다.


이는 마찬가지로 아래와 같은 판별 모델에도 적용할 수 있습니다.

$$
p(y \vert x) \\
p(y \vert x) = \sum_z p(y \vert x,z) p(z)
$$

이제 이를 더 확장해서 생각해보도록 하겠습니다.

![cs285_lec17_px](/assets/images/ae_to_vae/cs285_lec17_px.png){: width="70%"}
*Fig.*

![cs285_lec17_pz](/assets/images/ae_to_vae/cs285_lec17_pz.png){: width="70%"}
*Fig.*

$$
p(x) = \int p(x \vert z) p(z) dz
$$

![cs285_lec17_pxz1](/assets/images/ae_to_vae/cs285_lec17_pxz1.png){: width="70%"}
*Fig.*

![cs285_lec17_pxz2](/assets/images/ae_to_vae/cs285_lec17_pxz2.png){: width="60%"}
*Fig.*




- The model : $$p_{\theta} (x)$$
- The data : $$D = \{ x_1,x_2, \cdots, x_N \}$$
- Maximum likelihood fit : 

$$ 
\theta \leftarrow argmax_{\theta} \frac{1}{N} \sum_i log p_{\theta} (x_i)  \\ 
\theta \leftarrow argmax_{\theta} \frac{1}{N} \sum_i log ( \int p_{\theta}(x_i \vert z) p(z) dz ) 
$$



$$
\theta \leftarrow argmax_{\theta} \frac{1}{N} \sum_i \mathbb{E}_{z \sim p(z \vert x_i)} [log p_{\theta} (x_i \vert z)] 
$$


![cs285_lec17_vae2](/assets/images/ae_to_vae/cs285_lec17_vae2.png)
*Fig.*








### <mark style='background-color: #dcffe4'> VAE </mark>


![cs285_lec18_vae1](/assets/images/ae_to_vae/cs285_lec18_vae1.png)
*Fig.*

![cs285_lec18_vae2](/assets/images/ae_to_vae/cs285_lec18_vae2.png)
*Fig.*

![cs285_lec18_vae3](/assets/images/ae_to_vae/cs285_lec18_vae3.png){: width="40%"}
*Fig.*














### <mark style='background-color: #dcffe4'> VAE from scratch </mark>

***

- Encoding : $$q_{\phi} (z \vert x)$$, $$x$$를 입력으로 `분포를 예측하는` 네트워크 (고정된 크기의 벡터 아님, 예측하는 분포가 가우시안 분포라면 평균 (mean), 분산 (variance)을 예측하는 것)
- Sampling : $$z \sim q_{\phi} (z \vert x)$$, $$x$$를 입력으로 인코더가 내놓은 분포로부터 샘플링을 해서 벡터 하나를 추출. 
- Decoding : $$x' = p_{\theta}(x \vert z)$$, 추출한 벡터 $$z$$를 입력받은 디코더가 최종적으로 $$x'$$를 만들어냄, 우리의 목표는 $$x' = x$$가 되게끔 하는 것.

- Prior : $$p_\theta(\mathbf{z})$$
- Likelihood : $$p_\theta(\mathbf{x}\vert\mathbf{z})$$
- Posterior : $$p_\theta(\mathbf{z}\vert\mathbf{x})$$

***

$$
\theta^{*} = \arg\max_\theta \prod_{i=1}^n p_\theta(\mathbf{x}^{(i)})
$$

$$
\theta^{*} = \arg\max_\theta \sum_{i=1}^n \log p_\theta(\mathbf{x}^{(i)})
$$

$$
p_\theta(\mathbf{x}^{(i)}) = \int p_\theta(\mathbf{x}^{(i)}\vert\mathbf{z}) p_\theta(\mathbf{z}) d\mathbf{z} 
$$


![vae1](/assets/images/ae_to_vae/vae1.png)
*Fig. Variational AutoEncoder의 모식도*

![ae](/assets/images/ae_to_vae/ae.png)
*Fig. 일반적인 AutoEncoder의 모식도*

![vae2](/assets/images/ae_to_vae/vae2.png)
*Fig. Variational AutoEncoder의 모식도 2, 여기서는 우리가 추정하고자 하는 latent distribution $$z$$가 가우시안 분포라고 생각하기 때문에 평균, 분산을 찾게 됩니다.*



### <mark style='background-color: #dcffe4'> Objective Function of VAE : ELBO </mark>

### <mark style='background-color: #dcffe4'> Reparamaterization Trick </mark>

![repram](/assets/images/ae_to_vae/repram.png)

### <mark style='background-color: #dcffe4'> Normalizing flow models </mark>

### <mark style='background-color: #dcffe4'> Generative Adversarial Networks (GAN) </mark>

### <mark style='background-color: #dcffe4'> Conditional Variational AutoEncoder (CVAE) </mark>

### <mark style='background-color: #dcffe4'> Beta - Variational AutoEncoder (Beta-VAE) </mark>

### <mark style='background-color: #dcffe4'> Vector Quantized - Variational AutoEncoder (VQ-VAE) </mark>

![vq_vae](/assets/images/ae_to_vae/vq_vae.png)

### <mark style='background-color: #dcffe4'> Temporal Difference - Variational AutoEncoder (TD-VAE) </mark>













## <mark style='background-color: #fff5b1'> References </mark>

1. ['from AutoEncoder to beta VAE' form lillog](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)
2. ['오토 인코더의 모든 것 (1~3)' from Hwalseok Lee](https://www.youtube.com/watch?v=o_peo6U7IRM) 
3. ['On manifolds and autoencoders' from Pascal Vincent](http://videolectures.net/deeplearning2015_vincent_autoencoders/?q=vincent%20autoencoder)
4. [CS W182 / 282A at UC Berkeley - Designing, Visualizing and Understanding Deep Neural Networks](https://cs182sp21.github.io/)
5. [CS 182 - Lecture 17 - Generative Models](https://www.youtube.com/watch?v=AX5v5med3Rw&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=53)
6. [CS 182 - Lecture 18 - Latent Variable Models](https://www.youtube.com/watch?v=9KTrUea1apo&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=55)

