---
title: (미완)Normalization Series
categories: DeepLearning
tag: [tmp]

toc: true
toc_sticky: true
---

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> Normalization </mark>

## <mark style='background-color: #fff5b1'> References </mark>

- 1.[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167)

- 2.[How Does Batch Normalization Help Optimization?](https://arxiv.org/pdf/1805.11604)

- 3.[Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift](https://arxiv.org/pdf/1801.05134)

- 4.[Understanding Batch Normalization](https://arxiv.org/pdf/1806.02375)

- 5.[Layer Normalization](https://arxiv.org/pdf/1607.06450)

- 6.[PowerNorm: Rethinking Batch Normalization in Transformers](https://arxiv.org/pdf/2003.07845)

- 7.[PowerNorm slides](https://sincerass.github.io/docs/powernorm_slides.pdf)

- 7.[How Does Batch Normalization Help Optimization? slides from MIT](https://www.microsoft.com/en-us/research/uploads/prod/2019/05/How-does-Batch-Normalization-Help-Optimization-slides.pdf)

