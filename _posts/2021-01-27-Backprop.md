---
title: About BackPropagation
categories: DeepLearning
tag: [DeepLearning]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Deep Learning Mechanism </mark>

머신러닝, 딥러닝이란 뭘까?

디테일한 부분을 생략하고 High-level에서 생각해보겠습니다.

예를 들어 우리가 음성인식, 이미지 분류 같은 문제를 풀고 싶다고 합시다.

우리가 원하는건 입력 데이터 x를 넣으면 출력을 뽑아주는 (음성인식이면 dictation된 문장이, 이미지 분류면 분류의 결과 ex)'개' 같은 것) 어떤 $$oracle \space function$$인 $$y=f(x)$$를 얻는겁니다.

머신러닝, 딥러닝 알고리즘을 통해 솔루션을 구한다는것은 바로 이 $$oracle function$$이 뭔지는 알 수 없지만, 이에 가장 근사한 함수, $$approximate \space function$$을 무수히 많은 데이터를 가지고 

무수히 많이 시행 착오($$trial \space and \space error$$, 시행은 $$forward$$, 착오는 $$backward$$)를 통해, 즉 학습을 통해 구해 내는 것입니다.

$$approximate \space function$$의 출력은 확률 분포로 모델링 되어있고, 이 출력 분포는 입력 데이터를 각종 수많은(많게는 175 billion 개, GPT3) 파라메터 $$\theta$$들로 정의된 노드들을 

통과 시켜 얻기 때문에 $$approximate \space function$$를 학습시킨다는 것은 이 파라메터들의 값들을 최적의 값이 되도록 조정하는 것을 말합니다.

하지만 일반적으로 이러한 파라메터를 학습시키기 위한 closed form solution은 존재하지 않기때문에 한방에 최적의 파라메터를 구해낼 수 없고,

그렇기 때문에 우리는 일반적으로 gradient descent 라고 하는 최적화 기법을 통해서 파라메터를 점진적으로 바꿔갑니다.

학습이 잘 된다는 가정하에 파라메터를 점진적으로 바꾸다 보면 어느 지점에 수렴하게 되고, 최적해에 도달하게 됩니다.

최적의 값이 된다는 것은 '아 네트워크가 뽑은 출력값이, 출력분포가 정답과 유사해 지게 하는 것'이고 바로 이 실제와 추론한 것이 얼마나 차이가 나는지를 나타내는 손실 함수($$loss function$$) 입니다. 
( 출력 분포를 모델링 함으로써 자동으로 정해집니다, 분류 문제에서 Maximum likelihood가 곧 Cross Entropy Loss를 최소화 하는 것이 되듯)  

이 때 '내 네트워크의 10개의 파라메터 중 $$\theta_1$$은 얼만큼 바꾸고, $$\theta_2$$는 얼만큼 바꾸지?'라는 것을 손실 함수라는 것을 통해 구합니다.

손실 함수를 계산해서 네트워크는 '이 만큼 loss를 발생시킨데는 $$\theta_1$$의 값이 가장 주요했어, 그러니까 넌 크게 penalty를 받아야겠다, 크게 값을 조정해야겠어!' 라고 판단하여 파라메터를 조절합니다.

바로 여기서 '이 만큼 loss를 발생시킨데는 $$\theta_1$$의 값이 가장 주요했어'를 계산해 내는 것이 바로 오차 역전파(Back Propagation) 입니다. 


- <mark style='background-color: #fff5b1'> Back Propagation </mark>


- <mark style='background-color: #fff5b1'> "Yes you should understand backprop" - Andrej Karpathy </mark>



- <mark style='background-color: #fff5b1'> References </mark>

[link1](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)

[link2](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)

[link3](https://tensorflow.blog/2016/12/27/%ec%97%ad%ec%a0%84%ed%8c%8c-%ec%a7%81%ec%a0%91-%ec%a7%9c%eb%b4%90%ec%95%bc-%ed%95%98%eb%82%98%ec%9a%94/#more-20614)
