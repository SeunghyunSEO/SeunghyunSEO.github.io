---
title: (미완) Lecture 9 - Advanced Policy Gradients

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 9의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=ySenCHPsKJU&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=38)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-9.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

Lecture 9 의 주제는 다시 6장까지 다뤘던 정책 기반 알고리즘으로 돌아와 `Advanced Policy Gradient`입니다.

![slide1](/assets/images/CS285/lec-9/slide1.png)
*Slide. 1.*

***

이번에는 앞선 방식과 다르게 `Review Slide`를 먼저 보고가고 싶은데요,
본 강의에서 논하고자 하는 내용은 아래와 같습니다.

![slide25](/assets/images/CS285/lec-9/slide25.png)



***




## <mark style='background-color: #fff5b1'> Recap: policy gradients </mark>

![slide2](/assets/images/CS285/lec-9/slide2.png)
*Slide. 2.*

Recap 입니다. 


이제 두말하면 입아프지만 간략하게 이야기해보자면, 정책 경사 (Policy Gradient, PG) 알고리즘은 정책 (policy)을 위한 명시적인 (explicit) 네트워크를 (`Actor`) 따로 두고 이를 직접적으로 학습하는 방법론이었죠. $$(s_1,a_1,s_2,a_2,\cdots)$$와 같은 Trajectory를 여러번 샘플링 해서 데이터로 쓰며, 보상을 최대화하는 Objective, $$J(\theta)$$를 정의하고 최적화 문제를 풀기 위해서 이를 미분한 $$\bigtriangledown_{\theta} J(\theta)$$ 를 구해보면 이것은 일반적인 딥러닝의 log probability를 미분한것과 다르지 않으나 여기에 현재 상태에서 이 액션이 얼마나 좋은가?를 계산한 `reward to go`가 곱해진 weighted sum형태였음을 알 수 있습니다. 여기에 이 "reward to go"를 적당한 가치 기반 함수로 근사해서 학습할 수 있는데, 이 가치를 평가하는 것이 `Critic`이 되며, 이럴 경우를 `Actor-Critic` 알고리즘이라고 했습니다.


그리고 이러한 Vanilla Policy Gradient, Vanilla Actor-Critic 같은 알고리즘들의 업그레이드 버전들은 언제나 모델을 unbiased 하고 low-variance로 만드는 걸 목표로 이것저것 수정한 것들이었습니다.


여기서 질문, 왜 정책 경사 알고리즘이 실제로 잘 되는걸까요?
물론 정책 자체를 최적화 하는 알고리즘인데, `gradient descent (ascent)`가 잘 되니까 이거랑 같은 PG도 잘 되지않겠냐마는 
왜 이게 잘되는지에 대해서 더 좋은 idea가 있고, 이를 이해하면 더 좋은 알고리즘을 생각해 낼 수 있는 근간이 될거라고 Sergey 교수님은 얘기합니다.

![slide3](/assets/images/CS285/lec-9/slide3.png)
*Slide. 3.*

정책 경사 알고리즘의 일반적인 유형중 하나인 `REINFORCE`가 *Slide. 3.*의 상단에 나와있는데요, 이는 현재 정책을 위한 `Advantage Function`, $$A^{\pi}(s,a)$$를 추정하고 (A=Q-V니까 가치 평가랑 그게 그거), 이를 사용해서 정책을 한단계 업데이트하죠. 


이러한 매커니즘이 우리가 이전 장에서 배웠던 알고리즘과 비슷하지 않나요?


그렇습니다. 
사실 6장에서 정책 경사 ~ Actor-Critic까지 빌드업하다가 갑자기 정책 반복 알고리즘 등 가치기반 알고리즘을 배우고 다시 정책 기반 알고리즘으로 돌아온 이유가 있을겁니다. 
Policy Gradient는 Policy Iteration과 닮았습니다.


두 가지의 가장 큰 차이점이라면 policy를 업데이트 하는 데, 파라메터를 최적화하느냐? 아니면 평가한 가치를 기반으로 argmax하느냐 였습니다.
교수님은 정책 경사 알고리즘이 조금더 'gentle'하다고 표현하는데요, 왜냐면 정책을 업데이트 할 때 정책 반복은 argmax를 통해서 어떤 좋아보이는 행동에 과한 confidence를 주지만, 정책 경사 알고리즘은 좋아 보이는 (Advantage가 큰) 행동의 확률을 살짝 조정 (높혀) 하기 때문입니다.


즉 Policy Gradient는 Policy Iteration의 soft버전이라고 할 수 있다고 합니다.



둘의 관계에 대해서 좀 더 알아보기위해서 *Slide. 4.*에서는 정책 경사 알고리즘을 재해석 (re-interpret) 하려고 합니다.

![slide4](/assets/images/CS285/lec-9/slide4.png)
*Slide. 4.* 

현재 policy가 가지는 파라메터가 $$\theta$$, 그리고 최적화를 통해 한 스텝 업데이트한 policy의 파라메터가 $$\theta'$$라고 할 떄,

핵심은 $$J(\theta')-J(\theta)$$에 대해 생각해보는 것 입니다.
강화학습의 Objective는 아래와 같이 생겼는데요, 시작 지점부터 discount를 하는 방법은 실제로는 쓰지 않지만 일단 아래와 같이 정의했습니다.

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t \gamma^t r(s_t,a_t) ]
$$

우리는 $$J(\theta')-J(\theta)$$를 최대화 하는것이 RL objective $$J(\theta)$$를 최대화 하는것과 같다는 것을 보일 것입니다.
듣기에는 당연해 보일 수 있지만 우리가 증명하고자 하는 수식인 $$J(\theta')-J(\theta)$$ 은 아래와 같습니다.

$$
J(\theta')-J(\theta) = \mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ]
$$

즉 `old policy를 따라서 책정한 Advantage 값을 이용해서 new policy를 이용해 샘플링한 trajectory를 따라갔을때의 기대값` 인거죠.
우리가 원하는 것은 이 두개의 gap을 최대한 크게 하는 정책으로 한스텝 나아가는 것이기 때문에 $$\theta'$$에 관해서 위의 수식을 최대화 하는 것이 좋은 정책으로의 업데이트라는 것을 직감적으로 알 수는 있습니다.


한편 이는 Policy Iteration과 닮았는데요, 정책 반복이 Advantage Function을 계산 (평가) 하고, 이를 바탕으로 정책을 업데이트하는 것인데, 이와 크게 다르지 않은 흐름이기 때문입니다.


아무튼 위의 수식을 증명해보기 위해서 식을 전개할 건데요,
RL Objective를 전개하는 form은 여러가지 일 수 있습니다.

$$
J(\theta')-J(\theta) = J(\theta') - \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t \gamma^t r(s_t,a_t) ]
$$

위의 수식을 아래와 같이 전개할 수도 있습니다.

$$
J(\theta')-J(\theta) = J(\theta') - \mathbb{E}_{s_0 \sim p(s_0)}[V^{\pi_{\theta}(s_0)}]
$$

가치 함수의 수식을 전개해보면 그게 그거라는걸 알 수 있습니다.


위의 수식을 사용하면 $$\mathbb{E}_{s_0 \sim p(s_0)}[V^{\pi_{\theta}(s_0)}]$$가 더이상 $$\theta$$에 의존하지 않게 됩니다.
이 수식을 전개한 이유가 있는데요, 그건 바로 $$s_0 \sim p(s_0)$$ 부분을 같은 `state maginal`를 가지고 있는 어떠한 `Trajectory distribution`으로도 바꿀 수 있기 때문입니다.

즉 $$\theta'$$를 이용해서 뽑아도 되고, $$\theta,\theta_2',\theta_3',\cdots$$ 뭘 이용해서든 state marginal만 같으면 된다는 겁니다.
여기서 $$\theta'$$의 trajectory distribution을 사용해서 나타내면 아래와 같습니다. 

$$
J(\theta')-J(\theta) = J(\theta') - \mathbb{E}_{s_0 \sim p(s_0)}[V^{\pi_{\theta}(s_0)}]
$$

이렇게 해도 기대값 안의 수식은 $$V^{\pi_{\theta}}(s_0)$$로 $$s_0$$에만 의존하나 이 초기 상태는 같으므로 변하지 않습니다.
이 수식을 또 아래와 같이 표현할 수 있는데요,

$$
\begin{aligned}
&
J(\theta')-J(\theta) = J(\theta') - \mathbb{E}_{s_0 \sim p(s_0)}[V^{\pi_{\theta}(s_0)}]
& \\

&
= J(\theta') - \mathbb{E}_{s_0 \sim p(s_0)}[\sum_{t=0}^{\infty}\gamma^t V^{\pi_{\theta}}(s_t) - \sum_{t=1}^{\infty}\gamma^t V^{\pi_{\theta}}(s_t)]
& \\

&
= J(\theta') + \mathbb{E}_{s_0 \sim p(s_0)}[\sum_{t=0}^{\infty}\gamma^t (\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_{\theta}}(s_t) ) ]
& \\
\end{aligned}
$$

여기서 맨 마지막 수식에서 -가 +로 바뀌었다는 점에 주의하시고, 기대값에 있는 term을 보시면 Advantage Function과 비슷하게 생겼다는 걸 알 수 있습니다.
이제 아래처럼 수식을 끝까지 따라가다보면 우리가 원하는 결론을 증명할 수 있습니다.

$$
\begin{aligned}

&
J(\theta')-J(\theta) = J(\theta') + \mathbb{E}_{s_0 \sim p(s_0)}[\sum_{t=0}^{\infty}\gamma^t (\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_{\theta}}(s_t) ) ]
& \\

&
= \mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [\sum_{t=0}^{\infty} \gamma^t r(s_t,a_t)] + \mathbb{E}_{s_0 \sim p(s_0)}[\sum_{t=0}^{\infty}\gamma^t (\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_{\theta}}(s_t) ) ]
& \\

&
= \mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [\sum_{t=0}^{\infty} \gamma^t r(s_t,a_t) + (\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_{\theta}}(s_t) )]]
& \\

&
\mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ]
& \\ 

\end{aligned}
$$

즉, 만약 우리가 old policy로 계산한 Advantage 값을 이용해 new policy의 trajectory들을 따라서 계산한 기대값을 최대화 하는 게 곧 RL Objective를 최적화 하는 길이라는걸 증명한 것이죠.

이는 Policy Iteration이 올바른 방향으로 학습되는 알고리즘이라는 것을 보여주기도 한다고 합니다.


![slide5](/assets/images/CS285/lec-9/slide5.png)
*Slide. 5.*

우리가 앞서 증명한 증명한 수식인 $$J(\theta')-J(\theta)$$는 
아래와 같이 바꿀 수 있는데요,

$$
\begin{aligned}
&
J(\theta')-J(\theta) = \mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ]
& \\

&
J(\theta')-J(\theta) = \mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta'} (a_t \vert s_t)} [\gamma^t A^{\pi_{\theta} (s_t,a_t)}] ] 
& \\
\end{aligned}
$$

여기서 우리가 이전에 배웠던 `Importance Sampling (IS)`를 적용하면 Inner Expectation을 아래와 같이 정책 분포의 비율 (importance weight)을 곱하는 것으로 바꿀 수 있습니다.

$$
\begin{aligned}
&
\mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta'} (a_t \vert s_t)} [\gamma^t A^{\pi_{\theta} (s_t,a_t)}] ] 
& \\

&
\mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [ \frac{\pi_{\theta'} (a_t \vert s_t)}{\pi_{\theta} (a_t \vert s_t)} \gamma^t A^{\pi_{\theta} (s_t,a_t)}] ] 
& \\

\end{aligned}
$$

이제 수식이 굉장히 `Policy Gradient`의 수식과 비슷해졌는데요, 이와 비슷한 수식을 $$V(\theta) = V(\theta')$$ 일 때의 $$\theta'$$에 관해서 미분했기 때문입니다. 


주의할 점은 여기서는 state,$$s_t$$는 $$\pi_{\theta'}$$의 정책에서 샘플링되고 있고 $$(a_t \vert s_t)$$만 $$\pi_{\theta}$$로 부터 샘플링 되고 있다는 겁니다.

우리가 여기서 $$\theta' \rightarrow \theta$$로 바꾸면 Policy Gradient가 되는데,
과연 지금의 수식에서는 $$\pi_{\theta'}$$가 뭔지 아직 모르는 상태에서 state를 샘플링 하는게 되는데 이게 가능할까요? 아닙니다.
그리고 $$\theta'$$은 importance weight에도 하나 더 존재하네요.


![slide6](/assets/images/CS285/lec-9/slide6.png)
*Slide. 6.*

그러므로 여기서 약간은 근사 (무시, ignore)가 들어가야 되는데요, state distribution을 $$\theta$$에서 뽑는겁니다.

$$
\begin{aligned}
&
\mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [ \frac{\pi_{\theta'} (a_t \vert s_t)}{\pi_{\theta} (a_t \vert s_t)} \gamma^t A^{\pi_{\theta} (s_t,a_t)}] ]  \approx \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [ \frac{\pi_{\theta'} (a_t \vert s_t)}{\pi_{\theta} (a_t \vert s_t)} \gamma^t A^{\pi_{\theta} (s_t,a_t)}] ]
& \\

\end{aligned}
$$

된다고 치면, 그렇다면 남은것은 importance weight의 분자에 들어가있는 $$\theta'$$이기 때문에 우리는 Policy Iteration과 Policy Gradient가 같다는걸 증명할 수 있다고 합니다.


왜 이게 가능해야 할까요? 만약 우리가 아래의 수식을 $$\bar{A}(\theta')$$ 라고 한다면

$$
\bar{A}(\theta') = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [ \frac{\pi_{\theta'} (a_t \vert s_t)}{\pi_{\theta} (a_t \vert s_t)} \gamma^t A^{\pi_{\theta} (s_t,a_t)}] ]
$$

$$J(\theta')-J(\theta) \approx \bar{A}(\theta')$$가 되고 이 값이 최대가 되는 $$\theta'$$를 찾으면 정책이 최선의 방향으로 업데이트되는 것이나 마찬가지기 때문입니다.

$$
\theta' \leftarrow arg max_{\theta'} \bar{A} (\theta)
$$

다만 우리가 여기서 $$p_{\theta'}(s_t)$$를 $$p_{\theta}(s_t)$$로 바꿨기 때문에 이에 대한 근거가 필요한데요,
즉, $$\pi_{\theta}$$가 $$\pi_{\theta'}$$에 가까울 때 $$p_{\theta}(s_t)$$가 $$p_{\theta'}(s_t)$$ 두 분포가 충분히 가까워야 합니다.
그냥 듣기에는 "이게 왜? 당연한거 아니야?" 할 수 있지만 이를 제대로 증명하기가 쉽지 않다고 합니다. (non-vacuous하게)












## <mark style='background-color: #fff5b1'> Bounding the Distribution Change </mark>

![slide8](/assets/images/CS285/lec-9/slide8.png)
*Slide. 8.*

다시 Recap데요, 우리가 원하는건 `Distribution Mismatch`를 무시하는거 였습니다.
아래의 state distribution을 old policy로 바꿔서 `new policy`을 얻는 수식을 얻어냈죠.

$$
\begin{aligned}
&
\mathbb{E}_{\tau \sim p_{\theta'}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [ \frac{\pi_{\theta'} (a_t \vert s_t)}{\pi_{\theta} (a_t \vert s_t)} \gamma^t A^{\pi_{\theta} (s_t,a_t)}] ]  \approx \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t \gamma^t A^{\pi_{\theta}}(s_t,a_t) ] = \sum_t \mathbb{E}_{s_t \sim p_{\pi_{\theta'}}(s_t)} [\mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [ \frac{\pi_{\theta'} (a_t \vert s_t)}{\pi_{\theta} (a_t \vert s_t)} \gamma^t A^{\pi_{\theta} (s_t,a_t)}] ]
& \\

\end{aligned}
$$

이제 남은 새로운 정책의 파라메터가 있는 term은 importance weight의 분자에 있는 것 뿐이고,
앞서 말한 것 처럼 $$\pi_{\theta}$$가 $$\pi_{\theta'}$$에 가까울 때 $$p_{\theta}(s_t)$$가 $$p_{\theta'}(s_t)$$ 두 분포가 충분히 가까우면 좌변과 우변이 근사해질 겁니다. 
이번 subsection 에서는 이를 증명하고 이로부터 새로운 알고리즘을 도출하는게 목표입니다.






![slide9](/assets/images/CS285/lec-9/slide9.png)
*Slide. 9.*

*Slide. 9.*를 천천히 전개해보도록 하겠습니다.

일단 어떤 simple case를 가정해보도록 합니다. 

$$\theta$$로 파라메터화 된 어떤 정책 $$\pi_{\theta}$$이 `deterministic`하다고 합시다.

- Deterministic Policy : $$a = argmax_{a} \pi_{\theta}(s)$$ 
- Stochastic Policy : $$a \sim \pi_{\theta}(s)$$

```
사실 아직 deterministic policy가 뭐냐 stochastic이 뭐냐 하는 정의를 따로 하지는 않았으며
Sergey 교수님도 후에 stochastic policy를 따로 정의하겠다고 합니다.
```

어쨌든 우리가 지금 하려는걸 직관적으로 이해하기 위해서 deterministic policy로 설정하고
$$\pi_{\theta'}$$ 가 $$\pi_{\theta}$$에 가깝다는건 다음의 조건을 만족할 경우라고 생각하겠습니다. 

$$\pi_{\theta'} \text{ is close to } \pi_{\theta} \text{ if } \pi_{\theta'}(a_t \neq \pi_{\theta}(s_t) \vert s_t) \leq \epsilon$$

그렇다면 우리는 $$\pi_{\theta'}$$하에서 어떤 state, $$s_t$$에서 의 state marginal을 아래와 같이 쓸 수 있습니다.

$$
p_{\theta'}(s_t) = (1-\epsilon)^t p_{\theta}(s_t) + (1- (1-\epsilon)^t) p_{\text{mistake}}(s_t)
$$

이는 두 가지 term으로 구성되어있는데요, 우변의 첫 번째항은 모든 time-step, 즉 $$s_1 \sim s_t$$까지 새로운 정책 $$\pi_{\theta'}$$ 가 정확히 구 정책 $$\pi_{\theta}$$를 따라 가는것을 말합니다. 그리고 이렇게 정확히 똑같은 행동을 할 확률이 $$(1-\epsilon)$$ 이기 때문에 이 term에는 $$(1-\epsilon)^t$$만큼이 곱해지게 됩니다. 
두 번째 term은 다른 모든 경우를 말하는데요, 여기에는 다른 state distribution이 곱해집니다. 
이게 뭔지는 모릅니다.


이제 Bound를 구하기 위해서 2장에서 Imation Learning을 했을 때 했던 방식과 마찬가지로 $$p_{\theta'}(s_t) - p_{\theta}(s_t)$$의 KL-Divergence (KLD)를 구해보도록 하겠습니다.

$$(1-\epsilon)^t p_{\theta}(s_t) $$는 0이 되기때문에 빠지면 아래와 같이 되고, 

$$
\vert p_{\theta'}(s_t) - p_{\theta}(s_t) \vert = (1- (1-\epsilon)^t) \vert p_{\text{mistake}}(s_t) - p_{\theta}(s_t) \vert 
$$

우리가 $$p_{\text{mistake}}(s_t)$$가 뭔지  



![slide10](/assets/images/CS285/lec-9/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-9/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-9/slide12.png)
*Slide. 12.*











## <mark style='background-color: #fff5b1'> Policy Gradients with Constraints </mark>

![slide14](/assets/images/CS285/lec-9/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-9/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-9/slide16.png)
*Slide. 16.*











## <mark style='background-color: #fff5b1'> Natural Gradient </mark>

![slide18](/assets/images/CS285/lec-9/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-9/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-9/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-9/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-9/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-9/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-9/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-9/slide25.png)
*Slide. 25.*








### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse)










