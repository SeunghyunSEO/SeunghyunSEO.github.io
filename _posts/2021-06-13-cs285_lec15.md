---
title: (yet) Lecture 15 - Offline Reinforcement Learning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 15의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=ft1YdOEpAQg&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=66)
- [Lecture Slide Link 1](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-15.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

Lecture 15에서 배우게 될 내용은 `Offline Reinforcement Learning` 입니다.

![slide1](/assets/images/CS285/lec-15/slide1.png)
*Slide. 1.*

Lecture 15 에서부터 17까지는 Sergey Levine 교수님이 아닌 그의 제자이자 박사 과정 학생이신 [Aviral Kumar](https://aviralkumar2907.github.io/)님이 강의를 진행합니다.

![Aviral_Kumar](/assets/images/CS285/lec-15/Aviral_Kumar.png)
*Fig. Aviral_Kumar. 아마 Data Driven Offline RL이 연구 주제인 만큼 이번 강의를 맡은 듯 하다.*

Aviral_Kumar의 강의도 좋았으나 아무래도 대가(?)의 통찰력있는 도 같이 듣는게 좋을 것 같아 NIPS 2020 에서 진행한 2시간 43분짜리 Offline RL Tutorial, [NeurIPS 2020 Tutorial Offline Reinforcement Learning, From Algorithms to Practical Challenges](https://slideslive.com/38935785/offline-reinforcement-learning-from-algorithms-to-practical-challenges)을 참고하려고 했습니다 (여기서는 둘이 공동으로 진행하네요).


시작하기 전에 우리가 그동안 배웠던 것들에 대해서 생각해봅시다.
Deep RL을 관통하는 주제들에는 여러가지가 있었는데요, 한줄로 각각을 아주 간단하게 요약하자면

- `Policy-based vs Value-based` : Policy를 explicitly 학습할건지 아니면 implicitly 학습할건지?
- `On-Policy vs Off-Policy` : 현재 시점에서 가장 최근 (latest) Policy를 사용해서 $$s,a,a',r$$ 를 샘플해서 사용할건지? 아니면 다른 (latest여도 되고, 아니면 아예 다른거여도 됨) Policy를 사용해서 샘플한 데이터를 쓸건지? 

- `Model-based vs Model-Free` : Transition Dynamics를 알고 (이용할 수) 있는지?, 아니면 이를 모델링해서 학습하고 이를 기반으로 정책을 학습할건지? 아니면 아예 몰라도 되게끔 모델링할건지?

![on_policy_vs_off_policy](/assets/images/CS285/lec-15/on_policy_vs_off_policy.png)
*Fig. 클래식한 On-Policy의 경우 현재 시점이 $$k$$라면 $$\pi_k$$ 자신을 이용해 데이터를 샘플해서 $$\pi_{k+1}$$을 학습하는데 사용하는 반면, Off-Policy의 경우 예를들어 맨 처음의 $$\pi_0$$ 부터 $$\pi_1,\cdots,\pi_k$$까지 모든 정책들이 모은 데이터를 저장해두고 (Replay Buffer) 이를 $$\pi_{k+1}$$ 정책을 업데이트 하는 데 사용한다.* 

(이미지 출처 : [Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems](https://arxiv.org/pdf/2005.01643))


- `Exploration vs Exploitation` : 데이터로부터 학습한 정책으로 매 상태마다 최선의 수를 고를건지, 아니면 더 좋은 수를 위해 모험수를 둘건지?

였습니다.

그렇다면 Offline-RL 은 무엇일까요?

![offline_rl](/assets/images/CS285/lec-15/offline_rl.gif)
*Fig. Online RL vs Offline RL Animation*

(이미지 출처 : [An Optimistic Perspective on Offline Reinforcement Learning](https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html))

그건 바로 매번 데이터를 샘플해서 사용해야 한다는 단점을 보완하기 위해서 `어떤 Policy`를 두고 이로부터 샘플링을 엄청나게 진행한 뒤에 (roll-out) 이렇게 뽑은 데이터로 정책을 학습 하는겁니다.
중요한 점은 최초에 데이터를 엄청 뽑아두고 그 뒤로는 $$k=1,2,\cdots,k$$ 처럼 시간이 흐르더라도 데이터를 다시 샘플하지 않고 맨처음 저장해둔 데이터만 사용해서 계속 업데이트 한다는 거죠.
이 때 데이터를 뽑는 "어떤 Policy"를 바로 `Behavior Policy`, $$\pi_{\beta}$$라고 하며, 학습을 꽤 진행한 뒤에 한번 다시 데이터를 대거 샘플링해서 사용하기도 한다고 합니다. 

![offline](/assets/images/CS285/lec-15/offline.png)
*Fig. Offline RL은 일반적으로 데이터를 최초에 한번만 대량으로 모으고 그 뒤로 학습하는 과정에서는 따로 데이터를 더 만들지는 않는다.*

(이미지 출처 : [Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems](https://arxiv.org/pdf/2005.01643))


이는 현재 컴퓨터 비전, 자연어 처리, 음성 인식 등 수많은 문제들을 해결하고 있는 Data-Driven Approach인 Deep Learning 방법론이 대량의 데이터를 확보해서 이를 통해 분포를 학습하는 것과


![ml_vs_rl](/assets/images/CS285/lec-15/ml_vs_rl.png)
*Fig. ML vs RL*

매 번 정책을 학습하기 위한 유의미한 데이터를 샘플해야만 하는 RL이 굉장한 차이가 있기 때문에

![ml_vs_rl2](/assets/images/CS285/lec-15/ml_vs_rl2.png)
*Fig. ML vs RL 2*

"우리도 ML, DL 처럼 데이터 모아서 한번에 하면 안될까?" 라는 아이디어를 기반으로 하고 있습니다.


Offline RL은 결과적으로 Real-Wordl Application에 적용돼 좋은 퍼포먼스를 보여줬다고 하는데요,
사실 저는 이 강의를 통해 Deep RL, 특히 Offline RL 을 처음 공부해보는 것이기 때문에 "Offline RL이 이렇다더라~" 하는 자료들을 조금 봐도 "이게 실제로 학습이 잘 되나? 작용하는건가 실제 문제에서?" 라는 생각이 드는데요, 이게 정말 잘 되는건지에 대해서 이해해 보는 것이 이번 장의 목표입니다.



## <mark style='background-color: #fff5b1'> What is Offline RL?  </mark>

![slide2](/assets/images/CS285/lec-15/slide2.png)
*Slide. 2.*

![slide3](/assets/images/CS285/lec-15/slide3.png)
*Slide. 3.*

![slide4](/assets/images/CS285/lec-15/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-15/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-15/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-15/slide7.png)
*Slide. 7.*

![slide8](/assets/images/CS285/lec-15/slide8.png)
*Slide. 8.*

## <mark style='background-color: #fff5b1'> Part 1 : Classic Algorithms and  Challenges With Offline RL  </mark>

![slide10](/assets/images/CS285/lec-15/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-15/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-15/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-15/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-15/slide14.png)
*Slide. 14.*


## <mark style='background-color: #fff5b1'> So, why do RL algorithms fail, even  though imitation learning would work  in this setting (e.g., in Lecture 2)?  </mark>

![slide16](/assets/images/CS285/lec-15/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-15/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-15/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-15/slide19.png)
*Slide. 19.*

## <mark style='background-color: #fff5b1'> Part 2 : Deep RL Algorithms to  Address Distribution Shift </mark>

![slide21](/assets/images/CS285/lec-15/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-15/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-15/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-15/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-15/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-15/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-15/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-15/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-15/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-15/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-15/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-15/slide32.png)
*Slide. 32.*

![slide33](/assets/images/CS285/lec-15/slide33.png)
*Slide. 33.*

![slide34](/assets/images/CS285/lec-15/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-15/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-15/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-15/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-15/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-15/slide39.png)
*Slide. 39.*


## <mark style='background-color: #fff5b1'> Does Offline RL Work in Practice? </mark>

![slide41](/assets/images/CS285/lec-15/slide41.png)
*Slide. 41.*

![slide42](/assets/images/CS285/lec-15/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-15/slide43.png)
*Slide. 43.*











## <mark style='background-color: #fff5b1'> What’s the problem?  </mark>



### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)































