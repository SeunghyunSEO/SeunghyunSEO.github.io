---
title: LEAF - A Learnable Frontend For Audio Classification
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---

이 글은 [LEAF: A LEARNABLE FRONTEND FOR AUDIO CLASSIFICATION](https://arxiv.org/pdf/2101.08596)

- <mark style='background-color: #fff5b1'> A Frontend for Audio </mark>

<img width="871" alt="mirco1" src="https://user-images.githubusercontent.com/48202736/107218165-9c797480-6a52-11eb-90cb-5f61d5f03df6.png">
*Fig. a. Aud*

- <mark style='background-color: #fff5b1'> A Brief things for Digital Signal Processing (DSP) </mark>

- <mark style='background-color: #dcffe4'> Fourier Series </mark>

- <mark style='background-color: #dcffe4'> Fourier Transform (FT) </mark>

![fourier1](https://user-images.githubusercontent.com/48202736/107212081-c8442c80-6a49-11eb-9f4c-a609224f9327.png)
*Fig. a. Fourier Transform, 이미지 출처 : [link](http://www.sharetechnote.com/html/Eng_FFT.html#Common_Example_of_Fourier_Transform)*

- <mark style='background-color: #dcffe4'> Discrete Time Fourier Transform (DTFT) </mark>

- <mark style='background-color: #dcffe4'> Fast Fourier Transform (FFT) </mark>

- <mark style='background-color: #dcffe4'> Short Time Fourier Transform (STFT) </mark>

<center>$$ STFT(x[n])(m,w) = X(m,w) = \sum_{n=-\inf}^{\inf} x[n]w[n-m]e^{-jwn} $$</center>






- <mark style='background-color: #fff5b1'> LEAF - A Learnable Frontend For Audio Classification </mark>



- <mark style='background-color: #dcffe4'> 2018, SincNet </mark>

<img width="612" alt="sincnet1" src="https://user-images.githubusercontent.com/48202736/107185071-1430aa80-6a25-11eb-99f0-dacd18bc5110.png">
<img width="569" alt="sincnet2" src="https://user-images.githubusercontent.com/48202736/107185072-1430aa80-6a25-11eb-8f6f-0c629cca0806.png">
<img width="1280" alt="sincnet3" src="https://user-images.githubusercontent.com/48202736/107185073-14c94100-6a25-11eb-9c02-3c13181872b2.png">


- <mark style='background-color: #dcffe4'> 2019, PASE </mark>

<img width="860" alt="pase" src="https://user-images.githubusercontent.com/48202736/107185070-13981400-6a25-11eb-9f4f-ae88cab4e277.png">


- <mark style='background-color: #dcffe4'> 2020, Wav2Vec 2.0 </mark>

<img width="965" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/107185076-14c94100-6a25-11eb-8001-ae5f39f2e909.png">



- <mark style='background-color: #dcffe4'> 2021, LEAF </mark>

<img width="1234" alt="leaf1" src="https://user-images.githubusercontent.com/48202736/107185057-11ce5080-6a25-11eb-8bb5-2d5ecd524b7c.png">
<img width="1243" alt="leaf2" src="https://user-images.githubusercontent.com/48202736/107185059-11ce5080-6a25-11eb-8262-0109331103c4.png">
<img width="965" alt="leaf3" src="https://user-images.githubusercontent.com/48202736/107185060-1266e700-6a25-11eb-9c9f-139822038300.png">
<img width="1250" alt="leaf4" src="https://user-images.githubusercontent.com/48202736/107185062-12ff7d80-6a25-11eb-9a20-f33e6b373cc2.png">
<img width="874" alt="leaf5" src="https://user-images.githubusercontent.com/48202736/107185063-12ff7d80-6a25-11eb-887d-112f500a0756.png">
<img width="1073" alt="leaf6" src="https://user-images.githubusercontent.com/48202736/107185067-13981400-6a25-11eb-9317-5967aacdee0b.png">

- <mark style='background-color: #fff5b1'> References </mark>

- 1.[Zeghidour, Neil, Olivier Teboul, Félix de Chaumont Quitry, and Marco Tagliasacchi. "LEAF: A Learnable Frontend for Audio Classification." arXiv preprint arXiv:2101.08596 (2021).](https://arxiv.org/pdf/2101.08596)

- 2.[Ravanelli, Mirco, and Yoshua Bengio. "Interpretable convolutional filters with sincnet." arXiv preprint arXiv:1811.09725 (2018).](https://arxiv.org/pdf/1811.09725)

- 3.[Ravanelli, Mirco, and Yoshua Bengio. "Speech and speaker recognition from raw waveform with sincnet." arXiv preprint arXiv:1812.05920 (2018).](https://arxiv.org/pdf/1812.05920)

- 4.[Parcollet, Titouan, Mohamed Morchid, and Georges Linares. "E2E-SINCNET: Toward fully end-to-end speech recognition." In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7714-7718. IEEE, 2020.](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9053954&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkwNTM5NTQ/Y2FzYV90b2tlbj1BYmw5NTRvTWxxNEFBQUFBOjNQTzRkTHBLQUpJZlFzeGdmS3RZNkhkT2VMQTgzVXl3Z01lMjc4WWpWSkZ2VGt2b3ZOdEtTYU9RWjIyU0FiczVfd25vRkpZejBfcjF3QQ==)

- 5.[Sainath, Tara N., Brian Kingsbury, Abdel-rahman Mohamed, and Bhuvana Ramabhadran. "Learning filter banks within a deep neural network framework." In 2013 IEEE workshop on automatic speech recognition and understanding, pp. 297-302. IEEE, 2013.](https://ieeexplore.ieee.org/abstract/document/6707746?casa_token=HdyKapVCkUoAAAAA:_dykPJHmordtN-eDg0C8-5B7By8DodG4TZAZG7NkE6EJ6Qii1eqO06XkJAtFLJIhTPBTYqC2OQKHlQ)

- 6.[Sainath, Tara N., Ron J. Weiss, Andrew Senior, Kevin W. Wilson, and Oriol Vinyals. "Learning the speech front-end with raw waveform CLDNNs." In Sixteenth Annual Conference of the International Speech Communication Association. 2015.](https://research.google.com/pubs/archive/43960.pdf)

- 7.[Hoshen, Yedid, Ron J. Weiss, and Kevin W. Wilson. "Speech acoustic modeling from raw multichannel waveforms." In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4624-4628. IEEE, 2015.](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=7178847&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzcxNzg4NDc/Y2FzYV90b2tlbj1XUGhkZGNXektYWUFBQUFBOm9fSEVuZWhCYzhFUkl5bVlyTzAyR1Nidm9malZQNGstVWE4a1VvTVJJckY4R21aTGpjMUU5RmtvQzN3ckM5OWo5MG5SRDBmeVNLYjhyUQ==)

- 8.[Zeghidour, Neil, Nicolas Usunier, Iasonas Kokkinos, Thomas Schaiz, Gabriel Synnaeve, and Emmanuel Dupoux. "Learning filterbanks from raw speech for phone recognition." In 2018 IEEE international conference on acoustics, speech and signal Processing (ICASSP), pp. 5509-5513. IEEE, 2018.](https://arxiv.org/pdf/1711.01161)

- 9.[Paul-Gauthier Noe ́, Titouan Parcollet, and Mohamed Morchid. Cgcnn: Complex gabor convolu- tional neural network on raw speech. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7724–7728, 2020.](https://arxiv.org/pdf/2002.04569)


- 10.[Mirco's Slides for SincNet](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.clsp.jhu.edu%2Fwp-content%2Fuploads%2F2019%2F06%2FRavanelli_JSALT_Presentation.pdf&psig=AOvVaw1MR5MSW8jWulIyrfqZ88PD&ust=1612872508035000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCMC727mg2u4CFQAAAAAdAAAAABAD)
