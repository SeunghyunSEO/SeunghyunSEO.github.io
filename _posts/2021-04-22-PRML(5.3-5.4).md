---
title: 5.3 - 5.4 Error Backpropagation and Hessian Matrix
categories: Brief_Review_for_PRML
tag: [PRML,MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---

***

시작하기에 앞서 이 글은 유명한 머신러닝 서적 중 하나인 [Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/) 과 이 책을 요약한 [홍기호 님](https://github.com/norman3)의 [PRML 요약 정리 post](http://norman3.github.io/prml/)를 
조금 더 간략하게 요약하고 추가설명을 덧붙힌 글 임을 밝힙니다.

(공개용은 아니고 혼자 공부하기 위한 용도)

***

이번 5.3 ~ 5.4장의 핵심 내용을 정리하면 아래와 같습니다.

- 네트워크를 학습하기 위해 미분 가능한 모든 파라메터의 손실 함수에 대한 민감도(미분값)을 구해내야 한다. 하지만 손실 함수를 모든 파라메터 대해서 일일히 미분을 하는것은 비효율적이며 이를 해결하기 위해 제안된 연쇄 미분 법(chain rule)을 사용하는 오차 역전파에 대해 알아본다.
- 오차 역전파 (Error Backpropagation) 요약
    - 입력 벡터 \x_n\ 을 네트워크에 적용해 앞으로 전파시킨 후(매트릭스 곱 연산, \\( a_j=\sum_i w_{ji} z_i\, z_j=h(a_j) \\) ), 모든 은닉 유닛과 출력 유닛의 활성도를 구한다.
    - 식 \\( \delta_k=y_k-t_k \\)을 이용해 모든 출력 유닛의 \\delta_k\ 를 구한다.
    - 식 \\( \delta_k=h'(a_j)\sum_k w_{kj}\delta_k \\)을 이용해 \\delta\ 값들을 역전파 시킨다. 이를 통해 네트워크상의 은닉 유닛들의 \\delta_j\값을 구한다.
    - 식 \\( \frac{\partial{E}}{\partial{w_{ji}}} = \frac{\partial{E_n}}{\partial{a_{j}}} \frac{\partial{a_j}}{\partial{w_{ji}}} \\)을 이용해 필요한 미분을 계산한다.  


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> 5.3 오차 역전파 (Error Backpropagation) </mark>

asd

## <mark style='background-color: #fff5b1'> 5.4 헤시안 행렬 (Hessian Matrix) </mark>

asd

### <mark style='background-color: #dcffe4'> Notation </mark>

asd




- 이를 에러 역전파 알고리즘(*backpropagation*) 이라고 부른다. 
    - 혹은 줄여서 *backprop* 이라고 한다.
    - '역전파' 라는 용어보다는 원래 표현인 backprop 표현을 주로 사용하도록 하겠다.
- 사실 신경망에서는 backpropagation 이라는 용어가 좀 애매하게 사용되기도 한다.
    - 예를 들어 **MLP** 자체를 backprop망 이라 부른다.
    - 혹은 **MLP** 에서 sum-of-square 에러 함수를 최소화하기 위해 사용되는 그라디언트 방식을 backprop이라고도 한다.
    - 여기서는 이런 혼동을 피하기 위해 학습 과정을 자세히 살펴보도록 한다.
- 일반적인 학습 과정은 에러 함수를 최소화하기 위한 반복 과정을 포함하고 있다.
    - 반복 중에 가중치가 계속 갱신된다.
- 하나의 작업이 여러 번 반복되는 것으로 이해될 수 있는데, 이 때 하나의 작업은 크게 두 개의 단계로 나누어진다.
    - 첫번째 단계는 \\( {\bf w} \\) 에 대한 에러 함수의 미분 값을 구하는 것이다.
        - 이 때 backprop 은 그라디언트 값을 구하는 효율적인 도구를 의미한다.
        - 이후에 언급되겠지만 그라디언트를 구하는 기법은 야코비안(jacobian), 헤시안(Hessian) 을 구하는 계산에서도 응용될 수 있다.
    - 두번째 단계는 그라디언트를 이용해서 \\( {\bf w} \\) 의 업데이트 분량을 계산하는 과정이다.
        - 이 때 그라디언트 감소(gradient descent) 기법이 가장 유명한 기법이고 기타 다른 기법도 존재한다.
- 이 두 단계를 구분하는 것은 매우 중요하다.


## 5.3.1. 에러 함수의 미분 값을 평가하기 (Evaluation of error-function derivatives)
- backprop 알고리즘은 다음의 조건에서 쉽게 사용 가능하다.
    - 임의의 feed-forward 신경망
    - 임의의 미분 가능한 비선형 활성 함수
    - 다양한 종류의 에러 함수
- 가장 먼저 간단한 모델을 이용하여 식을 전해할 것이다.
    - 시그모이드 형태의 히든 레이어를 가지는 단일 레이어 모델 (용어가 이상한데 그냥 2-layer 모델이다.)
    - 이 때 사용되는 에러 함수는 sum-of-square 모델
- 현실에서는 정말 다양한 에러 함수를 정의할 수 있다.
    - 예를 들어 i.i.d 데이터를 이용한 MLE 함수를 사용할 수 있다.
    - 이 때 에러 함수는 모든 관찰 데이터의 에러의 합으로 표현 가능하다.

$$E({\bf w}) = \sum_{n=1}^{N}E_N({\bf w}) \qquad{(5.44)}$$

- 로그 가능도 함수가 각 관찰 데이터의 로그 가능도 함수 값의 합인것 처럼 에러 함수로 이와 비슷하게 정의된다.
- 이제 이 함수의 \\( \triangledown E({\bf w}) \\) 에 대해 고민을 하도록 하자.
- 이런 방식을 배치(batch) 모드라고 한다. (에러를 모두 합해서 한번에 사용)
- 이후에 온라인 업데이트 방식도 살펴볼 것이다.

- 이제 \\( y_k \\) 에 대한 간단한 선형 모델을 살펴보자.

$$y_k = \sum_i w_{ki}x_i \qquad{(5.45)}$$

- 위의 식은 사실 신경망과는 아무런 관계가 없다. 3장에서 살펴본 선형 함수라고 생각하면 된다.
- 다만 출력 값 \\( y\_k \\) 가 하나가 아니라 \\( K \\) 개이고 각각 독립적으로 계산된다.

- 이 때 sum-of-square 에러 함수는 다음과 같다.

$$E_n = \frac{1}{2}\sum_k (y_{nk}-t_{nk})^2 \qquad{(5.46)}$$

- 하나의 샘플에 대해 출력값이 \\( K \\) 개이므로 이에 대한 에러의 합으로 표현된다.
- 이 때 \\( y\_{nk}=y\_k({\bf x}\_n, {\bf w}) \\) 가 된다.

- 이를 \\( {\bf w}\_{ji} \\) 에 대해 미분해보자.
    - 갑자기 \\( j \\) 가 등장해서 이상할 수 있으나, 
    - 앞서 사용한 \\( k \\) 는 임의의 **특정** \\( k \\) 를 의미하고
    - \\( j \\) 는 말 그대로 임의의 출력 노드를 의미한다.
    - 전체 \\( {\bf w} \\) 벡터에 대한 미분이 아니라 \\( w\_{ji} \\) 에 대한 미분임에 주의할 것.
    
$$\dfrac{\partial E_n}{\partial w_{ji}} = (y_{nj}-t_{nj})x_{ni} \qquad{(5.47)}$$

- 이건 4장에서 보던 식과 비슷한 형태이다.
    - 이 식은 회귀, 이진 분류, 다변수 분류와 같은 목적에 상관이 없이 얻을 수 있는 일반화된 식이다.
    - 일반적인 feed-forward 네트워크에서 입력 유닛에 대한 가중치 합은 다음과 같다.

$$a_j = \sum_i w_{ji}z_i \qquad{(5.48)}$$


- 이미 5.1절에서 bias 라로 불리우는 추가 유닛의 존재를 확인했다. 활성 함수에 이 값이 +1 로 고정 추가된다.
- 여기서 \\( z\_i \\) 는 이전에 연결된 레이어의 유닛 출력값이거나 비선형 활성 함수를 통해 얻어진 결과 값이다.

$$z_j=h(a_j) \qquad{(5.49)}$$

- 하나 이상의 \\( z\_i \\) 가 존재하고 이는 식 (5.48)에서 기술한 입력 값의 합을 입력으로 받고 \\( j \\) 번째의 값을 출력하게 된다.
- 전방 전파 (forward propagation)
    - 학습 데이터가 입력되면 입력 벡터로 들어와서 히든 레이어의 활성 함수에 의해 계산된 값이 전파되어 최종 출력에 이르게 된다.
    - 이러한 과정을 전방 전파라고 한다.
- 이제 에러 함수 \\( E\_n \\) 을 파라미터 \\( w\_{ji} \\) 로 미분한 식을 살펴보도록 하자.
    - 최종 출력값은 입력 패턴 \\( n \\) 에 영향을 받는다. 
    - 하지만 여기서 식에 이걸 다 기입하면 복잡하니까 잠시 \\( n \\) 은 생략하고 보도록 하자. 
- 최종 에러값은 두 번의 함수를 거쳐 출력되므로 이를 나누어서 고려할 수 있다.

$$\frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_{j}}\cdot\frac{\partial a_j}{\partial w_{ji}} \qquad{(5.50)}$$

- 식이 점점 복잡해지므로 간단한 표기법을 도입하도록 한다.

$$\delta_j = \frac{\partial E_n}{\partial a_j} \qquad{(5.51)}$$

- \\( \delta\_j \\) 를 종종 에러처럼 표현하곤 한다. 
- 식 (5.48)을 편미분하여 다음과 같은 식을 얻을 수 있다.

$$\frac{\partial a_j}{\partial w_{ji}} = z_i \qquad{(5.52)}$$

- 미분값은 그냥 출력 값이 된다. 이제 식을 결합하자.

$$\frac{\partial E_n}{\partial w_{ji}} = \delta_j z_i \qquad{(5.53)}$$

- \\( w\_{ji} \\) 에 대한 미분값은 각 유닛마다 얻어지는 에러 \\( \delta\_j \\) 에 출력값 \\( z\_i \\) 를 곱하여 얻을 수 있다.
    - 따라서 실제 미분 값을 구하기 위해서는 각 유닛마다 에러 \\( \delta\_j \\) 만 계산하기만 하면 된다.

- 최종 출력단에서의 에러 값은 이미 구했었다. 
    - 4.3.6 절에서 언급했듯이 이를 정준 링크 함수(canonical link function)라고 생각하면 다음을 얻을 수 있다.

$$\delta_k = y_k - t_k \qquad{(5.54)}$$

- 히든 유닛의 에러 값은 체인 법칙(chain rule) 을 이용하여 전개 가능하다.

$$\delta_j = h'(a_j)\sum_k w_{kj}\delta_k \qquad{(5.56)}$$

- 이 식을 자세히 보면 히든 유닛의 \\( \delta \\) 값은 네트워크에서 전파된 쪽 노드의 \\( \delta \\) 값을 되받아(backward) 얻을 수 있다는 것을 알 수 있다.
    - 이를 그림으로 나타내면 다음과 같다.

![Fig5.7](/assets/images/PRML_5.3_to_5.4/Fig5.7.png)
*Fig. 5.7*


- 이를 **backpropagation** 이라고 부른다.

-----

- **Error Backpropagation**
    1. 입력 벡터 \\( {\bf x}\_n \\) 에대해 식 (5.48) 과 식 (5.49)을 이용한 전방향 전파를 진행한다.
    2. 출력 유닛에서 \\( \delta\_k \\) 값을 구한다. 식 (5.54) 를 사용한다.
    3. 히든 유닛의 \\( \delta\_j \\) 값을 구하기 위해 역전파를 진행한다. 식 (5.56)을 사용하면 된다.
    4. 식 (5.53)을 이용하여 미분 값을 구한다.

-----

- 배치 모드(batch mode)에서는 에러 \\( E \\) 를 모든 테스트 샘플의 에러 합으로 사용하면 된다.

$$\frac{\partial E}{\partial w_{ji}} = \sum_n \frac{\partial E_n}{\partial w_{ji}} \qquad{(5.67)}$$

- 사실 위의 식은 활성 함수 \\( h(\cdot) \\) 이 모두 동일하다고 가정하고 작성한 식이다.
    - 하지만 사실 각 유닛들이 서로 다른 활성 함수를 사용할 수도 있다.
    - 이 경우 개별적인 활성 함수를 식에 대입하여 사용하면 된다.
  
## 5.3.2 간단한 예제 (A simple example)

- 앞서 살펴본 역전파 방식은 에러 함수, 활성함수, 그리고 네크워크 구성을 일반화한 형태의 식이다
    - 이러한 알고리즘을 활용한 일반적인 응용을 살펴보기 위해서 여기서는 아주 간단한 형태의 예를 도입하도록 한다.
    - 아주 간단하면서도 실제 사용 가능한 수준의 신경망 예제를 만들어 보자.
    - 일단 그림 (5.1) 과 같이 2개의 레이어(layer)를 가지는 네트워크를 고려해보자.
        - 이 때 에러 함수는 최소 제곱합(sum-of-square)을 사용하고 선형 활성 함수를 이용한다.
            - 사용되는 활성 함수의 식은 다음과 같다.
        
$$h(a) = \tanh(a) \qquad{(5.58)}$$

- \\( \tanh(\cdot) \\) 함수는 다음과 같다.

$$\tanh(a) = \frac{e^a-e^{-a}}{e^a+e^{-a}} \qquad{(5.59)}$$

- 활성 함수를 미분한 식도 살펴보자.

$$h'(a) = 1-h(a)^2 \qquad{(5.60)}$$

- 기본적인 최소 제곱합 에러 함수는 다음과 같다.

$$E_n = \frac{1}{2}\sum_{k=1}^{K}(y_k-t_k)^2 \qquad{(5.61)}$$

- 출력값 \\( y_k \\) 는 출력 유닛 \\( k \\) 에서의 출력값이 된다. \\( t_k \\) 는 이에 대응되는 타겟 값을 의미한다.

- 이제 네트워크 구성을 위한 식들을 간단하게 정리해보자.

$$a_j = \sum_{i=0}^{D}w_{ji}^{(1)}x_i \qquad{(5.62)}$$

$$z_j = \tanh(a_j) \qquad{(5.63)}$$

$$y_k = \sum_{j=0}^{M}w_{kj}^{(2)}z_j \qquad{(5.64)}$$

- 출력 유닛에서의 \\( \delta \\) 함수 값은 다음과 같다.

$$\delta_k = y_k-t_k \qquad{(5.65)}$$

- 이제 역전파를 위한 히든 유닛에서의 \\( \delta \\) 값을 살펴보자.

$$\delta_j = (1-z_j^2)\sum_{k=1}^{K} w_{kj}\delta_k \qquad{(5.66)}$$

- 이 식들을 조합하여 첫번째 레이어와 두번째 레이어의 가중치 값의 미분 값을 구하면 다음과 같다.

$$\frac{\partial E_n}{\partial w_{ji}^{(1)}} = \delta_j x_i \qquad{(5.67)}$$

$$\frac{\partial E_n}{\partial w_{kj}^{(2)}} = \delta_k z_j \qquad{(5.67)}$$


## 5.3.3 역전파 알고리즘의 효율 (Efficiency of backpropagation)

- 역전파에서 가장 중요한 것 중 하나로 계산의 효율성을 들 수 있다.
- 이 문제를 이해하기 위해서 실제 에러 함수를 미분하여 평가하는 작업이 얼마나 많은 컴퓨터 연산을 발생시키는지 확인해보도록 하자.
    - 이 때 네트워크에서 구해야 하는 가중치 \\( W \\) 의 수를 함께 고려한다.
- 에러 함수를 평가하는 과정에서는 매우 많은 갯수의 파라미터 \\( W \\) 에 대해 \\( O(W) \\) 만큼의 연산이 필요하게 된다.
- 그리고 파라미터의 개수는 유닛의 수보다 크다. (물론 sparse 모델은 이렇지 않을수도 있겠지만 흔한 모델은 아니다.)
    - 따라서 식 (5.48)에 의해 전방향 전파는 \\( O(W) \\) 의 계산 비용이 필요한다.
- 반면 역전파의 경우에는 미분을 위해 유한 차분(finite difference) 기법을 도입해다고 생각하고 이에 대한 비용을 근사적으로 예측해보자.

$$\frac{\partial E_n}{\partial w_{ji}} = \frac{E_n(w_{ji}+e)-E_n(w_{ji})}{e} + O(e) \qquad{(5.68)}$$

- 보통 \\( e << 1 \\) 이다. 정확도를 향상시키려면 \\( e \\) 는 매우 작은 값이어야 한다.
- (참고) **finite difference method**
    - 유한 차분법은 \\( f(x+b)-f(x+a) \\) 형태의 수학 식을 의미한다.
    - 그리고 이를 \\( (b-a) \\) 값으로 나누게 되면 *difference quotient* 를 얻는다.
    - 이 때 \\( \|b-a\| \\) 가 0에 가까워지면 이를 미분 근사로 생각할 수 있다.
    - 따라서 *finite difference method* 는 수치적 미분 방식을 계산하기 위한 도구이다.

- 수치적인 미분식을 사용하는 경우에는 역전파의 계산 비용이 \\( O(W) \\) 로는 불가능하다.
- 정확도를 높이기 위해 *symmetrical central differences* 를 사용할 수도 있다.

$$\frac{\partial E_n}{\partial w_{ji}} = \frac{E_n(w_{ji}+e)-E_n(w_{ji}-e)}{2e} + O(e^2) \qquad{(5.69)}$$

- 참고로 위의 식은 테일러 급수를 이용하여 얻어낸 식이다.
    - 전개를 해 보면 마지막 차항이 \\( O(e) \\) 가 아닌 \\( O(e^2) \\) 으로 나오는지 그 이유를 알 수 있다.
    - 따라서 전방 전파시에는 \\( O(W) \\) 이고 역전파에서는 \\( O(W^2) \\) 의 비용이 소요된다.
- 실제로 이러한 수학적 미분 방식을 통한 계산 식이 매우 중요한데, 다른 방식을 이용하여 연전파를 구한 값과 비교를 할 수 있는 대상으로 사용할 수 있기 때문이다.
    - 따라서 다른 방식으로 얻어지는 결과가 올바른지를 비교해 볼 수 있는 방법으로 응용이 가능하다.

## 5.3.4 야코비안 행렬 (The Jacobian matrix)

- 지금까지 신경망에서의 전파, 역전파 방식에 대해 살펴보았다.
- 이 때 역전파 방식에서 에러 함수의 미분을 통한 갱신 방식을 사용하는 것도 확인하였다.
- 여기서 사용된 역전파 계산 방식은 이와 유사한 다른 미분 식에서도 응용 가능하다.
- 이 중에서 야코비안 행렬 (jacobian matrix)을 계산하는데에도 역전파 전개 방식을 도입할 수 있다.
- 이번 절은 좀 뜬금 없지만 야코비안 행렬 계산을 위한 역전파 기법을 살펴보는 절이라고 생각하면 된다.
    - 이 행렬의 정의는 출력 값을 입력 값으로 미분한 식으로 표현된다.
  
$$J_{ki} = \frac{\partial y_k}{\partial x_i} \qquad{(5.70)}$$

- 야코비안 행렬은 백터를 출력하는 (즉, 출력 값이 하나의 실수 값이 아니라 벡터이다.) 1차 미분 행렬이다.
    - 만약 출력 값이 하나의 경우에는 야코비안 행렬은 하나의 행(row) 만 가지게 된다. 
        - 이러한 경우는 우리가 알고 있는 일반 미분식이다.
    - 따라서 아코비안 행렬이라고 하는 것은 출력 값이 여러 개인 함수에 대한 편미분을 일반화한 행렬이라고 생각하면 된다.
- 야코비안 행렬은 독립된 모듈의 형태로 시스템을 구축할 때 유용하게 사용될 수 있다.

![Fig5.8](/assets/images/PRML_5.3_to_5.4/Fig5.8.png)
*Fig. 5.8*

- 위의 그림에서 박스는 고정된 모듈을 의미한다.
- 여기서 \\( E \\) 에 대한 \\( w \\) 에 대한 값을 계산하기 위해 다음의 식을 사용한다.

$$\frac{\partial E}{\partial w} = \sum_{k, j} \frac{\partial E}{\partial y_k}\frac{\partial y_k}{\partial z_j}\frac{\partial z_j}{\partial w} \qquad{(5.71)}$$

- 이 중 가운데 텀이 야코비안 행렬이 된다.
    - 아코비안 행렬은 입력에 대한 출력의 국지적인 민감도(local sensitivity)를 측정하는 용도로 사용 가능하다.

$$\Delta y_k \simeq \sum_i \frac{\partial y_k}{\partial x_i}\Delta x_i \qquad{(5.72)}$$

- 여기서 \\( \|\Delta x\_i\| \\) 는 매우 작은 값이다.
- 일반적으로 학습된 신경망에서 입력과 출력 사이에 비선형 함수가 존재하기 때문에 \\( J\_{ki} \\) 는 상수값이 되지 않는다.
    - 따라서 실제 값은 입력 값에 영향을 받게 된다.
    - 이 말은 입력 데이터가 들어올 때마다 야코비안 행렬도 다시 계산이 되어야 한다는 의미이다.
- 야코비안 행렬의 계산과 신경망의 역전파 방식은 매우 유사하게 전개될 수 있다.

$$J_{ki} = \frac{\partial y_k}{\partial x_i} = \sum_j \frac{\partial y_k}{\partial a_j} \frac{\partial a_j}{\partial x_i} = \sum_j w_{ji} \frac{\partial y_k}{\partial a_j} \qquad{(5.73)}$$

- 크게 중요한 내용들도 아니니 식만 좀 살펴보자.

$$\frac{\partial y_k}{\partial a_j} = \sum_l {\frac{\partial y_k}{\partial a_l}\frac{\partial a_l}{\partial a_j} } = h'(a_j)\sum_l w_{lj}\frac{\partial y_k}{\partial a_l} \qquad{(5.74)}$$

$$\frac{\partial y_k}{\partial a_l} = \delta_{kl} \sigma '(a_l) \qquad{(5.75)}$$

$$\frac{\partial y_k}{\partial a_l} = \delta_{kl} y_k - y_k y_l \qquad{(5.76)}$$

- 야코비안에서도 마찬가지로 수치 미분을 이용하여 실제 행렬이 정확히 구해졌는지 확인할 수 있다.

$$\frac{\partial y_k}{\partial x_i} = \frac{y_k(x_i+e) - y_k(x_i-e)}{2e} + O(e^2) \qquad{(5.77)}$$













- 지금까지 우리는 backprop 기법을 이용하여 에러 함수에 대한 \\( {\bf w} \\) 1차 미분값을 계산할 수 있었다.
- 이를 확장하여 2차 미분값인 Hessian 행렬을 계산하는 방법을 살펴보도록 하자.

$$\frac{\partial^2 E}{\partial w_{ji} \partial w_{lk} } \qquad{(5.78)}$$

- 여기서 \\( i, j \in \\\{ 1, ..., W \\\} \\) 이고 \\( W \\) 는 모든 weight 와 bias를 포함한다.
- 이 때 각각의 2차 미분 값을 \\( H_{ij} \\) 로 표기하고 이것으로 만들어지는 행렬을 헤시안(Hessian) 행렬 \\( {\bf H} \\) 라고 정의한다.
- 신경망에서 헤시안 행렬은 중요하게 여겨진다.
    - 일부 비선형 최적화 알고리즘에서 에러 곡면의 2차 미분 값을 사용한다.
    - 이미 학습이 완료된 신경망에 조금 변경된 데이터를 입력으로 주고 빠르게 재학습 시킬 때 사용된다.
    - 'pruning' 알고리즘의 일부로 *least significant weights* 를 식별할 때 헤시안 역행렬이 사용된다.
    - 베이지안 신경망을 위한 라플라스 근사식에서 중요한 역할을 차지한다. (5.7절 참고)
    - 설명이 너무 어려워서 이해가 안된다면 그냥 backprop 업데이트시 다양한 형태의 방식을 사용할 수 있고 이 때 헤시안 행렬을 이용하연 좋은 결과를 얻을 수 있다고만 이해하고 넘어가자.
- 헤시안 행렬의 연산량은 매우 높다.
    - 신경망에 \\( W \\) 개의 weight 가 존재한다면 헤시안은 \\( W \times W \\) 행렬이 된다.
    - 따라서 연산량은 입력 샘플당 \\( O(W^2) \\) 이 필요하다.

### 5.4.1. 대각 근사 ( Diagonal approximation)

- 헤시안 행렬을 사용하는 많은 경우에서 헤시안 행렬의 역행렬(inverse)을 이용하는 경우가 많다.
- 이 때 정방 행렬인 헤시안 행렬의 역행렬이 존재하려면 (즉, invertible or nonsingular) \\(det({\bf H})\\) 의 값이 \\(0\\) 이 아니어야 한다.
- 이 때 헤시안 행렬을 대각 근사하면 \\(det({\bf H})\\) 가 \\(0\\) 인 경우가 발생하지 않는다.
    - 즉, 대각만 남기고 다른 값들을 모두 0으로 치환
    - 대각 행렬의 경우 역행렬을 구할 수 있음이 보장된다.

$$\frac{\partial^2 E_n}{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_j}z^2_i \qquad{(5.79)}$$

- 참고로 위의 식은 다음과 같이 전개하여 얻은 식이다.

$$\frac{\partial^2 E_n}{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_{j}} \frac{\partial a^2_{j} }{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_j}z^2_i$$


- 식(5.48)과 식(5.49)를 사용하여 식(5.79)의 오른쪽 항을 체인(chain) 형태로 전개할 수 있다.

$$\frac{\partial^2 E_n}{\partial a^2_j} = h'(a_j)^2\sum_k\sum_{k'} w_{kj}w_{k'j}\frac{\partial^2 E_n}{\partial{a_k} \partial{a_{k'}} } + h''(a_j)\sum_k w_{kj}\frac{\partial E_n}{\partial a_k} \qquad{(5.80)}$$

- 참고로 \\(a_k\\) 와 \\(a_{k'}\\) 는 동일한 레이어의 유닛을 의미한다. \\(k\\) 와 \\(k'\\)가 다른 경우를 모두 무시하면 다음 식을 얻게 된다.
    - 즉, 대각만 남기고 다른 요소들은 모두 삭제한다.
    - *Becker & Le Cun* (1989)

$$\frac{\partial^2 E_n}{\partial a^2_j} = h'(a_j)^2\sum_k w^2_{kj}\frac{\partial^2 E_n}{a^2_k} + h''(a_j)\sum_k w_{kj}\frac{\partial E_n}{\partial a_k} \qquad{(5.81)}$$

- 이 때의 연산 비용은 \\(O(W)\\) 가 된다. (실제 헤시안 행렬은 \\(O(W^2)\\)임을 이미 확인했다.)
- 게다가 연산도 쉬운데 backprop 에서 얻어진 에러 \\( \delta \\) 를 이용하여 \\( \frac {\partial^2 E\_n }{\partial a^2\_j} \\) 만 구하면 헤시한 행렬을 구할 수 있다.
- 하지만 현실적으로 헤시안 행렬 자체는 대각행렬만으로 구성되는 경우는 거의 없다.
    - 따라서 헤시안 행렬의 대각 근사 자체를 잘 사용하지 않는다. (왜 다룬거야!)

### 5.4.2. 외적 근사 (Outer product approximation)

- 회귀(regression) 문제로 돌아가 에러 함수를 잠시 꺼내와보자.

$$E = \frac{1}{2} \sum_{n=1}^N (y_n - t_n)^2 \qquad{(5.82)}$$

- 이에 대한 헤시안 행렬은 다음과 같이 구할 수 있다.

$${\bf H} = \nabla \nabla E = \sum_{n=1}^N \nabla y_n (\nabla y_n)^T + \sum_{n=1}^N(y_n - t_n)\nabla\nabla y_n \qquad{(5.83)}$$

- 만약 충분한 데이터로 학습이 잘 이루어져서 네크워크 망의 출력값 \\(y_n\\) 가 \\(t_n\\) 와 매우 비슷한 값을 내어준다면 위의 식에서 2번째 텀은 생략 가능하다.
    - 물론 제대로 하려면 섹션 1.5.5 에서 다루었듯 출력 값의 조건부 평균을 사용해야 겠지만 일단 넘어가자.
    - 식(5.83)에서 두번째 텀을 생략한 식을 *Levenberg-Marquardt* 근사 또는 외적 근사(outer product approx.)라고 부른다.
    - 사실 헤시안 행렬 자체가 외적의 합으로 이루어진 행렬이다.

$${\bf H} \simeq \sum_{n=1}^N {\bf b_n}{\bf b_n}^T \qquad{(5.84)}$$

- 여기서 \\({\bf b_n} \equiv \nabla a\_n = y\_n\\) 이다.
    - 회귀 문제에서는 마지막 레이어의 활성(activation) 함수가 Identity 함수이기 때문이다.

- 이 근사법은 헤시안을 구하기 위해 오로지 1차 미분만을 요구하고 backprop 시 \\(O(W)\\) 에 값을 구할 수 있다.
- 그리고 \\(W\\) 차원의 두 벡터를 외적할 때 \\( O(W^2) \\) 이 요구된다.

- 로지스틱 시그모이드(logistic sigmoid) 활성 함수를 가지는 크로스 엔트로피(cross-entropy)를  사용하는 경우 근사식은 다음과 같이 된다.

$${\bf H} \simeq \sum_{n=1}^N y_n(1-y_n){\bf b_n}{\bf b_n}^T \qquad{(5.85)}$$

- 다중부류(multi-class)로도 쉽게 확장된다.

### 5.4.3. 헤시안 역행렬 (Inverse Hessian)

- 헤시안을 응용하는 곳에서는 주로 헤시안 역행렬을 사용하는 경우가 많다. (물론 신경망에서도!!!)
- 앞서 살펴보았던 외적 근사 기법을 통해 헤시안의 역행렬을 구하는 방법을 살펴볼 것이다.
- 끝에 나오는 quasi-Newton 을 주의깊게 살펴보자.

$${\bf H}_N = \sum_{n=1}^N {\bf b_n}{\bf b_n}^T \qquad{(5.86)}$$

- 여기서 \\( {\bf b_n} \equiv \nabla a\_n \\) 이다.
- 각 데이터 n 으로부터 얻어지는 활성 함수의 출력값을 미분하여 얻을 수 있다.
- 이제 이 값을 하나씩 갱신해나갈 수 있는 식을 작성해보자. (점진적 구축 방식)

$${\bf H_{L+1} } = {\bf H}_L + {\bf b}_{L+1}{\bf b}_{L+1}^T \qquad{(5.86)}$$

- 역 헤시안을 구하기 위해 이미 알려진 식을 사용한다.

$$({\bf M} + {\bf vv}^T)^{-1} = {\bf M}^{-1} - \frac{({\bf M}^{-1}{\bf v})({\bf v}^T {\bf M}^{-1})}{1+{\bf v}^T {\bf M}^{-1} {\bf v}} \qquad{(5.87)}$$

- 아래 식은 *Woodbury identy* 라는 식이다. (Appendix. C.7을 참고하자)

$$(A+BD^{-1}C)^{-1} = A^{-1} - A^{-1}B(D+CA^{-1}B)^{-1}CA^{-1} \qquad{(C.7)}$$

- 이 식을 이용하여 헤시안 역행렬을 구할 수 있다.

$${\bf H}_{L+1}^{-1} = {\bf H}_{L}^{-1} - \frac{ {\bf H}_{L}^{-1} {\bf b}_{L+1} {\bf b}_{L+1}^{T} {\bf H}_{L}^{-1} }{ 1 + {\bf b}_{L+1}^{T} {\bf H}_{L}^{-1} {\bf b}_{L+1} }  \qquad{(5.89)}$$


- 이제 식(5.87)과 식(5.88)을 활용해서 헤시안 역행렬을 전진적인 방식(incremental)으로 근사 할 수 있다.
- 참고로 *Woodbury identity* 에 대응되는 식은 다음과 같다.

$${\bf b}_{L+1} = {\bf v}$$

$${\bf H}_{L} = {\bf M}$$

$${\bf H}_{L+1} = {\bf M} + {\bf v}{\bf v}^T$$

### 5.4.4 유한 차분법 (Finite differences)

- 에러 함수를 한번 미분하는 방식과 마찬가지로 두번 미분해서 결과를 얻어낼 수 있다.
- 이건 이미 앞에서 한번 살펴본 방식이다.
- 다시 설명할 필요는 없을 듯 하고 식을 우선 보자.

$$\frac{\partial^2 E}{\partial w_{ji} w_{lk} } = \frac{1}{4e^2} \left\{ E(w_{ji} + e, w_{lk} + e) - E(w_{ji} + e, w_{lk}-e) \\ - E(w_{ji}-e, w_{lk}+e) + E(w_{ji}-e, w_{lk}-e)\right\} + O(e^2)\qquad{(5.90)}$$

- *symmetric* 미분 방식으로 인해서 \\( O(e) \\) 대신 \\( O(e^2) \\) 를 사용하게 된다. (앞서 살펴보았다.)
- 문제는 실제 연산량이 \\( O(W^3) \\) 이라는 것이다.
    - 따라서 이 방식을 도입하기가 현실적으로 어려운 점이 있다.
    - 대응 방법으로 다음과 같은 방식의 식을 사용할 수도 있다.

$$\frac{\partial^2 E}{\partial w_{ji} \partial w_{lk}} = \frac{1}{2}\left\{ \frac{\partial E}{\partial w_{ji}}(w_{lk}+e) - \frac{\partial E}{\partial w_{ji}}(w_{lk}-e) \right\} \qquad{(5.91)}$$

- 이 식을 사용하면 \\( O(W^2) \\) 으로 연산이 가능하다.
- 방식은 그냥 \\( \frac{\partial E}{\partial w_{ji}} \\) 를 함수로 생각하고 *central* 미분 방식을 도입. (식 (5.69)를 참고하자.)

### 5.4.5 제대로 헤시안 행렬을 구해보기 (Extract evaluation of the Hessian)

- 지금까지는 헤시안 행렬을 근사적 방식으로 얻어내는 것을 소개했다.
- 제대로 헤시안을 구하는 방법도 고려해보자.
    - 놀랍게도 backprop 방식을 확장해서 \\( O(W^2) \\) 에 이 계산을 수행할 수 있다.
    - 이를 확인해보기 위해 앞서 설명했던 내용들을 간단히 확장해보자.
- 간단하게 모델을 구성하고 식을 아래와 같이 정의한다.
   - 사용되는 모델은 히든 레이어가 1개인 간단한 신경망을 사용한다. 
   - 입력 레이어의 인덱스는 \\(i\\), 히든 레이어의 인덱스는 \\(j\\), 출력 레이어의 인덱스는 \\(k\\) 로 정의한다.

$$\delta_k = \frac{\partial E_n}{\partial a_k}, \qquad M_{kk'} \equiv \frac{\partial^2 E}{\partial a_k \partial a_{k'}} \qquad{(5.92)}$$

- 일단 지금까지 다루었던 일차 미분방식에서는 \\(\delta\_k\\) 만 backprop 대상으로 삼았는데,
- 헤시안을 활용하기 위해서는 \\(M\_{kk'}\\) 도 backprop 대상으로 삼아야 한다.

- 일단 두번째 레이어 (즉, 마지막 레이어) 에서의 backprop 을 확인해본다.

$$\frac{\partial^{2} E_{n}}{\partial w_{kj}^{(2)} \partial w_{k'j'}^{(2)}} = \frac{\partial a_{k}}{\partial w_{kj}^{(2)}} \frac{\partial a_{k'}}{\partial w_{k'j'}^{(2)}} \frac{\partial^{2} E_{n}}{\partial a_{k} \partial a_{k'}} = z_{j} z_{j'} M_{kk'}\qquad{(5.93)}$$

- 이어서 첫번째 레이어의 weight 값을 확인해보자.

$$\frac{\partial^2 E_n}{\partial w_{ji}^{(1)} \partial w_{j'i'}^{(1)}} = \frac{\partial a_j}{\partial w_{ji}^{(2)}} \frac{\partial a_{j'}}{\partial w_{j'i'}^{(2)}} \frac{\partial^2 E_n}{\partial a_j \partial a_{j'}} = x_i x_{i'} M_{jj'} \qquad{(5.94)}$$

$$M_{jj'} \equiv \frac{\partial^2 E_n}{\partial a_j \partial a_{j'}} = {h''}(a_{j'}) I_{jj'} \sum_k w_{kj'}^{(2)} \delta_k\ +\ {h'}(a_{j'}) {h'}(a_j) \sum_k \sum_{k'} w_{kj'}^{(2)} w_{k'j}^{(2)} M_{kk'}$$

- 교재에서 기술된 것과 다르게 식을 좀 분리해서 썼다.
    - 사실 이 식의 전개 과정은 식(5.80)과 거의 같다.

- 이차 미분을 사용하고 있으므로 각 레이어에 대한 weight 값도 필요하다.
    - 즉, 첫번째 레이어, 두번재 레이어 각각에 대해 한번씩 미분한 값.
    
$$\frac{\partial^2 E_n}{\partial w_{ji}^{(1)} \partial w_{kj'}^{(2)}} = x_i {h'}(a_{j}) \left\{ \delta_{k} I_{jj'} + z_{j'} \sum_{k'} w_{k'j}^{(2)} M_{kk'} \right\}\qquad{(5.95)}$$

### 헤시안을 이용한 빠른 곱 (Fast multiplication by the Hessian)

- 헤시안 \\(H\\)를 응용하는 방법들은 대부분 헤시안 행렬 그 자체를 얻는 것보다 \\(H\\) 에 어떤 벡터를 곱에 얻는 값을 활용하는 경우가 많다.
- 그리고 헤시안 \\(H\\) 계산의 시간 복잡도는 \\(O(W^2)\\) 이고 공간 복잡도도 \\(O(W^2)\\) 임을 이미 확인했다.
- 실제 \\(H\\) 의 차원은 \\(W \times W\\) 이지만 실제 벡터 \\(v\\) 를 곱한 \\(v^T\dot H\\) 의 필요 차원은 \\(W\\) 이다.
- 이제 \\(O(W^2)\\) 복잡도를 가지는 \\(H\\) 를 계산한 뒤 다시 \\(v^T \dot H\\) 를 계산하는 것이 아니라 바로 \\(v^T\dot H\\) 를 계산하는 방법을 살펴볼 것이다.
- 일단 식을 먼저 보자.

$$v^T H = v^T \nabla (\nabla E)\qquad{(5.96)}$$

- 이 때 \\(\nabla E\\) 는 backprop 과정 중에 얻을 수 있다.

$$R\{w\} = v\qquad{(5.99)}$$

$$a_{j} = \sum_i w_{ji} x_i\qquad{(5.98})$$

$$z_{j} = h(a_j)\qquad{(5.99)}$$

$$y_{k} = a_k = \sum_j w_{kj} z_j\qquad{(5.100)}$$

$$R\{a_j\} = \sum_i v_{ji} x_i\qquad{(5.101)}$$

$$R\{z_j\} = {h'}(a_j) R\{a_j\}\qquad{(5.102)}$$

$$R\{y_k\} = \sum_j w_{kj} R\{z_j\}\ +\ \sum_j v_{kj} z_j\qquad{(5.103)}$$

$$\delta_k = y_k - t_k\qquad{(5.104)}$$

$$\delta_j = {h'}(a_j) \sum_k w_{kj} \delta_k\qquad{(5.105)}$$

$$R\{\delta_k\} = R\{y_k\}\qquad{(5.106)}$$

$$R\{\delta_j\} = {h''}(a_j) R\{a_j\} \sum_k w_{kj} \delta_k\ +\ {h'}(a_j) \sum_k v_{kj} \delta_k\ +\ {h'}(a_j) \sum_k w_{kj} R\{\delta_k\}\qquad{(5.107)}$$
  
$$\frac{\partial E}{\partial w_{kj}} = \delta_k z_j\qquad{(5.108)}$$
    
$$\frac{\partial E}{\partial w_{ji}} = \delta_j x_i\qquad{(5.109)}$$

$$R \left\{ \frac{\partial E}{\partial w_{kj}} \right\} = R\{\delta_k\} z_j\ +\ \delta_k R\{z_j\}\qquad{(5.110)}$$

$$R \left\{ \frac{\partial E}{\partial w_{ji}} \right\} = x_i R\{\delta_j\}\qquad{(5.111)}$$
