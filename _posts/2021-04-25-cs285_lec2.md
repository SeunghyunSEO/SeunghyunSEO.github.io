---
title: (yet) Lecture 2 - Supervised Learning of Behaviors

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 2의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=HUzyjOsd2PA&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=5)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-2.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 강의의 주제는 "Supervised Learning of Behaviors" 입니다. 아마 본 class의 큰 주제가 `심층 강화 학습 (Deep Reinforcement Learning)`인 만큼 `딥 러닝 (Deep Learning)`에 대한 애기를 하는 것 같은데요, 현재 수많은 딥 러닝 어플리케이션과 방법론이 기본적으로 `지도 학습 (Supervised Learning)`이기 때문에 리마인드 하는 것 같네요. 

![slide1](/assets/images/CS285/lec-2/slide1.png)
*Slide. 1.*

## <mark style='background-color: #fff5b1'> Terminology & notation </mark>

우선 `Terminology` , `Notation` 에 대해서 알아볼 건데, 컴퓨터 비젼 (Computer Vision) task 를 가정하고 전개를 한다고 합니다.  

![additive_fig_1](/assets/images/CS285/lec-2/additive_fig_1.png){: width="80%"}
*Additive Fig.*

일반적으로 사물 인식 (Object Recognition) 같은 task 에서는  

- Input : $$x$$
- Mapping Function : $$p(y \vert x)$$ ($$f(x)$$)
- Output : $$y$$

같은 term으로 표현할텐데, 본 class에서는 강화학습을 고려하기 때문에,  

- Input : $$o$$ (observation)
- Mapping Function : $$ \pi_{\theta} (a \vert o) $$ (policy, function)
- Output : $$a$$ (action)

라고 표현을 하는 것 같습니다. (딥 러닝의 일반적인 표현에서 점진적으로 강화 학습의 표현을 사용하면서 자연스럽게 둘을 융화시키는 것 같네요.)


위의 `teminology`를 조금 더 생각해 볼까요?

![additive_fig_2](/assets/images/CS285/lec-2/additive_fig_2.png){: width="80%"}
*Additive Fig.*
 
강화 학습이란 기본적으로 "sequential decision making problem" 이기 때문에 $$o \rightarrow a$$ 같이 입력(상태) 을 받아, 그 상황에서의 적절한 행동(행동)을 하는 것이 매 순간 일어납니다.

- Input : $$o_t$$ (observation)
- Mapping Function : $$ \pi_{\theta} (a_t \vert o_t) $$ (policy, function)
- Output : $$a_t$$ (action)

그래서 위와 같이 시간 $$t$$ 를 나타내는 아랫 첨자 (subscript)를 추가합니다 (기본적으로 강화학습에서는 discreter time-step을 사용합니다).


그리고 일반적인 딥 러닝의 지도 학습과는 다르게, 강화 학습에서는 어떤 시점의 output이 그 다음 시점의 input에 영향을 미칩니다
($$a_t$$가 $$o_{t+1}$$에 영향을 줌).


그러니까 만약 호랑이라는 걸 제대로 인식하지 못하면 그게 영향을 미쳐서 그 다음엔 조금 더 나에게 가까워진 호랑이를 볼 (observe) 수 있는거죠.  

이 아이디어를 조금 더 확장해서 생각해 보겠습니다. Mapping Function $$ \pi_{\theta} (a_t \vert o_t) $$ 가 출력하는 $$a_t$$가 가령 object에 대한 간단한 label이 아니라 action 이라고 생각해보겠습니다 (물론 discrete한 액션). 그렇게 하는 편이 행동을 하는 주체를 policy function에 따라서 자연스럽게 control 할  있기 때문이죠.

![additive_fig_3](/assets/images/CS285/lec-2/additive_fig_3.png){: width="80%"}
*Additive Fig.*

![additive_fig_4](/assets/images/CS285/lec-2/additive_fig_4.png){: width="80%"}
*Additive Fig.*

(물론 출력 분포가 discrete (Categorical) 하지 않고 continuous할 수도 있으며, 그럴 경우 만약 출력 분포가 가우시안 분포라면 평균 (mean), 분산 (variance)를 출력하게 되겠죠.) 


![slide2](/assets/images/CS285/lec-2/slide2.png)
*Slide. 2.*

여기에 추가적으로 추가되는 term에는 상태를 나타내는 $$s_t$$와 이를 given으로 output을 예측하는 $$\pi_{\theta} (a_t \vert s_t)$$가 있는데요,
$$o_t$$와 $$s_t$$의 차이는 $$s_t$$ 마르코프 상태를 가정했을 때 마르코프 그래프의 노드를 나타내는 "상태", 이고 $$o_t$$는 이러한 $$s_t$$로 부터 얻어내는 것 입니다. 그래서 보통은 $$ \pi_{\theta} (a \vert o) $$를 사용하지만 좀 더 엄격한 (restrictive) 상황에서는 $$\pi_{\theta} (a_t \vert s_t)$$를 사용하기도 한다고 합니다.



이를 


![slide3](/assets/images/CS285/lec-2/slide3.png)
*Slide. 3.*



![slide4](/assets/images/CS285/lec-2/slide4.png)
*Slide. 4.*



## <mark style='background-color: #fff5b1'> Imitation Learning </mark>


![slide5](/assets/images/CS285/lec-2/slide5.png)
*Slide. 5.*


![slide6](/assets/images/CS285/lec-2/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-2/slide7.png)
*Slide. 7.*


![slide8](/assets/images/CS285/lec-2/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-2/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-2/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-2/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-2/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-2/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-2/slide14.png)
*Slide. 14.*


### <mark style='background-color: #dcffe4'> Deep imitation learning in practice </mark>


![slide16](/assets/images/CS285/lec-2/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-2/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-2/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-2/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-2/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-2/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-2/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-2/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-2/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-2/slide25.png)
*Slide. 25.*



## <mark style='background-color: #fff5b1'> A case study: trail following from  human demonstration data </mark>

![slide27](/assets/images/CS285/lec-2/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-2/slide28.png)
*Slide. 28.*



## <mark style='background-color: #fff5b1'> Cost functions, reward functions, and a  bit of theory </mark>

![slide30](/assets/images/CS285/lec-2/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-2/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-2/slide32.png)
*Slide. 32.*

![slide34](/assets/images/CS285/lec-2/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-2/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-2/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-2/slide37.png)
*Slide. 37.*



## <mark style='background-color: #fff5b1'> Another way to imitate </mark>

![slide39](/assets/images/CS285/lec-2/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-2/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-2/slide41.png)
*Slide. 41.*

![slide42](/assets/images/CS285/lec-2/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-2/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-2/slide44.png)
*Slide. 44.*



## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








