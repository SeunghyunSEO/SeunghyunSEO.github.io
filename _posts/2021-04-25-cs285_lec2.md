---
title: Lecture 2 - Supervised Learning of Behaviors

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)

Lecture 2의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=HUzyjOsd2PA&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=5)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-2.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 강의의 주제는 "Supervised Learning of Behaviors" 입니다. 

![slide1](/assets/images/CS285/lec-2/slide1.png)
*Slide. 1.*

## <mark style='background-color: #fff5b1'> Terminology & notation </mark>

우선 `Terminology` , `Notation` 에 대해서 알아볼 건데, 컴퓨터 비젼 (Computer Vision) task 를 가정하고 전개를 합니다.  

![additive_fig_1](/assets/images/CS285/lec-2/additive_fig_1.png){: width="80%"}
*Additive Fig.*

일반적으로 사물 인식 (Object Recognition) 같은 task 에서는  

- Input : $$x$$
- Mapping Function : $$p(y \vert x)$$ ($$f(x)$$)
- Output : $$y$$

같은 term으로 표현할텐데, 본 class에서는 강화학습을 고려하기 때문에,  

- Input : $$o$$ (observation)
- Mapping Function : $$ \pi_{\theta} (a \vert o) $$ (policy, function)
- Output : $$a$$ (action)

라고 표현을 합니다. (딥 러닝의 일반적인 표현에서 점진적으로 강화 학습의 표현을 사용하면서 자연스럽게 둘을 융화시켜 설명하는듯)


위의 `teminology`를 조금 더 생각해 볼까요?

![additive_fig_2](/assets/images/CS285/lec-2/additive_fig_2.png){: width="80%"}
*Additive Fig.*
 
강화 학습이란 기본적으로 `Sequential Decision Making Problem` 이기 때문에 $$o \rightarrow a$$ 같이 입력(상태) 을 받아, 그 상황에서의 적절한 행동(행동)을 하는 것이 매 순간 일어납니다.

- Input : $$o_t$$ (observation)
- Mapping Function : $$ \pi_{\theta} (a_t \vert o_t) $$ (policy, function)
- Output : $$a_t$$ (action)

그래서 위와 같이 시간 $$t$$ 를 나타내는 아랫 첨자 (subscript)를 추가합니다 (기본적으로 강화학습에서는 discrete time-step을 사용합니다).


그리고 일반적인 딥 러닝의 지도 학습과는 다르게, 강화 학습에서는 어떤 시점의 output이 그 다음 시점의 input에 영향을 미칩니다
($$a_t$$가 $$o_{t+1}$$에 영향을 줌).


그러니까 만약 호랑이라는 걸 제대로 인식하지 못하면 그게 영향을 미쳐서 그 다음엔 조금 더 나에게 가까워진 호랑이를 볼 (observe) 수 있는거죠.  

이 아이디어를 조금 더 확장해서 생각해 보겠습니다. Mapping Function $$ \pi_{\theta} (a_t \vert o_t) $$ 가 출력하는 $$a_t$$가 가령 object에 대한 간단한 label이 아니라 

![additive_fig_3](/assets/images/CS285/lec-2/additive_fig_3.png){: width="80%"}
*Additive Fig.*

action 이라고 생각해보겠습니다 (물론 discrete한 액션). 그렇게 하는 편이 행동을 하는 주체를 policy function에 따라서 자연스럽게 control 할  있기 때문이죠.

![additive_fig_4](/assets/images/CS285/lec-2/additive_fig_4.png){: width="80%"}
*Additive Fig.*

(물론 출력 분포가 위의 그림에서 처럼 discrete (Categorical) (1.도망친다, 2.무시한다, 3.쓰다듬는다(?...)) 하지 않고 continuous할 수도 있으며, 그럴 경우 만약 출력 분포가 가우시안 분포라면 평균 (mean), 분산 (variance)를 출력하게 되겠죠.) 


![slide2](/assets/images/CS285/lec-2/slide2.png)
*Slide. 2.*

여기에 추가적인 term들, 상태를 나타내는 $$s_t$$와 이를 given으로 output을 예측하는 $$\pi_{\theta} (a_t \vert s_t)$$가 있습니다.
여기서 $$o_t$$와 $$s_t$$의 차이는 $$s_t$$ `마르코프 상태 (Markov State)`를 가정했을 때 Markov Graph의 노드를 나타내는 것이 "상태"라면, $$o_t$$는 이러한 $$s_t$$로 부터 얻어내는 것이라는 겁니다 ($$o_t$$가 더 작은 개념인것). 그래서 보통은 $$ \pi_{\theta} (a \vert o) $$를 사용하지만 좀 더 엄격한 (restrictive) 상황에서는 $$\pi_{\theta} (a_t \vert s_t)$$를 사용하기도 한다고 합니다.


두 가지 차이에 대해 조금 더 얘기해 보도록 하겠습니다.

예를 들어 치타가 가젤을 쫓아가는 이미지가 입력으로 들어왔다고 해보겠습니다. (이미지는 픽셀로 이루어져 있죠.)
이러한 이미지를 통해서 치타와 가젤의 위치가 각각 상대적으로 어디에 있으며, 이를 통해 치타가 가젤을 쫓아가는 것을 유추할 수 있죠.

![additive_fig_5](/assets/images/CS285/lec-2/additive_fig_5.png){: width="50%"}
*Additive Fig.*

이게 `Observation` 입니다.

![additive_fig_6](/assets/images/CS285/lec-2/additive_fig_6.png){: width="50%"}
*Additive Fig.*

`State`는 근본적인 물리계의 정보를 말하는데요, 예를 들면 절대적인 위치 (position) 이나 물체의 속도 (velocity) 같은 것들이 되겠습니다. 
(State는 minimal representation을 포함하고 있습니다.)

지금은 두 가지의 차이가 명확해 보이지 않는데, 만약 어떤 자동차가 지나가면서 치타를 가렸다고 생각해보면 이야기는 달라집니다.

![additive_fig_7](/assets/images/CS285/lec-2/additive_fig_7.png){: width="50%"}
*Additive Fig.*

위의 경우에 Observation에는 치타가 안 보이지만 State는 그렇지 않죠. 
즉, State는 실제 정보 (True Configuration)을 나타내는 것이고 Observation은 이것의 일부분 (이미지) 인 것입니다. (Observation은 State를 추론하기에 (deduce) 충분할 수도, 아닐 수도 있습니다.)

```
그렇다면 State를 이용해서 판단하는게 쉬울까? Observation을 이용해서 판단을 하는게 쉬울까? 그리고 Observation은 많을수록 좋을까?

한 번 생각해봅시다.
```


이를 간단한 Graphical Model로 생각해보자면 아래와 같은데 (*Slide. 3.*의 아래 부분)

![slide3](/assets/images/CS285/lec-2/slide3.png)
*Slide. 3.*

이를 조금 더 살펴보자면,

![additive_fig_8](/assets/images/CS285/lec-2/additive_fig_8.png){: width="80%"}
*Additive Fig.*

State, $$s_t$$에서 얻어낸 Observation, $$o_t$$와 이를 given으로 어떤 행동, $$a_t$$을 할 지를 정하고, 이를 행동의 영향으로 인해 State가 바뀝니다. 

![additive_fig_9](/assets/images/CS285/lec-2/additive_fig_9.png){: width="80%"}
*Additive Fig.*

가 아니라, 사실 내가 어떤 행동을 하면 상태가 무조건 변하지는 않습니다.
상태가 변할지는 확률 $$p(s_{t+1} \vert s_t, a_t)$$에 따라 결정됩니다.
(즉 상태가 안변하고 그대로 일 확률도 있다는 거죠, 예를 들어 내가 호랑이를 보고 무시했더니 호랑이도 날 똑같이 무시해서 교착 상태에 빠지는... . 이러한 현재 상태가 다른 어떤 상태로 변할 수 있는지에 대한 확률을 포함하고 있는 것을 `상태 천이 행렬 (State Transition Probability)`라고 합니다.)


또한 Markov 상태를 가정한 경우, 중요한 가정을 특성이 있는데요, 이는 $$s_{t+1}$$ 상태를 결정하는 데에는 $$s_{t}$$ 이전의 상태는 필요 없고 (conditionally independent) 오직 $$s_{t}$$만 필요하다는 겁니다. 그리고 $$s_t$$ 만으로 모든 걸 합리적으로 결정할 수 있다는 겁니다. 이는 sequential decision making을 하는 강화 학습에 있어 굉장히 중요한 특성이라고 하는데요, 이러한 특성 (property)이 없이는 optimal policies를 정확하게 나타낼 (formulate) 수 없기 때문이라고 합니다.


Observation을 사용하냐 State를 사용하냐는 Markov Property를 만족하느냐 (과연 현재 (present) 상태만으로 미래 (future)를 제대로 판단할 수 있느냐) 하는 부분에 있어 차이가 있는데요, 위의 치타가 가젤을 쫓는 경우를 생각해 봤을 때, 치타가 차에 가려진 어떤 시점에서는 그 시점의 Observation 정보로는 제대로 된 판단을 내릴 수가 없습니다. 그렇기 때문에 이럴 경우에는 과거의 정보 (past Observation)를 기억해서 (Memorize) 사용 할 수 있다고 하는데, State를 사용하는 경우에는 Markov Property를 만족하기 때문에 그럴 필요가 없다고 합니다.

```
Memorize 한다는 것이 Memory에 이전 time-step을 저장하고 현재 time-step의 몇개 전 까지의 정보를 같이 condition 하겠다는 건지, 아니면 RNN 같은 모듈을 사용하겠다는건지 지금은 잘 모르겠네요.
```

Lecturer는 앞으로 이 class에서는 대부분 State를 사용하는 $$\pi_{\theta} (a_t \vert s_t)$$를 policy로 사용하겠다고 합니다.
혹은 종종 이러한 non-Markovian Observation 들을 사용하는 알고리즘들도 소개할 예정이라고 합니다.





![slide4](/assets/images/CS285/lec-2/slide4.png)
*Slide. 4.*

위의 슬라이드는 어떤 term을 사용해도 상관 없다는 내용입니다.
왼쪽은 벨만에 의해 많이 연구된 `Dynamic Programming`의 term들인데, 이걸 써도 되고 오른쪽의 `Robotics` 분야의 term을 써도 된다고 하네요. 


## <mark style='background-color: #fff5b1'> Imitation Learning </mark>

이제 위에서 term들을 정의했으니 딥러닝의 `Image Classifier`에 해당하는 강화학습의 `정책 (Policy)`이라는 것을 어떻게 학습 할 것인가에 대해서 간단하게 얘기하려고 합니다.
크게 다를 바가 없는데요, 물론 더 복잡한 강화학습의 방법론들이 존재하지만 이번 장에서는 간단한 방법으로 직관적인 이해를 더하려고 합니다.

![slide5](/assets/images/CS285/lec-2/slide5.png)
*Slide. 5.*

호랑이 한테서 도망가는 예제 말고, 자율 주행 task를 생각 해 보도록 하겠습니다.
자율 주행 task에서는 카메라의 매 프레임이 Observation이 되겠고, 이에 따라 운전대를 어떻게 돌릴지 (행동)를 결정하면 되겠습니다. 
딥러닝의 `지도 학습 (Supervised Learning)`을 생각 해 볼까요? 간단하게 매 프레임 마다 사람이 어떻게 운전대를 control 했는지에 대한 정답 label이 존재 할 수 있다고 생각할 수 있겠습니다. 


이걸 바로 `Imitation Learning` 혹은 `Behavior Cloning`이라고 합니다. 
일반적으로 전문가 (Expert)의 행동을 따라 (Clone) 하게 되기 때문입니다.


근데 이러한 방법론이 잘 작용할까요?

![slide6](/assets/images/CS285/lec-2/slide6.png)
*Slide. 6.*

사실 이러한 방법은 (DNN 사용하기에 Deep Imitation Learning) 1989에 제안된 방법이며 그런대로 잘 작동해서 미 대륙 횡단을 해보려는 시도까지 했었다고 합니다.


![slide7](/assets/images/CS285/lec-2/slide7.png)
*Slide. 7.*

하지만 이렇게 일반적으로 딥러닝의 지도 학습에서 사용되는 방법론이 대부분의 강화학습에서 잘 작용하지는 않는데요,
이는 *Slide. 7.*의 두 `궤적 (trajectory)`을 보면 알 수 있습니다.

그래프가 의미하는 바는, 시간이 흐를수록 state value가 변한다는 것인데요, Policy $$\pi_{\theta}$$를 학습하기 위한 학습 데이터의 실제 궤적 (training trajectory)이 (검은색)과 같을 때, 이는 $$\pi_{\theta}$$를 통해서 진행한 궤적 (빨간색)과 굉장히 다르기 쉽다는 겁니다.
 
그림을 보시면 초반에는 궤적을 잘 따라가나 싶다가도, 중간에 사소한 실수 (mistake)를 하게 되면 맥을 못추고 다른 궤적을 그리는 걸 볼 수 있습니다. 
이는 사소한 실수를 통해서 전혀 본 적 없는 state에 빠지게 되면 policy가 이를 어떻게 처리할지를 모르기 때문에 잘못된 선택을 하게 되고, 이러한 실수가 쌓이면 (`compounding error problem`) 학습이 어렵기 때문입니다.




![slide8](/assets/images/CS285/lec-2/slide8.png)
*Slide. 8. Animation*

하지만 이는 practical하게는 잘 작동한다는데요, (...? Sergey 선생님?)
데이터를 굉장히 많이 수집하고 약간의 tricks을 사용해서 잘 작용하게 만들 수 있다고 합니다.




![slide9](/assets/images/CS285/lec-2/slide9.png)
*Slide. 9.*

후에 다시 다루게 되겠지만 간단하게 얘기해보겠습니다.
NVIDIA가 제안했던 논문을 보면 왜 이게 작동하는지 알 수 있는데요, 그림을 보면 세 가지 다른 곳을 응시하는 카메라가 있는데, 여기서 정면 카메라의 데이터로 위에서 언급한 지도 학습대로 학습을 하게되고, 양쪽의 카메라도 마찬가지로 학습이 되지만 이렇게 카메라를 여러 대 사용하는 trick이 사소한 실수들을 잡아주기 때문에, 실수가 많이 쌓여서 궤적이 심하게 이탈할 일이 없다고 합니다.



하지만 이러한 practical한 trick 말고 더 일반적인 (general) 방법이 있는데요,

![slide10](/assets/images/CS285/lec-2/slide10.png)
*Slide. 10.*

그건 바로 training trajectory를 조금 수정하는 겁니다. 일부러 실수를 포함하게 만들고 이 실수를 수정하는 (feedback to correct) 걸 policy가 배우게 하는 겁니다.

![slide11](/assets/images/CS285/lec-2/slide11.png)
*Slide. 11.*

한편, 다시 Policy를 학습하는 것에 대해서 생각해 보겠습니다. 
강화학습을 한다는 것은 Policy를 학습해서 매 순간(state)마다 행동(action)을 sampling 하는 겁니다.

$$
\pi_{\theta}(a_t|o_t)
$$

우리는 일반적으로 딥러닝을 출력의 확률 분포를 모델링하는 방법론이라고 생각할 수 있는데 이는 "`학습 데이터의 분포 (training distribution)`를 Mapping Function이 모방하는 것 입니다".

$$
p_{\pi_{\theta}}(o_t) = p_{data}(o_t)
$$

그리고 학습 데이터 샘플이 적당히 많아서, 실제 분포를 찾는데 무리가 없고, 학습을 통해 분포를 잘 모방하기만 한다면 우리는 `unseen test data` 에 대해서도 올바른 결정을 잘 내릴 수 있게 될겁니다 (학습 데이터 뭉치가 실제 분포로부터 잘 sampling된 것들이어야 잘 작용할겁니다).  


어떻게 이 실제 학습 분포와, policy가 만들어내는 분포를 매칭 시킬 수 있을까요? (이렇게만 하면 에러를 만들어 낼 일이 없을텐데요...)


간단한 방법은 policy를 완벽하게 만드는겁니다. (??)
하지만 이는 매우 어려운 방법이죠.





그렇다면 혹시 policy를 바꾸지 말고 data를 바꿔보면 어떨까요?

![slide12](/assets/images/CS285/lec-2/slide12.png)
*Slide. 12.*

위와 같이 data를 바꾸는 방법을 `DAgger (Dataset Aggregation)`이라고 합니다. ([Aggregation](https://en.dict.naver.com/#/entry/enko/69130e56aefc47e4944701ad1266f213); 집합체)

이 방법은 데이터를 $$p_{data}(o_t)$$에서 뽑지 말고, $$p_{\pi_{\theta}}(o_t)$$ 에서 뽑아 보자는 아이디어 입니다.
이런식으로 학습을 하면 앞서 말한 `Distributional Shift Problem`이 일어나지 않을 거라는 겁니다.


학습 방법은 슬라이드에도 나와있지만 다시 쓰자면 아래와 같습니다.

1. 우선 $$\pi_{\theta} (a_t \vert o_t)$$ 를 인간이 수집한 데이터 $$ D = \{o_1,a_1, \cdots, o_N, a_N\}$$을 통해 학습한다 (initialize $$\theta$$).
2. 어느정도 학습된 파라메터의 Policy, $$\pi_{\theta} (a_t \vert o_t)$$를 이용해서 추가 데이터를 {D_{\pi}=\{ o_1,\cdots,o_M \}}뽑는다. 
3. Observation만 있는 데이터에 대해서 사람이 직접 labeling을 한다. (...???)
4. 이렇게 완성한 추가 데이터를 원래의 학습 데이터와 합친다(Aggregate). ($$D \leftarrow D \cup D_{\pi}$$)
5. 다시 학습 loop를 돈다.





![slide13](/assets/images/CS285/lec-2/slide13.png)
*Slide. 13. Animation*

(*Slide. 13.*는 드론이 `DAgger`로 학습된 결과를 보여주는데, 자세한 건 강의를 참조하세요!)





![slide14](/assets/images/CS285/lec-2/slide14.png)
*Slide. 14.*

하지만 DAgger가 별로 좋은 알고리즘은 아닌데요, 왜 그럴까요?
당연하게도, 중간에 사람이 일일히 다 labeling 해줘야 하는 과정이 있기 때문입니다. 


이를 해결하려면 어떻게 해야 할까요?





### <mark style='background-color: #dcffe4'> Deep imitation learning in practice </mark>

이번에는 training loop 중간에 사람이 개입해야 하는 DAgger와 다른 방식으로 `Distributional Shift Problem`을 해결 해 보도록 하겠습니다.

그건 바로 행동을 하거나 레이블링 하는 주체인 `Expert` 자체를 더욱 정교하게 모방하자는 건데요 (오버피팅은 하면 안됨), 이것은 DAgger처럼 이론적으로 보장이 되지는 않지만(?), 경험적으로 굉장히 잘 작용하는 `어떤 방법`을 쓰자는 겁니다.




![slide16](/assets/images/CS285/lec-2/slide16.png)
*Slide. 16.*

우리가 앞으로 논할 내용 (미리 말하자면 Deep Imitation Learning)에 대해서 이해하기 위해 아래의 이유에 대해서 생각 해 보도록 하겠습니다. => "Why might we fail to fit the expert?"







![slide17](/assets/images/CS285/lec-2/slide17.png)
*Slide. 17.*


우리가 강화학습을 하기 위해 가정했던 것 중에는 `Markovian 상태` 가 있었습니다.
이는 즉 우리가 현재 상태만으로 (과거의 정보 없이, 과거 상태는 다 무시해도 될 정도) 다음 미래 상태를 추론 (infer) 할 수 있다는 겁니다.

하지만 우리는 실제로는 위의 특성과는 다른 걸 다루고 있는데요, 즉 사람이 행동하는 것 자체가 `non-markovian`이라는 겁니다.
그리고 또 하나는 이러한 행동이 모방하기 쉽지 않은 `Multimodal behavior` 이라는 겁니다. (잘 와닿지 않네요...) 


첫 번째 문제점에 대해서 생각해보자면, 강화 학습의 정책은 `Markov Property`에 따라서 오직 $$o_t$$에 의해서만 $$a_t$$를 결정하는데 ($$o_{t-1}$$, $$o_{t-2}$$ 등은 고려하지 않음.) 이 말은 즉 똑같은 state가 두 번 반복되면 전에 어떤 상태였는지 don't care이기 때문에 완전히 똑같은 행동을 두 번 하게 된다는 겁니다. 
하지만 사람은 그렇지 않죠, 운전을 할 때도 똑같은 state가 지속되지만 그동안 해왔던 state를 다 고려해서 행동합니다.
$$\pi_{\theta} (a_t \vert o_t)$$ 보다는 $$\pi_{\theta} (a_t \vert o_t, o_{t-1}, o_{t-2}, \cdots)$$ 에 가깝죠.




![slide18](/assets/images/CS285/lec-2/slide18.png)
*Slide. 18.*

그렇다면 인간의 특성을 잘 반영해서 과거의 모든 history들을 네트워크에 전부 태우면 되는걸까요?
아니죠, 이렇게 하면 몇가지 이슈가 있는데요. 
우선 이미지를 그냥 붙히면 (concatenate) 우리는 variable input length에 대해서 고려해야 하며 이를 어떻게 `CNN (Convolutional Neural Network)` 에 넣어야 하는지 감도 잘 안옵니다.
그리고 과거까지 봐야 하는 이미지의 수가 많다면 너무나 많은 weight가 필요하게 된다는 문제점도 있죠.





![slide19](/assets/images/CS285/lec-2/slide19.png)
*Slide. 19.*

이러한 문제를 해결하기 위한 일반적인 방법론은 바로 `RNN (Recurrent Neural Network)`을 사용하는 겁니다 (위에서 말한 더욱 사람처럼 행동하기 위한 `어떤 방법`이 바로 RNN을 쓰는 것이죠). 물론 전체 뼈대는 RNN이지만 CNN으로 이미지를 인코딩 해 Representation Vector로 만든 것을 RNN의 입력으로 씁니다. 이런 RNN이 (LSTM, GRU가 같은 것도 있죠) `non-markovian promlem` 잘 완화시킨다 (mitigate)고 합니다.





![slide20](/assets/images/CS285/lec-2/slide20.png)
*Slide. 20.*

`Imitation Learning`의 또 다른 문제점에는 `Causal Confusion` 이라는 것도 있습니다.

이는 차라리 Observation에 너무 많은 정보가 포함되어있지 않은게 낫다 라는 건데요, 왜냐하면 여러가지 상호작용으로 인해서 모델이 헷갈려 할 수 있기 때문입니다.
왼쪽의 경우 카메라가 전면 유리 안쪽에 위치해 있어 대쉬보드 같은것들을 전부 관측 (observe)할 수 있는 상황인데, 브레이크를 밟으면 대쉬보드에 어떤 불빛이 켜집니다.
처음 사람을 보고 브레이크를 밟겠지만 그 뒤로는 불빛이 켜진 게 무엇때문이지 모델이 헷갈린다는 겁니다. 즉 잘못된 인과 관계를 학습하게 된다는 거죠.


자 이제 우리가 생각해야 할 것은 두가지 입니다.

- `Imitation Learning with RNN Approach` 가 이러한 `Causal Confusion` 문제를 경감시킬 수 있을 것인가?
- 아니라면 `DAgger`는?

(마지막에 논한다고 하네요)


이번에는 왜 Expert를 모방하는 것이 어려운지에 대해, `Multimodal Behavior` 관점에서 생각 해 보도록 하겠습니다.







![additive_fig_10](/assets/images/CS285/lec-2/additive_fig_10.png){: width="30%"}
*Additive Fig.*

어떤 드론이 비행하는 상황에 대해서 생각 해 보겠습니다. 우리는 학습 데이터로부터 드론의 Policy를 배우는게 목적이겠죠?

비행하는 경로 상에 나무가 존재한다고 하면 드론은 왼쪽이나 오른쪽으로 나무를 피해 비행해야 할 겁니다.




![additive_fig_11](/assets/images/CS285/lec-2/additive_fig_11.png){: width="30%"}
*Additive Fig.*

만약에 Policy가 `Discrte`한 `Categorical Distribution`을 가정한다면 (위), 문제는 별로 어렵지 않죠.
매 순간마다 Softmax 확률이 주어질테고 가장 쉽게는 `argmax operation`을 통해 구한 가장 큰 확률을 따르는 행동을 하면 되니까요.




![additive_fig_12](/assets/images/CS285/lec-2/additive_fig_12.png){: width="30%"}
*Additive Fig.*

하지만 행동 (action)에 대한 분포가 `Continuous`한 상황이라면 어떨까요? 일반적으로 연속적인 분포 `Gaussian Distribution`을 모델링 한다고 생각해 보면 (위), 이럴 경우 `평균 (mean)` 과 `분산 (varioance)`이 출력으로 나오는데 이를 어떻게 사용해서 결정을 내려야 할지 감이 잘 오지 않습니다. 하지만 대부분 사람이 결정을 내릴 때는 간단한 일을 할 때라도 매우 복잡한 분포 (very complicate distributions)을 이용해서 결정을 내린다고 하는데요, 이런게 강화학습에서는 문제가 된다는 것입니다.


(아마 이 문제는 학습 데이터에 왼쪽으로 도는게 반, 오른쪽으로 도는게 반 이었다면, 이들의 절반인 가운데로 가는 것을 결과로 내놓을 것이라는 의미인 것 같습니다.
그럴 경우, 양쪽의 확률이 비슷하다고 예측하게 될테니 가운데로 확률이 몰려서 가운데로 비행을 할 수 있겠죠?
하지만 Categorical인 경우에 왼쪽 40%, 가운데 18%, 오른쪽 42%면 의심의 여지없이 오른쪽을 고르니 문제가 없을겁니다.) 






![slide21](/assets/images/CS285/lec-2/slide21.png)
*Slide. 21.*

이를 해결하기 위한 간단한 해결책은 바로 하나의 평균 (mean) 과 분산 (varioance)을 하나만 가지는 `One Mode Distribition` 을 사용하지 않고, 봉우리(Mode)가 여러개인 `Multi Modal Distribution`을 모델링 하는 것 입니다. 
이러면 예를 들어 전방에 나무가 있는 상황을 마주했을 때, 분포에서 왼쪽으로 몇 도 만큼 튼다는 부분이 조금 높게 튀고 오른쪽으로 몇 도 만큼 튼다는 부분이 조금 높게 튀게 되는 이른바 `bi-modal distribution`을 모델링 할 수 있게 되겠고 봉우리가 하나 였던 `uni-mode gaussian distribution`에서 발생하는 문제를 해결할 수 있게 됩니다.


이렇게 하기 위한 방법론은 간단하게는 여러 가우시안 분포를 합치는 방법부터 `Latent Variable Model`을 사용하는 복잡한 모델까지 다양합니다. 
이제 이들에 대해 조금 더 얘기 해 보도록 하겠습니다.

- Output Mixture of Gaussians
- Latent Variable Models
- Autoregressive Discretization






![slide22](/assets/images/CS285/lec-2/slide22.png)
*Slide. 22.*

가장 간단한 방법인 가우시안 여러개를 weighted sum 하는 방법은 `Mixture Density Network`라고 불리기도 합니다. 여러개의 `평균 (mean)` 과 `분산 (varioance)`을 예측하고 이를 weighted sum 하는 겁니다. 
이러한 방법의 단점은 파라메터 수가 엄청나게 증가하며, 고차원의 분포를 모델링 하게 될 수록 더욱 학습이 어려워 진다는 겁니다. 

$$
w_1,\mu_1, \sigma_1, \cdots, \w_N,\mu_N,\sigma_N
$$

즉 이미지를 이용해서 자율주행을 하는 task에서는 그다지 어렵지 않겠지만 (2차원 이라서 그렇다는데, 이미지가 2차원 매트릭스(채널까지 하면 3차원 텐서)긴 하지만 상당히 고차원 아닌가...?), 엄청나게 고차원 (고관절)을 다루는 Humanoid Robot을 다루는 task에서는 그렇지 않다는 문제가 있습니다.




![slide23](/assets/images/CS285/lec-2/slide23.png)
*Slide. 23.*

잠재 변수 모델 (Latent Variable Model)은 원래 모델에서 `잠재 변수를 Integrated Out` 해서 학습하는 모델인데요, *Slide. 23.*에서 잘 보시면 어떤 분포 (prior에서 샘플링한 벡터를 이미지에 추가하는 느낌인데, 즉 이는 이미지와 어떤 Noise를 더한다고 생각할 수 있고, 이렇게 된 입력을 가우시안 분포로 Mapping 해주는 모델입니다.)
그니까 유사한 이미지더라도 Noise에 따라 다른 분포로 Mapping이 된다는 거죠. 이러한 모델의 장점은 다양한 분포들을 표현할 수 있지만 이는 학습하기가 굉장히 까다롭고 여러가지 trick들이 필요합니다. 
이를 어떻게 학습하는지에 대해서는 나중에 (Lec 18) 다루도록 한다고 합니다.  




![slide24](/assets/images/CS285/lec-2/slide24.png)
*Slide. 24.*

마지막으로 생각해 볼 `Autoregressive Discretization`는 매우 간단한 `Output Mixture of Gaussians)`와 복잡한 `Latent Variable Model`의 중간 정도의 학습 난이도와 표현력을 가지는 방법론 입니다.
만약 우리가 연속적인 행동 (Continuous Actions Space)를 가정한다면, 이를 이산화 (Discretizing)하는 것은 굉장히 어렵습니다. 
왜냐하면 이런 경우 일반적으로 n차원의 Action Space를 discretizing하는 데 필요한 Bin의 수가 $$exp^n$$이 되기 때문입니다.
만약 우리가 2개의 Action (1.Steering, 2.Break)만 있다면 쉽긴 하겠죠.


`Autoregressive Discretization`는 Autoregressive하게 (반복적으로) 한 번에 한 차원만 discretize 합니다. 


예를 들어, 위의 2차원 Action Space를 가지는 자율 주행 문제를 예로 들어보면, 우선 첫 번째 차원에 대해서 discretize를 합니다. 다시 말해서, 어떤 네트워크가 이미지를 입력으로 받아서 Steering에 대한걸 Softmax분포로 나타내는 거죠. 
그 다음에 이 첫 번째 Softmax 분포로부터 샘플링을 한번 하고, 이를 다른 네트워크에 넣습니다. 
이렇게 두 번째 Action, 브레이크를 밟는 것에 대한 분포를 만들어내는거죠. 여기서 샘플링을 또 하고 반복하는겁니다.

이렇게 어떤 Action Dimension에 대해서 먼저 분포를 예측하고 이를 다음 예측할 Action Dimension 분포의 condition으로 사용해 순차적으로 분포를 찾은다음 Chain Rule을 통해서 전체 Action에 대해서 Full Joint Probability를 나타내는 방법은 그렇게 cost가 비싸지 않게 되는겁니다.
(이렇게 하는게 실제로도 잘 작용한다고 하네요.)


***

아마 Autoregressive Discretization이라는 것 자체가 어느 논문에서 Continuous Action Space를 정의하기 위해 사용된 방법을 Lecturer가 새로 정의한 단어 같네요.

출처 : [Reddit](https://www.reddit.com/r/reinforcementlearning/comments/i0iyf5/autoregressive_discretization_in_reinforcement/)

***




![slide25](/assets/images/CS285/lec-2/slide25.png)
*Slide. 25.*

*Slide. 25.*는 여태까지 배운 것을 종합한 슬라이드입니다.









## <mark style='background-color: #fff5b1'> A case study: trail following from  human demonstration data </mark>

짧게 지금까지 배운 Real-World Robotics 에서 Imitation Learning에 대한 사례 연구 (Case Study) 에 대해서 알아본다고 합니다.

(영상은 본 글에서는 나오지 않으니 강의를 참고하시기 바랍니다.)

![slide27](/assets/images/CS285/lec-2/slide27.png)
*Slide. 27.*

드론 자율주행에 관한 논문인 [A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots](https://ieeexplore.ieee.org/document/7358076)의 데모 영상이며, 직진, 좌회전, 우회전 세 가지 Discrete Action Space에 대한 Classification 문제라고 합니다. 
(드론이 직접 비행하면서 데이터를 모으기 어렵기 때문에 사람이 카메라 달린 머리띠를 메고 직접 돌아다니면서 모았다는게 재밌는 것 같습니다.)








## <mark style='background-color: #fff5b1'> Cost functions, reward functions, and a  bit of theory </mark>

이번에는 Imitation Learning이 가지는 문제점들과 이에 대한 해결책들에 대해서 알아보도록 하겠습니다.

![slide30](/assets/images/CS285/lec-2/slide30.png)
*Slide. 30.*

*Slide. 30.*에 언급된 데이터 이슈나 어느 상황에서 어떤 행동을 하는것이 최선인지를 사람이 알기 어렵다는 것 등이 바로 그 문제점들이 됩니다. 

![slide31](/assets/images/CS285/lec-2/slide31.png)
*Slide. 31.*

다시 위에서 다뤘던 호랑이에 쫓기는 상황을 생각해볼까요? 
우리가 Observation을 Action으로 Mapping하는 네트워크를 가진 상황에서, 과연 어떤식으로 Mapping하는게 좋고 나쁜지에 대해서 정한 적은 없습니다.


하지만 데이터도 풍부하고 다른 이슈는 없다고 생각할 때, 우리가 원하는 것은 단지 호랑이한테 먹히지 않고 살아남는 것이 될겁니다. 
이는 수학적으로 아래의 식으로 나타낼 수 있는데요, (강의 슬라이드에는 수식들이 뭔가 겹쳐서 잘 알아보기 힘들어서 다시 씁니다.)

$$
min_{\theta} \mathbb{E}_{a \sim \pi_{\theta}(a \vert s), s' \sim p(s' \vert s,a)} [ \delta (s'= eaten \space by \space tiger)]
$$

호랑이한테 먹히는 경우에 대한 `기대 값 (Expectation)`을 최소화 하는게 바로 목적이 되는겁니다.
(여기서 delta function, $$\delta$$을 사용했기 때문에 먹힐 경우만 1이고 나머진 전부 0입니다.)


수식의 의미는 "현재 상태에서, 어떤 행동을 취했는데, 그렇게 했을 때 내가 잡아먹힐까?" 입니다.
수식에 기대값이 있는데요, 기대값의 정의는 아래처럼 모든 outcome들에 대해서 다 확률을 곱하는것 이기 때문에 

$$
\mathbb{X} = \sum_{i=1}^k x_i  p_i = x_1 p_1 + x_2 p_2 + \cdots + x_k p_k
$$

우리는 어떤 행동이 초래하게될 다음 상태, $$s'$$가 호랑이 뱃속이 아니게끔, 그렇게 만들 행동을 할 확률을 가장 작게 만들면 됩니다. 


이는 더 일반적으로 아래와 같이 나타낼 수 있는데요,

$$
min_{\theta} \mathbb{E}_{s_{1:T},a_{1:T}} [ \sum_t \delta (s'= eaten \space by \space tiger)]
$$

즉 호랑이한테 항상 먹히지 않았으면 좋겠다는 의미의 수식인거죠.
이를 조금 더 일반적이게 나타내면 $$\delta$$ 함수를 다른 다양한 `손실 함수 (Cost Function)`으로도 나타낼 수 있습니다.

$$
min_{\theta} \mathbb{E}_{s_{1:T},a_{1:T}} [ \sum_t c(s_t,a_t)]
$$

다음 챕터에서 배우겠지만 $$c(s_t,a_t)$$를 `보상 함수 (Reward Function)`로 나타내면

$$
max_{\theta} \mathbb{E}_{s_{1:T},a_{1:T}} [ \sum_t r(s_t,a_t)]
$$

최종적으로 일반적인 `강화 학습의 Objective Function` 을 얻어낼 수 있습니다. 
(위험은 감소 (minimize) 하는게 맞고, 보상은 늘려야 (maximize) 하기 때문에 수식이 조금 다르지만 결론은 같습니다.)







![slide32](/assets/images/CS285/lec-2/slide32.png)
*Slide. 32.*

(*Slide. 32.*는 대충 두 notation이 같다는 뜻, 하지만 강화학습에서는 보통 왼쪽의 notation을 씁니다.)






그렇다면 Imitation Learning을 위한 좋은 `Cost Function`, `Reward Function`은 과연 무엇일까요?

![slide34](/assets/images/CS285/lec-2/slide34.png)
*Slide. 34.*

모방 학습에서는 사람 (Expert) 이 하는 것을 따라하면 되기 때문에 이럴 경우 Reward는 `Log Probability`가 됩니다.

$$
r(s,a) = log p(a=\pi^{\ast}(s) \vert s)
$$

($$\pi^{\ast}$$는 `Unknown Policy of Expert` 입니다. 즉 사람의 행동을 잘 모방하지 못하면 낮은 확률이 나오기 때문에 reward가 작음.)


여기서 우리는 $$r(s,a)$$에 대해서 Policy 하에 기대값을 취하는데 여기서 Policy는 Expert의 Policy가 아니라 Learned Policy이며, 우리가 원하는 것은 Expert의 Action들을 우리가 시뮬레이션을 돌리면서 실제 방문한 State하에서 매칭시키고 싶은 것입니다.
바로 이 때문에 Behavior Cloning 방법이 제대로 된 Objective Function을 최적화 하는게 아닌게 되는건데요, Behavior Cloning이 Expert의 State나 Observation의 log likelihood들의 기대 값의 합을 최대화 하기 때문입니다. (Distributional Mismatch)

바로 이 문제를 해결하기 위해서 `DAgger`가 제안된 거죠.



이를 좀 더 분석해 보도록 하겠습니다.

![slide35](/assets/images/CS285/lec-2/slide35.png)
*Slide. 35.*

Trajectory의 총 길이를 $$T$$라고 하고, Cost Function을 아래와 같이 정의하겠습니다.

$$
c(s,a)= 
\left\{\begin{matrix}
0 \space if \space a=\pi^{\ast}(s)
\\ 
1 \space otherwise
\end{matrix}\right.
$$

즉 어떤 상태에서 사람이랑 똑같은 행동을 하면 손실이 0이고 아니면 1입니다. (손실이 클수록 안좋습니다.)

$$
min_{\theta} \mathbb{E}_{s_{1:T},a_{1:T}} [ \sum_t c(s_t,a_t)]
$$

위의 수식은 어떤 Trajectory를 따랐을 때 얼마나 실수했는지에 대해서 count하는게 되겠군요.


우리가 분석 해 보고 싶은것은 위의 수식의 Upper Bound인데요, 이를 외줄타기를 하는 사람 (Tightrope Walker) 처럼 극단적으로 생각해 보겠습니다.
한번 줄에서 떨어지면 그 뒤는 없는 것 처럼 (아앗...), 한번 경로를 이탈하면 그 뒤로는 어떻게 할 줄 몰라서 뒤에도 다 실수했다고 counting 하는 것처럼 계산을 해 보는 겁니다.

$$
\mathbb{E}[\sum_t c(s_t,a_t)] \leq  \epsilon T + (1-\epsilon) (\epsilon(T-1) + (1-\epsilon)(\cdots))
$$

위의 수식에서 $$\epsilon$$는 

$$
assume : \pi_{\theta}(a  \neq \pi^{\ast}(s) \vert s) \leq \epsilon 
$$

라는 가정에서 왔으며, 이는 Training Dataset에 있는 상태들 ($$s \in D_{train}$$)에서 실수를 저지를 확률은 굉장히 작다는 의미입니다. (여기서 $$\epsilon$$이 작은 값이며, 지도 학습을 했기 때문에 우리가 본 상황에 대해서 어떤 행동을 할지를 잘 맞추는건 어찌보면 당연합니다.)


수식을 다시 생각해보면, 우리가 처음 외줄타기를 시작할 때 발을 헛딛을 확률은 엄청 작겠죠? (마찬가지로 강화학습 알고리즘도 처음 시작할 때 (at initial state)는 이상한 행동을 할 확률이 적을겁니다.)

그래도 만약 엄청나게 작은 확률, $$\epsilon$$으로 실수한다고 하면 줄에서 떨어지게 될거고, 그 뒤의 time-step $$T$$ 동안은 죽은거나 다름 없기 때문에 아래처럼 $$\epsilon T$$가 되고,

$$
\mathbb{E}[\sum_t c(s_t,a_t)] \leq  \epsilon T 
$$

그리고 $$(1-\epsilon)$$과 나머지를 곱한걸 더하면 다음과 같이 나타낼 수 있습니다.

$$
\mathbb{E}[\sum_t c(s_t,a_t)] \leq  \epsilon T + (1-\epsilon) (\qquad \qquad \qquad)
$$

여기서 우변의 괄호 안의 수식을 생각해보면 (두 번째 time-step이 될 것)

$$
\mathbb{E}[\sum_t c(s_t,a_t)] \leq  \epsilon T + (1-\epsilon) (\epsilon(T-1) \qquad \qquad)
$$

$$
\mathbb{E}[\sum_t c(s_t,a_t)] \leq  \epsilon T + (1-\epsilon) (\epsilon(T-1) + (1-\epsilon)(\cdots))
$$

위와 같은 수식으로 전개를 할 수 있습니다.


이 수식은 $$T$$개의 term으로 이루어져있고, 각 텀들은 $$O(\epsilon T)$$를 가지게 되기 때문에 total bound는 $$O(\epsilon T^2)$$가 됩니다.
이는 `굉장히 좋지 않은 bound`라고 하는데요, 왜냐하면 Trajectory의 길이가 늘어날수록 Error가 제곱해서 (quadratically) 증가하기 때문입니다.
바로 이런 이유에서 `Naive Behavior Cloning`이 이론적으로 좋지 않다는 것입니다.



![slide36](/assets/images/CS285/lec-2/slide36.png)
*Slide. 36.*

하지만 방금 전의 분석은 우리가 Training Dataset에 포함된 State에 대해서만 생각했기 때문에 일반화된 분석이 아니었는데,
이번에는 좀 더 일반적인 분석을 해보면 아래와 같이 되는데,

![slide37](/assets/images/CS285/lec-2/slide37.png)
*Slide. 37.*

중요한 점은 더욱 일반적인 가정 하에 error bound를 구해보면 여전히 $$O(\epsilon T)$$가 나오며, DAgger를 사용해서 $$p_{train}(s) \approx p_{\theta}(s)$$가 되면 Error Bound는 quadratic가 되지 않는다는 것이죠.

$$
\mathbb{E} [ \sum_t (s_t,a_t) ] \geq \epsilon T
$$

즉, DAgger를 사용하지 않고 Behavioral Cloning을 하는 것은 어쨌든 Distributional Shift가 일어나기 쉽다는 거죠.










## <mark style='background-color: #fff5b1'> Another way to imitate </mark>

이번에는 다른 Imitation Learning 기법에 대해서 소개를 하는데요,
key idea는 주어진 데이터가 어떤 한 task에 대해서는 optimal이 아닐 수 있지만, 다른 task에 대해서는 optimal 할 수도 있다는 겁니다. 

![slide39](/assets/images/CS285/lec-2/slide39.png)
*Slide. 39.*

어떤 Trajectory가 $$p_1$$ 지점에 도달 하는 것이고, 이러한 경로가 많이 있다고 생각해 보도록 하겠습니다.
이는 $$p_1$$에 도달하는 방법을 학습하기에는 좋을 수 있습니다. 


하지만 각각의 Trajectory가 전부 다른 point에 도달하는 경우를 생각해보면 어떤 한 지점에 도달하는 정책을 학습하기에 충분한 데이터가 없다고 생각 할 수 있죠.
이런 경우 만약에 policy $$\pi$$에 도착 지점을 condition하게 되면 $$p_1$$으로 가는 데이터가 (Trajectory) 부족할 지라도 $$p_1$$으로 가는 방법을 배울 수 있습니다.


![slide40](/assets/images/CS285/lec-2/slide40.png)
*Slide. 40.*

이를 위와같은 `Goal-conditioned Behavioral Cloning`이라고 합니다.
이러한 방법론은 아래 예시에서 사용되었는데요, 해당 task에서는 정해지지 않은 행동들을 하면서 데이터들을 (Trajectories) 모으고 
해당 Trajectory의 마지막 장면을 `Goal`이라고 정해서 앞서 설명한 `Latent Vairable Model`과 `Autoregressive Discretization`을 합친 모델로 학습해서 특정 Goal에 도달하기 위한 Policy를 잘 학습한 모습을 보여줍니다.

![slide41](/assets/images/CS285/lec-2/slide41.png)
*Slide. 41.*

![slide42](/assets/images/CS285/lec-2/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-2/slide43.png)
*Slide. 43.*

(궁금하신 분들은 강의에서 영상을 보시길 바랍니다.) 즉 위의 논문이 시사하는 바는 어떤 정해진 하나의 point로 향하는 task를 달성하지는 못할 수도 있지만 (데이터가 모자라면) 다양한 goal을 (tasks) 하기 위한 정책을 학습할 수는 있다는 겁니다.


더 관심있으신 분들은 아래의 논문을 검색해서 보시면 될 것 같습니다.


![slide44](/assets/images/CS285/lec-2/slide44.png)
*Slide. 44.*

논문에서는 휴먼 데이터는 전혀 사용하지 않았고, random policy로 시작해서 random data을 모은 뒤, 이 데이터가 어떤 goal에 도착하던지 이를 re-label 한 다음 이 goal을 condition한 정책을 학습했습니다. 이런식으로 학습한 policy이 굉장히 잘 된다고 본인의 논문을 홍보(...)하면서 2장을 마쳤습니다.


긴 글 읽어주셔서 감사합니다.



## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








