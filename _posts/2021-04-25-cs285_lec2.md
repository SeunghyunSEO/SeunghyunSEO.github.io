---
title: (yet) Lecture 2 - Supervised Learning of Behaviors

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 2의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=HUzyjOsd2PA&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=5)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-2.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 강의의 주제는 "Supervised Learning of Behaviors" 입니다. 아마 본 class의 큰 주제가 `심층 강화 학습 (Deep Reinforcement Learning)`인 만큼 이번 장에서는 `딥 러닝 (Deep Learning)` 기법에 대해 얘기를 하면서 강화 학습과 연관지어 개념을 전개할 것 같습니다.

![slide1](/assets/images/CS285/lec-2/slide1.png)
*Slide. 1.*

## <mark style='background-color: #fff5b1'> Terminology & notation </mark>

우선 `Terminology` , `Notation` 에 대해서 알아볼 건데, 컴퓨터 비젼 (Computer Vision) task 를 가정하고 전개를 한다고 합니다.  

![additive_fig_1](/assets/images/CS285/lec-2/additive_fig_1.png){: width="80%"}
*Additive Fig.*

일반적으로 사물 인식 (Object Recognition) 같은 task 에서는  

- Input : $$x$$
- Mapping Function : $$p(y \vert x)$$ ($$f(x)$$)
- Output : $$y$$

같은 term으로 표현할텐데, 본 class에서는 강화학습을 고려하기 때문에,  

- Input : $$o$$ (observation)
- Mapping Function : $$ \pi_{\theta} (a \vert o) $$ (policy, function)
- Output : $$a$$ (action)

라고 표현을 하는 것 같습니다. (딥 러닝의 일반적인 표현에서 점진적으로 강화 학습의 표현을 사용하면서 자연스럽게 둘을 융화시키는 것 같네요.)


위의 `teminology`를 조금 더 생각해 볼까요?

![additive_fig_2](/assets/images/CS285/lec-2/additive_fig_2.png){: width="80%"}
*Additive Fig.*
 
강화 학습이란 기본적으로 "sequential decision making problem" 이기 때문에 $$o \rightarrow a$$ 같이 입력(상태) 을 받아, 그 상황에서의 적절한 행동(행동)을 하는 것이 매 순간 일어납니다.

- Input : $$o_t$$ (observation)
- Mapping Function : $$ \pi_{\theta} (a_t \vert o_t) $$ (policy, function)
- Output : $$a_t$$ (action)

그래서 위와 같이 시간 $$t$$ 를 나타내는 아랫 첨자 (subscript)를 추가합니다 (기본적으로 강화학습에서는 discreter time-step을 사용합니다).


그리고 일반적인 딥 러닝의 지도 학습과는 다르게, 강화 학습에서는 어떤 시점의 output이 그 다음 시점의 input에 영향을 미칩니다
($$a_t$$가 $$o_{t+1}$$에 영향을 줌).


그러니까 만약 호랑이라는 걸 제대로 인식하지 못하면 그게 영향을 미쳐서 그 다음엔 조금 더 나에게 가까워진 호랑이를 볼 (observe) 수 있는거죠.  

이 아이디어를 조금 더 확장해서 생각해 보겠습니다. Mapping Function $$ \pi_{\theta} (a_t \vert o_t) $$ 가 출력하는 $$a_t$$가 가령 object에 대한 간단한 label이 아니라 action 이라고 생각해보겠습니다 (물론 discrete한 액션). 그렇게 하는 편이 행동을 하는 주체를 policy function에 따라서 자연스럽게 control 할  있기 때문이죠.

![additive_fig_3](/assets/images/CS285/lec-2/additive_fig_3.png){: width="80%"}
*Additive Fig.*

![additive_fig_4](/assets/images/CS285/lec-2/additive_fig_4.png){: width="80%"}
*Additive Fig.*

(물론 출력 분포가 discrete (Categorical) 하지 않고 continuous할 수도 있으며, 그럴 경우 만약 출력 분포가 가우시안 분포라면 평균 (mean), 분산 (variance)를 출력하게 되겠죠.) 


![slide2](/assets/images/CS285/lec-2/slide2.png)
*Slide. 2.*

여기에 추가적으로 추가되는 term에는 상태를 나타내는 $$s_t$$와 이를 given으로 output을 예측하는 $$\pi_{\theta} (a_t \vert s_t)$$가 있는데요,
$$o_t$$와 $$s_t$$의 차이는 $$s_t$$ 마르코프 상태를 가정했을 때 마르코프 그래프의 노드를 나타내는 "상태", 이고 $$o_t$$는 이러한 $$s_t$$로 부터 얻어내는 것 입니다. 그래서 보통은 $$ \pi_{\theta} (a \vert o) $$를 사용하지만 좀 더 엄격한 (restrictive) 상황에서는 $$\pi_{\theta} (a_t \vert s_t)$$를 사용하기도 한다고 합니다.


두 가지 차이에 대해 조금 더 얘기해 보도록 하겠습니다.

예를 들어 치타가 가젤을 쫓아가는 이미지가 입력으로 들어왔다고 해보겠습니다. 또한 이미지는 픽셀로 이루어져 있죠.
이러한 이미지를 통해서 치타와 가젤의 위치가 각각 상대적으로 어디에 있으며, 이를 통해 치타가 가젤을 쫓아가는 것을 유추할 수 있죠.

![additive_fig_5](/assets/images/CS285/lec-2/additive_fig_5.png){: width="50%"}
*Additive Fig.*

이게 `Observation` 입니다.

![additive_fig_6](/assets/images/CS285/lec-2/additive_fig_6.png){: width="50%"}
*Additive Fig.*

`State`는 근본적인 물리계의 정보를 말하는데요, 예를 들면 절대적인 위치 (position) 이나 물체의 속도 (velocity) 같은 것들이 되겠습니다. 
(State는 minimal representation을 포함하고 있습니다.)

지금은 두 가지의 차이가 명확해 보이지 않는데, 만약 어떤 자동차가 지나가면서 치타를 가렸다고 생각해보면 이야기는 달라집니다.

![additive_fig_7](/assets/images/CS285/lec-2/additive_fig_7.png){: width="50%"}
*Additive Fig.*

위의 경우에 Observation에는 치타가 안 보이지만 State는 그렇지 않죠. 
즉, State는 실제 정보 (True Configuration)을 나타내는 것이고 Observation은 이것의 일 부분 (이미지) 인 것입니다. (Observation은 State를 추론하기에 (deduce) 충분할 수도, 아닐 수도 있습니다.)


이를 간단한 Graphical Model로 생각해보자면 아래와 같은데 (*Slide. 3.*의 아래 부분)

![slide3](/assets/images/CS285/lec-2/slide3.png)
*Slide. 3.*

이를 조금 더 살펴보자면,

![additive_fig_8](/assets/images/CS285/lec-2/additive_fig_8.png){: width="80%"}
*Additive Fig.*

State, $$s_t$$에서 얻어낸 Observation, $$o_t$$와 이를 given으로 어떤 행동, $$a_t$$을 할 지를 정하고, 이를 행동의 영향으로 인해 State가 바뀝니다. 

![additive_fig_9](/assets/images/CS285/lec-2/additive_fig_9.png){: width="80%"}
*Additive Fig.*

가 아니라, 내가 어떤 행동을 했을때 사실 상태가 변할지는 확률 $$p(s_{t+1} \vert s_t, a_t)$$에 따라 결정됩니다.
(즉 상태가 안변하고 그대로 일 확률도 있다는 거죠, 예를 들어 내가 호랑이를 보고 무시했더니 호랑이도 날 똑같이 무시해서 교착 상태에 빠지는...)

또한 Markov 상태를 가정한 경우, 중요한 가정을 특성이 있는데요, 이는 $$s_{t+1}$$ 상태를 결정하는 데에는 $$s_{t}$$ 이전의 상태는 필요 없고 (conditionally independent) 오직 $$s_{t}$$만 필요하다는 겁니다. 그리고 $$s_t$$ 만으로 모든 걸 합리적으로 결정할 수 있다는 겁니다. 이는 sequential decision making을 하는 강화 학습에 있어 굉장히 중요한 특성이라고 하는데요, 이러한 특성 (property) optimal policies를 정확하게 나타낼 (formulate) 수 없기 때문이라고 합니다.


Observation을 사용하냐 State를 사용하냐는 Markov Property를 만족하느냐 (과연 현재 (present) 상태만으로 미래 (future)를 제대로 판단할 수 있느냐) 하는 부분에 있어 차이가 있는데요, 위의 치타가 가젤을 쫓는 경우를 생각해 봤을 때, 치타가 차에 가려진 어떤 시점에서는 그 시점의 Observation 정보로는 제대로 된 판단을 내릴 수가 없습니다. 그렇기 때문에 이럴 경우에는 과거의 정보 (past Observation)를 기억해서 (Memorize) 사용 할 수 있다고 하는데, State를 사용하는 경우에는 Markov Property를 만족하기 때문에 그럴 필요가 없다고 합니다.

```
Memorize 한다는 것이 Memory에 이전 time-step을 저장하고 현재 time-step의 몇개 전 까지의 정보를 같이 condition 하겠다는 건지 , 아니면 RNN 같은 모듈을 사용하겠다는건지 지금은 잘 모르겠습니다.
```

Lecturer는 앞으로 이 class에서는 대부분 State를 사용하는 $$\pi_{\theta} (a_t \vert s_t)$$를 policy로 사용하겠다고 합니다.
혹은 종종 이러한 non-Markovian Observation 들을 사용하는 알고리즘들도 소개할 예정이라고 합니다.





![slide4](/assets/images/CS285/lec-2/slide4.png)
*Slide. 4.*

위의 슬라이드는 어떤 term을 사용해도 상관 없다는 내용입니다.
왼쪽은 벨만에 의해 많이 연구된 `Dynamic Programming`의 term들인데, 이걸 써도 되고 오른쪽의 `Robotics` 분야의 term을 써도 된다고 하네요. 


## <mark style='background-color: #fff5b1'> Imitation Learning </mark>

이제 위에서 term들을 정의했으니 딥러닝의 `Image Classifier`에 해당하는 강화학습의 `정책 (Policy)`을 어떻게 학습 할 것인가에 대해서 간단하게 얘기하려고 합니다.
크게 다를 바가 없는데요, 물론 복잡한 강화학습의 방법론들이 존재하지만 이번 장에서는 간단한 방법으로 직관적인 이해를 더하려고 합니다.

![slide5](/assets/images/CS285/lec-2/slide5.png)
*Slide. 5.*

호랑이 한테서 도망가는 예제 말고, 자율 주행 task를 생각 해 보도록 하겠습니다.
자율 주행 task에서는 카메라의 매 프레임이 Observation이 되겠고, 이에 따라 운전대를 어떻게 돌릴지 (행동)를 결정하면 되겠습니다. 
딥러닝의 `지도 학습 (Supervised Learning)`을 생각 해 볼까요? 간단하게 매 프레임 마다 사람이 어떻게 운전대를 control 했는지에 대한 정답 label이 존재 할 수 있다고 생각하면 간단합니다. 


이걸 바로 `Imitation Learning` 혹은 `Behavior Cloning`이라고 합니다. 
일반적으로 전문가 (Expert)의 행동을 따라 (Clone) 하게 되기 때문입니다.


근데 이러한 방법론이 잘 작용할까요?

![slide6](/assets/images/CS285/lec-2/slide6.png)
*Slide. 6.*

사실 이러한 방법은 (DNN 사용하기에 Deep Imitation Learning) 1989에 제안된 방법이며 그런대로 잘 작동해서 미 대륙 횡단을 해보려는 시도까지 했었다고 합니다.


![slide7](/assets/images/CS285/lec-2/slide7.png)
*Slide. 7.*

하지만 이렇게 일반적으로 딥러닝의 지도 학습에서 사용되는 방법론이 대부분의 강화학습에서 잘 작용하지는 않는데요,
이는 *Slide. 7.*의 두 `궤적 (trajectory)`을 보면 알 수 있습니다.

그래프가 의미하는 바는, 시간이 흐를수록 state value가 변한다는 것인데요, Policy $$\pi_{\theta}$$를 학습하기 위한 학습 데이터의 실제 궤적 (training trajectory)이 (검은색)과 같을 때, $$\pi_{\theta}$$를 통해서 진행한 궤적 (빨간색)과 굉장히 다르다는 겁니다.
 
그림을 보시면 초반에는 궤적을 잘 따라가나 싶다가도, 중간에 사소한 실수 (mistake)를 하게 되면 맥을 못추고 다른 궤적을 그리는 걸 볼 수 있습니다. 
이는 사소한 실수를 통해서 전혀 본 적 없는 state에 빠지게 되면 policy가 이를 어떻게 처리할지를 모르기 때문에 잘못된 선택을 하게 되고, 이러한 실수가 쌓이면 학습이 어렵기 때문입니다.

![slide8](/assets/images/CS285/lec-2/slide8.png)
*Slide. 8. Animation*

하지만 이는 practical하게는 잘 작동한다는데요, (...? Sergey 선생님?)
데이터를 굉장히 많이 수집하고 약간의 tricks을 사용해서 잘 작용하게 만들 수 있다고 합니다.

![slide9](/assets/images/CS285/lec-2/slide9.png)
*Slide. 9.*

NVIDIA

![slide10](/assets/images/CS285/lec-2/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-2/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-2/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-2/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-2/slide14.png)
*Slide. 14.*


### <mark style='background-color: #dcffe4'> Deep imitation learning in practice </mark>


![slide16](/assets/images/CS285/lec-2/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-2/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-2/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-2/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-2/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-2/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-2/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-2/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-2/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-2/slide25.png)
*Slide. 25.*



## <mark style='background-color: #fff5b1'> A case study: trail following from  human demonstration data </mark>

![slide27](/assets/images/CS285/lec-2/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-2/slide28.png)
*Slide. 28.*



## <mark style='background-color: #fff5b1'> Cost functions, reward functions, and a  bit of theory </mark>

![slide30](/assets/images/CS285/lec-2/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-2/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-2/slide32.png)
*Slide. 32.*

![slide34](/assets/images/CS285/lec-2/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-2/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-2/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-2/slide37.png)
*Slide. 37.*



## <mark style='background-color: #fff5b1'> Another way to imitate </mark>

![slide39](/assets/images/CS285/lec-2/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-2/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-2/slide41.png)
*Slide. 41.*

![slide42](/assets/images/CS285/lec-2/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-2/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-2/slide44.png)
*Slide. 44.*



## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








