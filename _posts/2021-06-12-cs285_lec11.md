---
title: (미완) Lecture 11 - Model-Based Reinforcement Learning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---

이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)


Lecture 11의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=LkTmiylbHYk&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=47)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-11.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

Lecture 11에서 다룰 내용은 10장에 이어서 `Model-based RL` 입니다.

![slide1](/assets/images/CS285/lec-11/slide1.png)
*Slide. 1.*

지난 번 강의에서는 Model-based RL이 무엇인지에 대해 간단하게 알아보고, Model을 학습한 뒤에 이를 이용해서 Control을 하는 naive한 방법론들에 대해 알아봤습니다.
이번 장에서는 naive approach보다 더 잘 작동하는 알고리즘들에 대해서 배울 것이며, 학습 목표는 아래와 같습니다.

![slide2](/assets/images/CS285/lec-11/slide2.png)
*Slide. 2.*

"Next time"에서 알 수 있듯, 다음주에는 Model-based Policy learning approach까지 배우게 됩니다. 


## <mark style='background-color: #fff5b1'> Recap : Model-based RL and it's problem</mark>

Lecture 10에서 배웠던 내용이 *Slide. 3.*에 잘 나와있는데요,

![slide3](/assets/images/CS285/lec-11/slide3.png)
*Slide. 3.*

Model-based RL은 데이터로부터 `Deterministic Model`, $$f(s_t,a_t)=s_{t+1}$$이나 `Stochastic Model`, $$p(s_{t+1} \vert s_t,a_t)$$를 배워서 plan을 하는게 목표였습니다.
그리고 이러한 방법론의 가장 간단하지만 명백한 `version 0.5`에 대해서 정의하고 알아봤습니다.


이러한 naive approach가 잘 작동하는가?에 대한 대답은

![slide4](/assets/images/CS285/lec-11/slide4.png)
*Slide. 4.*

"yes" 였습니다. 
클래식한 로보틱스 같은 문제에 이런 간단한 알고리즘을 적용한 사례들이 있었습니다.
이러한 방법론은 하지만 `Deep Neural Network (DNN)`같은 High-Capacity `Model` (not Policy) 을 사용하는 경우에는 잘 작용하지 않았는데요,

![slide5](/assets/images/CS285/lec-11/slide5.png)
*Slide. 5.*

그 예로 *Slide. 5.* 에는 등산을 하는 Agent에 대한 예가 나와 있습니다.
가장 높은 곳에 도달하는게 목표인 Agent가 어떤 `base policy (random policy)`, $$\pi_0$$로 $$(s,a,a')$$ 데이터를 엄청 모은 뒤 이를 통해서 학습을 했습니다.
그리고 Agent가 이를 기반으로 planning을 하게 되면 "오른쪽으로 갈 수록 더 높아져서 reward가 커지는구나?"라는 생각을 가지고 $$\pi_k$$를 얻게 됩니다. (여기서 $$\pi_k$$는 신경망도 아니고 그저 플래닝)
하지만 그림에서 볼 수 있듯 계속 오른쪽으로가면 어떻게될까요? 

그렇습니다. 바로 절벽 아래로 떨어지고 말텐데요, 이러한  문제가 생기는 이유는 바로 우리의 model이 `방문했던 상태 (visited state)`를 기반으로한 action을 정할때만 유효하기 때문입니다.
이는 앞서 Lecture 2~4에서 살펴봤던 Imitation Learning의 `Distribution Shift`와 비슷한 문제인데요,

$$
p_{\pi_0}(s_t) \neq p_{\pi_k}(s_t)
$$ 

즉 Agent는 $$p_{\pi_0}(s_t)$$가 굉장히 높은 state에 대해서 action을 하다가 결국에는 굉장히 낮은 확률을 가지고 있는 state을 만나서 이상한 판단 (erroneous prediction)을 하게 되는 겁니다.
이렇게 나쁜 상태에서 나쁜 행동을 하고 그게 또 이어지고 하는 스노우볼이 굴러가서 갈피를 못잡게 되는거죠.


이러한 문제는 `more expressive model`, 즉 NN에서 더 큰 문제를 야기하는데, 이는 NN이 학습 데이터에 더욱 tight하게 데이터에 fit하기 때문입니다.
몇 개 파라메터가 안되는 로보틱스의 system identification과는 양상이 다른거죠.


그렇다면 어떻게 이 문제를 해결할 수 있을까요?

![slide6](/assets/images/CS285/lec-11/slide6.png)
*Slide. 6.*

Imitation Learning에서 비슷한 문제를 직면했을 때 사용했던 `DAgger`와 비슷하게 접근해서 
두 정책 하에서의 어떤 state, $$s_t$$에 있을 확률 분포를 같게하면 될까요?

DAgger는 간단하게 데이터를 더 모아서 학습하는 전략이었는데요,
이와 유사한 방법으로 개선시킨 `version 1.0`이 *Slide. 6.*에 나와있습니다.

```
여담이지만 이런 방식으로 Model을 retraining하는 방법이 DAgger보다 먼저 나왔다고 합니다.
```

아무튼 DAgger와 유사한 방법으로 수정해서 Dynamics를 배우는 알고리즘이 잘 되기는 하지만, 이보다 더 잘해낼 수 있는 방법들이 많이 있다고 합니다.

![slide7](/assets/images/CS285/lec-11/slide7.png)
*Slide. 7.*

절벽에서 떨어지는 경우에는 예를 들어, "아 일로가면 안됐는ㄷ..." 하고 생각해봐야 이미 떨어지는 중이기 때문에 되돌릴 방법이 없지만,
실제 강화학습이 적용되는 real world problem, 예를 들어 자율주행 같은 경우에는 실수를 바로잡을 수 있는 기회가 있다고 합니다.


바로 실수하는 순간에 `re-plan` 하는 전략입니다.

![slide8](/assets/images/CS285/lec-11/slide8.png)
*Slide. 8.*

*Slide. 8.*에는 바로 re-plan 전략을 사용한 `version 1.5`가 정의되어 있는데요, 이는 `Model Predictive Control (MPC)`라고 하며, 강의 막바지에 살짝 언급한다고 합니다.

아이디어는 

- Base Policy로 데이터를 수집한다.
- Model을 학습한다.
- 학습된 Model을 기반으로 Action을 쭉 Plan한다.
- 실제 관측되는 state들을 보고 Re-Plan한다.
- Plan하면서 축적된 데이터를 원래 Dataset, D에 붙혀서 Re-Training한다.
- 반복?

가 됩니다.

이렇게 하면 loop를 더 많이 돌기때문에 (수정하려고) 계산 복잡도가 상당히 증가하는 단점이 있습니다.
하지만 여기서 한가지 의문점이 들 수 있는데요, 바로 `"어떻게 Re-Plan 한다는 얘기지?"` 입니다.

![slide9](/assets/images/CS285/lec-11/slide9.png)
*Slide. 9.*

그다지 좋지 않은 불완전한 plan을 많이 해보고 많이 re-plan 할 수록 계산 비용이 늘어나기 때문에 3번 스텝을 조금만 진행하는 식으로 한다고 합니다.



## <mark style='background-color: #fff5b1'> Uncertainty in model-based RL  </mark>

이번 subsection에서는 `Model-Uncertainty`가 Model-based RL에서 어떤 역할을 하는지에 대해서 살펴보려고 하는데요,
불확실성 (Uncertainty)를 이해하고 적용하는게 Model-based RL에서는 중요하다고 합니다.

![slide11](/assets/images/CS285/lec-11/slide11.png)
*Slide. 11.*

*Slide. 11.*에서는 3년 전 (현재 2021년)인 2018년에 ICRA에서 발표한 BAIR의 논문 결과가 나와있는데요, 여기서 사용한 모델은 `version 1.5`같은 Deep Model-based RL 이라고 합니다.


그래프를 보시면 version 1.5 Model-based (Mb) 로 짧게 학습한게 500점의 reward를 얻었고, 10일동안 학습한 Model-free Method (Mf) 는 5000점을 얻었습니다. 그리고 Model-based로 한번 학습하고 Model-Free로 학습한 경우 (Mb-Mf) 는 더 높은 점수를 얻었는데요, 어쨌든 Mb로만 학습한 경우가 점수가 낮긴해도 치타 로봇이 앞으로 전진을 하기는 했다고 합니다. 

***

[Youtube Link](https://www.youtube.com/watch?v=G7lXiuEC8x0)를 보시면 Mb, Mf, Mb-Mf 세 가지의 데모 영상을 보실 수 있습니다.

***

![slide12](/assets/images/CS285/lec-11/slide12.png)
*Slide. 12.*

이 그림이 시사하는 바는 "Model-based method 가 구리다"라는게 아니라, "후반부로 갈수록 (데이터가 모일수록) 더 잘 학습할 여지가 있는 capacity를 가졌다", 그러니까 초기에 데이터가 적을 때 충분히 여러 state들을 탐색 (Exploration)해보지 못한 경우를 조금 더 잘하게 만들어주면 되지 않을까? 라는 것 입니다.

![slide13](/assets/images/CS285/lec-11/slide13.png)
*Slide. 13.*

"왜 데이터가 적지만, 우리 모델이 high-capacity일 때 (파라메터가 많은 NN) 퍼포먼스가 구린가?" 는 즉 `과적합 (Over-Fitting)` 문제라고 생각할 수 있습니다. 

![slide14](/assets/images/CS285/lec-11/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-11/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-11/slide16.png)
*Slide. 16.*



### <mark style='background-color: #dcffe4'> Uncertainty-Aware Neural Net Models </mark>

![slide18](/assets/images/CS285/lec-11/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-11/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-11/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-11/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-11/slide22.png)
*Slide. 22.*


### <mark style='background-color: #dcffe4'> Planning with Uncertainty, Examples </mark>

![slide24](/assets/images/CS285/lec-11/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-11/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-11/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-11/slide27.png)
*Slide. 27.*


### <mark style='background-color: #dcffe4'> Model-Based RL with Images </mark>

![slide29](/assets/images/CS285/lec-11/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-11/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-11/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-11/slide32.png)
*Slide. 32.*

![slide33](/assets/images/CS285/lec-11/slide33.png)
*Slide. 33.*

![slide34](/assets/images/CS285/lec-11/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-11/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-11/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-11/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-11/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-11/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-11/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-11/slide41.png)
*Slide. 41.*



## <mark style='background-color: #fff5b1'> Model-based RL with complex observations  </mark>



## <mark style='background-color: #fff5b1'> Next time: policy learning with model-based RL  </mark>





### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








