---
title: (미완)Self-Supervised Learning for Speech Representation
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---


이번 글에서는 자기지도학습(Self-Supervised Learning, SSL)이 무엇인지 알아보고, 자연어처리와 음성인식 분야에서 이 학습 방법이 어떻게 사용 되어왔는지,
그리고 2020년에 공개된 SSL을 통해서 음성인식 최고 성능(State-Of-The-Arts, SOTA)을 내서 화제가 됐던 논문 Facebook AI Research(FAIR)  `Wav2Vec 2.0`에 대해 알아보도록 하겠습니다. 
 
<img width="1125" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/108096805-bce5a680-70c4-11eb-8831-e621254e5ed0.png">
*Fig. Wav2Vec 2.0 Framework*
 
본격적으로 시작하기에 앞서, 글의 말미에 다루게 될 핵심적인 내용인 Wav2Vec2.0 논문에 대해 얘기해 보자면, 이 논문의 중요 포인트는 크게 두 가지로 볼 수 있습니다. 
이 두가지는 바로 

> 1. 50000시간의 데이터로 '입력-정답' pair 없이 자기지도학습을 한 뒤, '입력-정답' pair 데이터(960시간, Librispeehc dataset)로 지도 학습(Supervised Learning, SL) 파인튜닝을 했더니 SOTA 성능을 냈다.
> 2. 마찬가지로 SSL로 사전 학습을 한 뒤, 굉장히 적은 시간의 데이터(10분)로 파인튜닝을 했더니 음성인식을 어지간한 음성인식 네트워크들만큼 하더라.

인데요, 여기서 2번째가 시사하는 바는 굉장히 클 수가 있습니다.

이는 일반적으로 딥러닝에서 사용되는 '데이터를 주고 정답을 알려준 뒤 이를 맞추는 방향으로 학습'하는 지도 학습의 경우 데이터를 구성하는 비용이 굉장히 비싼데, 
특히 이는 음성인식에서 더 두드러지기 때문입니다.

음성 인식 데이터는 다른 도메인에 비해 annotation하기가 힘들어 상대적으로 더 적다고 볼 수 있는데, 
이 말은 즉, 만국 공통어라고 불리는 영어, 중국어등이 아니면 다른 나라 언어의 경우 더욱 데이터가 부족하다는 겁니다. 

따라서 딥러닝 방법으로 음성인식기를 만들기란 굉장히 어려운데, 여기서 2번이 시사하는 바는 만약 우리가 소수민족의 언어에 대한 음성인식기를 만든다고 해도, 그들이 뱉은 음성 데이터를 엄청 모아 SSL 방법으로 사전학습한 뒤 10분 정도의 pair 데이터만 있다면 그럴싸한 음성인식기를 만들어 낼 수 있다는 부분에서 굉장히 중요한 부분이라고 할 수 있을 것 같습니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> Why Self-Supervised Learning? </mark>

자 이제 본격적으로 자기지도학습에 대해서 이야기 해보도록 하겠습니다. 왜 이게 중요할까요?
앞서 말했지만, 다시 한번 말씀드리자면 딥러닝에서 사용되는 지도학습 방법론을 위한 데이터셋을 구축하기에 비용이 너무 많이 들기 때문입니다. 

![ssl2](https://user-images.githubusercontent.com/48202736/108098887-44341980-70c7-11eb-8027-1caa7b7545f5.png)

예를 들어 음성인식을 예로 들어보겠습니다. '음성<->정답 문장' pair가 1000시간이 있다고 생각해 보겠습니다.
이런 경우 사람이 직접 음성에 대해 받아쓰기를 했다거나, 스크립트를 배우들이 직접 읽는 방식으로 annotation이 되었겠죠.

우리는 annotation이 없는 음성 데이터는 더 많



![ssl3](https://user-images.githubusercontent.com/48202736/108098891-45fddd00-70c7-11eb-867d-754bdc8e0c20.png)
![ssl4](https://user-images.githubusercontent.com/48202736/108098893-45fddd00-70c7-11eb-91b6-6521e62ebdbb.png)
![ssl5](https://user-images.githubusercontent.com/48202736/108098896-46967380-70c7-11eb-8134-7e932e7062f8.png)

![ssl1](https://user-images.githubusercontent.com/48202736/108098880-4302ec80-70c7-11eb-9ae2-af9e0ba1f5e1.png)







### <mark style='background-color: #dcffe4'> Word2Vec </mark>

<img width="1126" alt="word2vec" src="https://user-images.githubusercontent.com/48202736/108096719-a8a1a980-70c4-11eb-833d-c210d9136d7b.png">
*Fig. Word2Vec model architectures from Google 2013. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.*


<img width="826" alt="word2vec_animation" src="https://user-images.githubusercontent.com/48202736/108102442-e524d380-70cb-11eb-8f88-50da23a07725.png">
*Fig. PCA Visualization of trained Word2Vec Network, 이미지 출처 : [link](https://projector.tensorflow.org/)*


![skipgram](https://user-images.githubusercontent.com/48202736/108102464-ece47800-70cb-11eb-9e8a-c79ab4a24930.png)
*Fig. Skip-gram*

![cbow](https://user-images.githubusercontent.com/48202736/108102472-ee15a500-70cb-11eb-9372-1e48b0ea7590.png)
*Fig. Continuous Back Of Words, CBOW*






### <mark style='background-color: #dcffe4'> BERT </mark>

<img width="981" alt="BERT" src="https://user-images.githubusercontent.com/48202736/108096746-adfef400-70c4-11eb-9829-05a58478be3a.png">
*Fig. Bidirectional Encoder Representation from Transformer, BERT from Google, 2018*


![BERT_jay1](https://user-images.githubusercontent.com/48202736/108104460-ae9c8800-70ce-11eb-8e69-522ab5617624.png)
![BERT_jay2](https://user-images.githubusercontent.com/48202736/108104469-b1977880-70ce-11eb-9dfa-dabb06a4337e.png)
![BERT_jay3](https://user-images.githubusercontent.com/48202736/108104473-b2c8a580-70ce-11eb-9f9a-68da2332a1cd.png)








### <mark style='background-color: #dcffe4'> Speech2Vec </mark>

<img width="1364" alt="speech2vec" src="https://user-images.githubusercontent.com/48202736/108096750-af302100-70c4-11eb-9cba-54aeefa418d8.png">
*Fig. The structures of Speech2Vec trained with skipgrams and cbow, respectively from Yu-An Chung(MIT CSAIL), 2018*




## <mark style='background-color: #fff5b1'> Contrastive Learning </mark>

### <mark style='background-color: #dcffe4'> Contrastive Predictive Coding </mark>

<img width="1230" alt="cpc" src="https://user-images.githubusercontent.com/48202736/108101659-e30e4500-70ca-11eb-96e5-30a79de0f5c3.png">
*Fig. Overview of Contrastive Predictive Coding(CPC) for Audio inputs from Deepmind, 2019*






### <mark style='background-color: #dcffe4'> SimCLR Framework </mark>








## <mark style='background-color: #fff5b1'> Former Wav2Vec Series</mark>

### <mark style='background-color: #dcffe4'> Wav2Vec </mark>

<img width="783" alt="wav2vec" src="https://user-images.githubusercontent.com/48202736/108096770-b5260200-70c4-11eb-9987-d7ee7dd73886.png">





### <mark style='background-color: #dcffe4'> VQ-Wav2Vec </mark>

<img width="1283" alt="vq-wav2vec" src="https://user-images.githubusercontent.com/48202736/108096787-b8b98900-70c4-11eb-884a-6f0f6718a1db.png">

<img width="1272" alt="vq-wav2vec_2" src="https://user-images.githubusercontent.com/48202736/108096795-ba834c80-70c4-11eb-9bda-d2daa78418a2.png">






## <mark style='background-color: #fff5b1'> Wav2Vec2.0 </mark>

<img width="1125" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/108096805-bce5a680-70c4-11eb-8831-e621254e5ed0.png">








## <mark style='background-color: #fff5b1'> Further Study </mark>

### <mark style='background-color: #dcffe4'> Unsupervised Cross-Lingual Representation Learning </mark>

<img width="1330" alt="multilingual" src="https://user-images.githubusercontent.com/48202736/108098859-3b434800-70c7-11eb-82b3-537e077e8e61.png">






## <mark style='background-color: #fff5b1'> References </mark>

- 1.[The Illustrated Self-Supervised Learning from Amit Chaudhary](https://amitness.com/2020/02/illustrated-self-supervised-learning/)

- 2.[The Illustrated SimCLR Framework from Amit Chaudhary](https://amitness.com/2020/03/illustrated-simclr/)

- 3.[Self-Supervised Representation Learning from Lillian Weng](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html#why-self-supervised-learning)

- 4.[Learning Word Embedding from Lillian Weng](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#noise-contrastive-estimation-nce)

- 5.[The Illustrated Word2vec from Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)

- 6.[The Illustrated BERT, ELMo, and co. from Jay Alammar](https://jalammar.github.io/illustrated-bert/)
