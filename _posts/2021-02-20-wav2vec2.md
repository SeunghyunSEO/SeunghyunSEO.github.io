---
title: (미완)Wav2Vec 2.0
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---


이번 글에서는 음성인식 분야에서 자기지도학습(Self-Supervised Learning, SSL)을 통해서 음성인식 최고 성능(State-Of-The-Arts, SOTA)을 내서 화제가 됐던
 2020년에 공개된 논문 Facebook AI Research(FAIR) `Wav2Vec 2.0`에 대해 알아보도록 하겠습니다. 
 
<img width="1125" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/108096805-bce5a680-70c4-11eb-8831-e621254e5ed0.png">
*Fig. Wav2Vec 2.0 Framework*
 
본 논문의 중요 포인트는 크게 두 가지로 볼 수 있는데요, 이 두가지는 

> 1. 50000시간의 데이터로 '입력-정답' pair 없이 자기지도학습을 한 뒤, '입력-정답' pair 데이터(960시간, Librispeehc dataset)로 지도 학습(Supervised Learning, SL) 파인튜닝을 했더니 SOTA 성능을 냈다.
> 2. 마찬가지로 SSL로 사전 학습을 한 뒤, 굉장히 적은 시간의 데이터(10분)로 파인튜닝을 했더니 음성인식을 어지간한 음성인식 네트워크들만큼 하더라.

입니다.

여기서 2번째가 시사하는 바는 굉장히 클 수가 있는데요, 이는 일반적으로 딥러닝에서 사용되는 '데이터를 주고 정답을 알려준 뒤 이를 맞추는 방향으로 학습'하는 지도 학습의 경우 데이터를 구성하는 비용이 굉장히 비싼데, 
특히 이는 음성인식에서 더 두드러지기 때문입니다.

음성 인식 데이터는 다른 도메인에 비해 annotation하기가 힘들어 상대적으로 더 적다고 볼 수 있는데, 
이 말은 즉, 만국 공통어라고 불리는 영어, 중국어등이 아니면 다른 나라 언어의 경우 더욱 데이터가 부족하다는 겁니다. 

따라서 딥러닝 방법으로 음성인식기를 만들기란 굉장히 어려운데, 여기서 2번이 시사하는 바는 만약 우리가 소수민족의 언어에 대한 음성인식기를 만든다고 해도, 그들이 뱉은 음성 데이터를 엄청 모아 SSL 방법으로 사전학습한 뒤 10분 정도의 pair 데이터만 있다면 그럴싸한 음성인식기를 만들어 낼 수 있다는 부분에서 굉장히 중요한 부분이라고 할 수 있을 것 같습니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> Why Self-Supervised Learning? </mark>

![ssl1](https://user-images.githubusercontent.com/48202736/108098880-4302ec80-70c7-11eb-9ae2-af9e0ba1f5e1.png)
![ssl2](https://user-images.githubusercontent.com/48202736/108098887-44341980-70c7-11eb-8027-1caa7b7545f5.png)
![ssl3](https://user-images.githubusercontent.com/48202736/108098891-45fddd00-70c7-11eb-867d-754bdc8e0c20.png)
![ssl4](https://user-images.githubusercontent.com/48202736/108098893-45fddd00-70c7-11eb-91b6-6521e62ebdbb.png)
![ssl5](https://user-images.githubusercontent.com/48202736/108098896-46967380-70c7-11eb-8134-7e932e7062f8.png)







### <mark style='background-color: #dcffe4'> Word2Vec </mark>

<img width="1126" alt="word2vec" src="https://user-images.githubusercontent.com/48202736/108096719-a8a1a980-70c4-11eb-833d-c210d9136d7b.png">





### <mark style='background-color: #dcffe4'> BERT </mark>

<img width="981" alt="BERT" src="https://user-images.githubusercontent.com/48202736/108096746-adfef400-70c4-11eb-9829-05a58478be3a.png">





### <mark style='background-color: #dcffe4'> Speech2Vec </mark>

<img width="1364" alt="speech2vec" src="https://user-images.githubusercontent.com/48202736/108096750-af302100-70c4-11eb-9cba-54aeefa418d8.png">



## <mark style='background-color: #fff5b1'> Former Wav2Vec Series</mark>

### <mark style='background-color: #dcffe4'> Wav2Vec </mark>

### <mark style='background-color: #dcffe4'> VQ-Wav2Vec </mark>





## <mark style='background-color: #fff5b1'> Wav2Vec2.0 </mark>

### <mark style='background-color: #dcffe4'> Contrastive Learning (SSL) </mark>






## <mark style='background-color: #fff5b1'> Further Study </mark>






## <mark style='background-color: #fff5b1'> References </mark>
