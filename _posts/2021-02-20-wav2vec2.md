---
title: (미완) A Long Way to Wav2Vec Series - Self-Supervised Learning for Speech Representation
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---


이번 글에서는 자기지도학습(Self-Supervised Learning, SSL)이 무엇인지 알아보고, 자연어처리와 음성인식 분야에서 이 학습 방법이 어떻게 사용 되어왔는지,
그리고 2020년에 공개되어 음성인식 최고 성능(State-Of-The-Arts, SOTA)을 내서 화제가 됐던 음성인식계의 대표적인 SSL Method를 제안한 Facebook AI Research(FAIR)의 논문 `Wav2Vec 2.0`에 대해 알아보도록 하겠습니다. 
 
<img width="1125" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/108096805-bce5a680-70c4-11eb-8831-e621254e5ed0.png">
*Fig. Wav2Vec 2.0 Framework*
 
본격적으로 시작하기에 앞서, 글의 말미에 다루게 될 핵심적인 내용인 Wav2Vec2.0 논문에 대해 얘기해 보자면, 이 논문의 중요 포인트는 크게 두 가지로 볼 수 있습니다. 
이 두가지는 바로 

> 1. 50000시간의 데이터로 '입력-정답' pair 없이 자기지도학습을 한 뒤, '입력-정답' pair 데이터(960시간, Librispeehc dataset)로 지도 학습(Supervised Learning, SL) 파인튜닝을 했더니 SOTA 성능을 냈다.
> 2. 마찬가지로 SSL로 사전 학습을 한 뒤, 굉장히 적은 시간의 데이터(10분)로 파인튜닝을 했더니 음성인식을 어지간한 음성인식 네트워크들만큼 하더라.

인데요, 여기서 2번째가 시사하는 바는 굉장히 클 수가 있습니다.

이는 일반적으로 딥러닝에서 사용되는 '데이터를 주고 정답을 알려준 뒤 이를 맞추는 방향으로 학습'하는 지도 학습의 경우 데이터를 구성하는 비용이 굉장히 비싼데, 
특히 이는 음성인식에서 더 두드러지기 때문입니다.

음성 인식 데이터는 다른 도메인에 비해 annotation하기가 힘들어 상대적으로 더 적다고 볼 수 있는데, 
이 말은 즉, 만국 공통어라고 불리는 영어, 중국어등이 아니면 다른 나라 언어의 경우 더욱 데이터가 부족하다는 겁니다. 

따라서 딥러닝 방법으로 음성인식기를 만들기란 굉장히 어려운데, 여기서 2번이 시사하는 바는 만약 우리가 소수민족의 언어에 대한 음성인식기를 만든다고 해도, 그들이 뱉은 음성 데이터를 엄청 모아 SSL 방법으로 사전학습한 뒤 10분 정도의 pair 데이터만 있다면 그럴싸한 음성인식기를 만들어 낼 수 있다는 부분에서 굉장히 중요한 부분이라고 할 수 있을 것 같습니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> Why Self-Supervised Learning? </mark>

자 이제 본격적으로 자기지도학습에 대해서 이야기 해보도록 하겠습니다. 왜 이게 중요할까요?
앞서 말했지만, 다시 한번 말씀드리자면 딥러닝에서 사용되는 `지도학습 방법론을 위한 데이터셋을 구축하기에 비용이 너무 많이 들기 때문`입니다. 

![ssl2](https://user-images.githubusercontent.com/48202736/108098887-44341980-70c7-11eb-8027-1caa7b7545f5.png)

예를 들어 이미지 분류를 예로 들어보겠습니다. '이미지<->정답' pair가 10000개 있다고 생각해 보겠습니다.
이런 pair데이터들을 보통 annotation되어 있다고 이야기 합니다.

어쨌든 지도 학습을 생각해보겠습니다. 
우리가 네트워크를 '이미지<->정답' pair를 가지고 학습을 할 경우, 이미지를 잘 표현하는 특징을 추출하는 Representation(보통 Encoder가 추출)을 학습을 통해 배우게 되고, 
이를 최종적으로 분류층(Classifier)이 분류를 하게 됩니다.

우리는 하지만 annotation이 없는 개, 고양이 사진들도 엄청 많이 가지고 있을겁니다.
바로 이러한 `널려있는 annotation 되어있지 않은 이미지들만 가지고 Representation을 잘 학습할 수는 없을까?`라는 생각을 토대로 고안된 방법이 자기지도학습입니다.

![ssl4](https://user-images.githubusercontent.com/48202736/108098893-45fddd00-70c7-11eb-91b6-6521e62ebdbb.png)

다른 말로 이는 `데이터들을 스스로 annotation 한다`고 생각할 수도 있습니다.

어쨌든 이렇게 방대한 annotation되지 않은 입력 데이터들만을 가지고 스스로 정답이라고 정의한것을 학습하면서 Representation를 Encoder 네트워크가 배우게 되고
이렇게 학습된 (보통 `사전 학습(pre-training)` 했다고 표현합니다) 네트워크를 내가 원하는 task에 해당하는 소량의 데이터로(여유가 된다면 이번에도 대량의 데이터로) 지도학습을 해 주면 (이를 `미세조정(fine-tuning)` 한다고 합니다.) 우리는 소량의 데이터만을 가지고 지도학습을 한 경우보다 훨씬 좋은 성능의 네트워크를 얻을 수 있게 되는겁니다.

![ssl5](https://user-images.githubusercontent.com/48202736/108098896-46967380-70c7-11eb-8134-7e932e7062f8.png)

지금까지는 이미지를 다루는 Computer Vision task에 대해서 예시로 들어봤습니다. 아마 알듯말듯 하신 분들도 다시 생각해보면 이미 이러한 SSL방법에 대해서 이미 익숙하실겁니다. 
바로 딥러닝을 접하고 이것저것 읽다보면 배우게 되는 Word2Vec이나 자연어처리 분야의 SOTA 기법들인 (BERT, GPT)등이 모두 이러한 방법론을 따르기 때문입니다.

![ssl1](https://user-images.githubusercontent.com/48202736/108098880-4302ec80-70c7-11eb-9ae2-af9e0ba1f5e1.png)

이제 이러한 SSL와 비슷하게 정답 pair 없이 Representation을 학습하는 네트워크들에 대해서 간단하게 알아보도록 하겠습니다. 






### <mark style='background-color: #dcffe4'> Word2Vec </mark>

2013년에 구글에서 제안된 Word2Vec은 뉴럴네트워크를 사용해서 자기지도학습 방법으로 Word Representation을 학습하는 방법을 제안한 논문입니다.

엄밀히 말하면 Word2Vec은 Self-Supervised Learning으로 분류하지는 않고 Unsupervsied라고 하는 것 같은데요, Wav2Vec까지 가는 과정에서 Speech2Vec이 나오는 등 관련이 많으며, SSL이나 Unsupervised나 유사한 방법론이라고 생각하기 때문에 살짝 다루고 넘어가도록 하겠습니다.

이는 2018년에 제시되어 현재까지도 대부분의 자연어처리 (Natural Language Processing, NLP) task의 Encoder단에서 Representation을 뽑아주는 feature extractor로 쓰이는 [BERT](https://arxiv.org/pdf/1810.04805)와 유사한 역할을 하는(어떻게 학습을 통해 Representation을 얻어내는가는 다릅니다) 학습 방법론 인겁니다.
더 거슬러 올라가서는 차원을 축소하고 좋은 Representation을 배우기 위한 Encoder를 학습하기 위한 AutoEncoding 방법론들도 이와 크게 다르지 않습니다.


<img width="1126" alt="word2vec" src="https://user-images.githubusercontent.com/48202736/108096719-a8a1a980-70c4-11eb-833d-c210d9136d7b.png">
*Fig. Word2Vec model architectures from Google 2013. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.*

Word2Vec 논문에서 제안된 방법은 주변 단어들을 통해서 타깃이 되는 가운데 단어 하나를 예측하는 `CBOW(Continuous Back Of Words)` 방법과 `Skip-gram` 방법 두 가지가 있으며,
이런식으로 학습 된 단어들은 유사한 의미(semantic)를 가지는 단어일 수록 같은 곳에 뭉치게 되는 현상을 보여주게 됩니다.  

(어떻게 뭉치는지에 대한 결과물은 Representation을 3차원으로 projection해서 보여주는 아래의 사이트를 통해 확인하실 수 있습니다.)

<img width="826" alt="word2vec_animation" src="https://user-images.githubusercontent.com/48202736/108102442-e524d380-70cb-11eb-8f88-50da23a07725.png">
*Fig. PCA Visualization of trained Word2Vec Network, 이미지 출처 : [link](https://projector.tensorflow.org/)*

이 글이 Word2Vec을 설명하는 글은 아니나 이야기가 나왔기 때문에, 이에 대해서 조금만 더 간략하게 설명하고 넘어가보겠습니다.

![skipgram](https://user-images.githubusercontent.com/48202736/108102464-ece47800-70cb-11eb-9e8a-c79ab4a24930.png)
*Fig. Skip-gram*

우선 Skip-gram은 위와 같이 생겼는데, 앞서 말씀드렸다 싶이, 주변의 단어들을 통해서 타겟 단어를 예측하는 Key idea 입니다.

![skip1](https://user-images.githubusercontent.com/48202736/108198913-958fe780-715f-11eb-83d9-6ed6beaf25bc.png)

예를 들어 'Jay was hit by a red bus in...' (이 이미지들을 직접 만든 Jay Alammar는 대체 왜 이런 예문을...) 라는 문장에서 주변 단어 'by,a,bus,in'를 통해서 'red'를 맞추는거죠.

여기에 중요한 것은 저희가 명시적으로 정답을 주어 학습하는게 아니고, 네트워크가 스스로 'by,a,bus,in'가 주어졌을때 정답은 'red'라는 즉 '주변 단어 좌2개 우2개로 가운데를 맞추자'라는 규칙을 통해 스스로
'입력-정답' pair를 만들어 학습한다는 것입니다.

디테일하게 보자면 아래와 같습니다.

![skip2](https://user-images.githubusercontent.com/48202736/108198918-96c11480-715f-11eb-8ca2-b7b9e504e97c.png)

![skip4](https://user-images.githubusercontent.com/48202736/108198924-99236e80-715f-11eb-999d-9b2aa4f70192.png)

이런식으로 '입력-정답' pair를 엄청나게 많이 만들어 내는 것이죠. (5칸짜리, 좌2개 우2개 입력, 가운데 정답 인것을 window라고 하며, 이를 sliding하면서 많은 데이터를 만들어냅니다.)

CBOW 방법도 한번 살펴보도록 하겠습니다.

![cbow](https://user-images.githubusercontent.com/48202736/108102472-ee15a500-70cb-11eb-9372-1e48b0ea7590.png)
*Fig. Continuous Back Of Words, CBOW*

이것은 반대로 가운데 한 단어를 통해서 주변 단어들을 예측하는 방법으로 아래와 같이 데이터를 구성할 수 있습니다. (역으로 되겠죠?)

![cbow1](https://user-images.githubusercontent.com/48202736/108198892-8d37ac80-715f-11eb-9dbe-63fc729b7a4d.png)

![cbow2](https://user-images.githubusercontent.com/48202736/108198894-8f017000-715f-11eb-84d8-506aa0ba859a.png)

여기서 무슨 방법이 더 좋다고는 확실하게 알려져 있지 않지만(만들어낼 수 있는 데이터 수 차이가 있을 수 있음), 이러한 방법들을 통해 우리는 단어 표현(Word Representation)을 학습할 수 있습니다.

학습을 하는 방법에 대해서는 아래와 같지만 앞서 말했듯 우리가 지금 Word2Vec만을 다루는 것이 아니기 때문에 수식적으로 접근하거나 디테일에 대해서 더 이야기 하지는 않겠습니다.

![word2vec_train](https://user-images.githubusercontent.com/48202736/108199006-ba845a80-715f-11eb-90e9-df757ffaa04e.png)

이렇게 학습이 된 경우 우리는 단어를 SSL방법으로 학습한 Representation으로 매핑해주는 Embedding Matrix를 얻을 수 있고, 예를들어 우리가 가지고 있는 데이터셋이 50000개의 사전 단어를 가지고 있어,
입력 단어가 $$ \mathbb{R}^{1 \times 50000} $$ 차원일 경우 Embedding Matrix가 $$ \mathbb{R}^{50000 \times 50} $$ 차원이라면, 
즉 우리가 학습을 통해 서로 유의미한 문장들끼리 뭉치게 할 단어 표현 벡터의 차원이 $$ \mathbb{R}^{1 \times 50} $$ 이 됩니다.

![word2vec_result](https://user-images.githubusercontent.com/48202736/108198996-b7896a00-715f-11eb-935e-1978c360b462.png)

우리가 얻은 단어 표현 벡터에 숫자를 입히면 아래와 같이 나타낼 수 있고,

![word2vec_result2](https://user-images.githubusercontent.com/48202736/108199002-b9532d80-715f-11eb-9be5-48f083afd033.png)

잘 학습이 되었다는 가정하에, 아래와 같이 'king - man + woman = queen'이 된다는 대량의 데이터를 annotation 없이 자기지도학습 방법으로 학습한 Word2Vec 네트워크의 놀라운 효과는 너무나 유명한 이야기 입니다. 

![word2vec_result3](https://user-images.githubusercontent.com/48202736/108199005-b9ebc400-715f-11eb-95df-927863e0003d.png)




### <mark style='background-color: #dcffe4'> BERT </mark>

BERT는, Word2Vec과는 또 조금 다른데요, Transformer Encoder를 `Denoising-Autoencoding`같은 방법론으로 학습해서 좋은 Representation을 학습하는 방법론 입니다.
네트워크 모식도는 아래와 같이 생겼는데요, Encoder의 인풋이 되는 token들을 임베딩 벡터로 만들어주고, 거기에 위치정보를 가지고 있는 positional encoding을 추가하고 segment embedding (선택) 까지 추가합니다. 

<img width="981" alt="BERT" src="https://user-images.githubusercontent.com/48202736/108096746-adfef400-70c4-11eb-9829-05a58478be3a.png">
*Fig. Bidirectional Encoder Representation from Transformer, BERT from Google, 2018*


앞서 말했던 것처럼 BERT는 labeld data 없이 스스로 annotation 해서 Representation을 학습하는 방법론 중 하나인데, 그렇게 하기 위해서 BERT는 입력 문장의 단어들 중 일부를 `마스킹 (Masking)` 하거나 다른 단어로 `대체 (Replace)` 함으로써 입력 문장을 더렵 (corruption)히고, 이 더럽힌 토큰이 바로 self-annotated 된 정답이 되어 이를 맞추는 방법을 통해 Representation을 학습합니다.

아래는 간단히 이를 도식화 한것이며 사진 출처는 마찬가지로 너무나 유명한 [Jay Alammar의 post](http://jalammar.github.io/illustrated-bert/)입니다.

![BERT_jay1](https://user-images.githubusercontent.com/48202736/108104460-ae9c8800-70ce-11eb-8e69-522ab5617624.png)
*Fig. Masking한 단어가 무엇인지 맞추는 일을 함으로써 Representation을 학습하는 BERT*

![BERT_jay2](https://user-images.githubusercontent.com/48202736/108104469-b1977880-70ce-11eb-9dfa-dabb06a4337e.png)
*Fig. 여러 문장이 이어진 Sequence에서 뒤에 오는 Sentence B가 앞에 오는 Sentence A와 관련이 있는지를 체크하는 일도 Representation을 학습하는데 도움이 된다. (하지만 후속 논문들에서 이는 쓸모없다고 하는 논문들이 많다.)*

![BERT_jay3](https://user-images.githubusercontent.com/48202736/108104473-b2c8a580-70ce-11eb-9f9a-68da2332a1cd.png)
*Fig. 최종적으로 unlabeld data를 학습하고 난 뒤 이 문장이 Spam인지 아닌지, 긍정적인지 부정적인지 등의 분류 문제를 labeled data로 학습할 수 있으며 이를 fine-tuning한다고 한다.*







### <mark style='background-color: #dcffe4'> Speech2Vec </mark>

<img width="1364" alt="speech2vec" src="https://user-images.githubusercontent.com/48202736/108096750-af302100-70c4-11eb-9cba-54aeefa418d8.png">
*Fig. The structures of Speech2Vec trained with skipgrams and cbow, respectively from Yu-An Chung(MIT CSAIL), 2018*




## <mark style='background-color: #fff5b1'> Contrastive Learning with NCE Loss</mark>

### <mark style='background-color: #dcffe4'> NCE and InfoNCE </mark>

`Noise Contrastive Estimation (NCE)`과 `InfoNCE` 는 현재 Wav2Vec 2.0 뿐만 아니라 다른 많은 SSL 방법론들에서 사용하는 Loss입니다. NCE와 InfoNCE는 각각 [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)와 [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748) 의 논문에서 제안되었으며 이름에서 알 수 있듯 InfoNCE는 NCE에서 영감을 받은 것이며 NCE는 앞서 살펴봤던 `Word2Vec`과 같은 Word Embedding을 학습하기 위한 Objective 였습니다.

***

Contrastive Representation Learning에서 사용하는 수많은 Objective들에 대해서 궁금하신 분들은 OpenAI의 연구자인 [Lilian Weng의 post](https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html)를 살펴보시면 더 좋을 것 같습니다.

***

그 중에서 InfoNCE에 대해서 조금 살펴보기 위해 `Contrastive Predictive Coding (CPC)`를 잠깐 보면

<img width="1230" alt="cpc" src="https://user-images.githubusercontent.com/48202736/108101659-e30e4500-70ca-11eb-96e5-30a79de0f5c3.png">
*Fig. Overview of Contrastive Predictive Coding(CPC) for Audio inputs from Deepmind, 2018*

`Context Vector`, $$c$$가 주어졌을 때 postivie sample을 $$p(x \vert c)$$로부터 하나 샘플하고 나머지 $$p(x)$$로 부터 $$N-1$$개를 샘플해서 총 $$N$$개를 샘플해서 아래의 `Likelihood`를 최대화 하는 방향으로 학습 합니다.

$$
\begin{aligned}
& 
p(C=\texttt{pos} \vert X, \mathbf{c}) 
= \frac{p(x_\texttt{pos} \vert \mathbf{c}) \prod_{i=1,\dots,N; i \neq \texttt{pos}} p(\mathbf{x}_i)}{\sum_{j=1}^N \big[ p(\mathbf{x}_j \vert \mathbf{c}) \prod_{i=1,\dots,N; i \neq j} p(\mathbf{x}_i) \big]}
= \frac{ \frac{p(\mathbf{x}_\texttt{pos}\vert c)}{p(\mathbf{x}_\texttt{pos})} }{ \sum_{j=1}^N \frac{p(\mathbf{x}_j\vert \mathbf{c})}{p(\mathbf{x}_j)} }
= \frac{f(\mathbf{x}_\texttt{pos}, \mathbf{c})}{ \sum_{j=1}^N f(\mathbf{x}_j, \mathbf{c}) }
& \\
\end{aligned}

&
\text{ where } X = {x_i}_{i=1}^N \text{ is sample vectors, and } f(x,c) \approx \frac{p(x \vert c)}{p(x)}
& \\
$$


이를 `Negative Log-Likelihood (NLL)`로 표현하면 아래와 같고,

$$
\mathcal{L}_\text{InfoNCE} = - \mathbb{E} \Big[\log \frac{f(\mathbf{x}, \mathbf{c})}{\sum_{\mathbf{x}' \in X} f(\mathbf{x}', \mathbf{c})} \Big]
$$

결국 이를 최소화 하는 방향으로 최적화를 합니다.

CPC에서 중요한 점은 일련의 벡터를 기반으로 그 다음 값이 뭐가 될지를 곧바로 (directly) 예측하는 일반적인 Autoregressive Model이 아니라 `Mutual Information`을 사용했다는 건데요, 이는 어떤 벡터 $$x$$ 와 맥락을 나타내는 (RNN 등으로 모델링된) context vector, $$c$$ 가 있을 때 아래와 같이 나타낼 수 있습니다.

$$
I(x ; c) = \sum_{x,c} log \frac{p(x,c)}{p(x) p(c)} = \sum_{x,c} p(x,c) log \color{red}{ \frac{p(x \vert c)}{p(x)} }
$$

***

이해를 돕기 위해, 일반적으로 Entropy는 분포가 얼마나 퍼져 있는가를 나타내며

$$
H[x] = \sum p(x) log p(x)
$$

어떤 두 변수 $$x,y$$간의 상호 정보량은

$$
I[x,y] = H[x] - H[x \vert y] = H[y] - H[y \vert x]
$$

으로 나타내거나 두 확률 분포의 유사도를 나타내는 KLD를 사용해서

$$
\begin{aligned}
&
I[x,y] = KL(p(x,y) \parallel p(x) p(y))
& \\

&
= - \int \int ( p(x,y) log ( \frac{p(x)p(y)}{p(x,y)} )) dx dy
& \\
\end{aligned}
$$

처럼 나타낼 수 있으며 이는 두 변수가 얼마나 독립적인지를 나타냅니다.

***

즉 CPC는 샘플링 된 벡터들 N개, $$x_{1,\cdots,N}$$와 맥락 정보를 가지고 있는 context vector, $$c$$ 간의 `상호 정보량 (Mutual Information)`을 

$$
I(x ; c) = \sum_{x,c} log \frac{p(x,c)}{p(x) p(c)} = \sum_{x,c} p(x,c) log \color{red}{ \frac{p(x \vert c)}{p(x)} }
$$

신경망 $$f(x,c)$$로 모델링해서 

$$
f_k(\mathbf{x}_{t+k}, \mathbf{c}_t) = \exp(\mathbf{z}_{t+k}^\top \mathbf{W}_k \mathbf{c}_t) \propto \frac{p(\mathbf{x}_{t+k}\vert\mathbf{c}_t)}{p(\mathbf{x}_{t+k})}
$$

이를 통해 Objective를 구성하는 것입니다.


이렇게 pre-training을 진행하고 난 뒤에는 task에 따라서 $$z_t,c_t$$를 골라서 사용해 fine-tuning할 수 있는데,

<img width="1230" alt="cpc" src="https://user-images.githubusercontent.com/48202736/108101659-e30e4500-70ca-11eb-96e5-30a79de0f5c3.png">
*Fig. Overview of Contrastive Predictive Coding(CPC) for Audio inputs from Deepmind, 2018*

일반적으로 $$c_t$$는 Autoregressive Model (RNN)을 사용해서 과거의 맥락 정보가 담겨 있기 때문에 음성 인식 같은 경우에 적합하고, 아닌 경우에는 $$z_t$$를 써도 되는 등 feature를 선택 할 수 있습니다.




## <mark style='background-color: #fff5b1'> Former Wav2Vec Series</mark>

### <mark style='background-color: #dcffe4'> Wav2Vec </mark>

<img width="783" alt="wav2vec" src="https://user-images.githubusercontent.com/48202736/108096770-b5260200-70c4-11eb-9987-d7ee7dd73886.png">





### <mark style='background-color: #dcffe4'> VQ-Wav2Vec </mark>

<img width="1283" alt="vq-wav2vec" src="https://user-images.githubusercontent.com/48202736/108096787-b8b98900-70c4-11eb-884a-6f0f6718a1db.png">

<img width="1272" alt="vq-wav2vec_2" src="https://user-images.githubusercontent.com/48202736/108096795-ba834c80-70c4-11eb-9bda-d2daa78418a2.png">






## <mark style='background-color: #fff5b1'> Wav2Vec2.0 </mark>

<img width="1125" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/108096805-bce5a680-70c4-11eb-8831-e621254e5ed0.png">








## <mark style='background-color: #fff5b1'> Further Study </mark>

### <mark style='background-color: #dcffe4'> Unsupervised Cross-Lingual Representation Learning </mark>

<img width="1330" alt="multilingual" src="https://user-images.githubusercontent.com/48202736/108098859-3b434800-70c7-11eb-82b3-537e077e8e61.png">






## <mark style='background-color: #fff5b1'> References </mark>

- 1.[The Illustrated Self-Supervised Learning from Amit Chaudhary](https://amitness.com/2020/02/illustrated-self-supervised-learning/)

- 2.[The Illustrated SimCLR Framework from Amit Chaudhary](https://amitness.com/2020/03/illustrated-simclr/)

- 3.[Self-Supervised Representation Learning from Lillian Weng](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html#why-self-supervised-learning)

- 4.[Learning Word Embedding from Lillian Weng](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#noise-contrastive-estimation-nce)

- 5.[The Illustrated Word2vec from Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)

- 6.[The Illustrated BERT, ELMo, and co. from Jay Alammar](https://jalammar.github.io/illustrated-bert/)
