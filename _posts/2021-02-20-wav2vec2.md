---
title: (미완)Self-Supervised Learning for Speech Representation
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---


이번 글에서는 자기지도학습(Self-Supervised Learning, SSL)이 무엇인지 알아보고, 자연어처리와 음성인식 분야에서 이 학습 방법이 어떻게 사용 되어왔는지,
그리고 2020년에 공개된 SSL을 통해서 음성인식 최고 성능(State-Of-The-Arts, SOTA)을 내서 화제가 됐던 논문 Facebook AI Research(FAIR)  `Wav2Vec 2.0`에 대해 알아보도록 하겠습니다. 
 
<img width="1125" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/108096805-bce5a680-70c4-11eb-8831-e621254e5ed0.png">
*Fig. Wav2Vec 2.0 Framework*
 
본격적으로 시작하기에 앞서, 글의 말미에 다루게 될 핵심적인 내용인 Wav2Vec2.0 논문에 대해 얘기해 보자면, 이 논문의 중요 포인트는 크게 두 가지로 볼 수 있습니다. 
이 두가지는 바로 

> 1. 50000시간의 데이터로 '입력-정답' pair 없이 자기지도학습을 한 뒤, '입력-정답' pair 데이터(960시간, Librispeehc dataset)로 지도 학습(Supervised Learning, SL) 파인튜닝을 했더니 SOTA 성능을 냈다.
> 2. 마찬가지로 SSL로 사전 학습을 한 뒤, 굉장히 적은 시간의 데이터(10분)로 파인튜닝을 했더니 음성인식을 어지간한 음성인식 네트워크들만큼 하더라.

인데요, 여기서 2번째가 시사하는 바는 굉장히 클 수가 있습니다.

이는 일반적으로 딥러닝에서 사용되는 '데이터를 주고 정답을 알려준 뒤 이를 맞추는 방향으로 학습'하는 지도 학습의 경우 데이터를 구성하는 비용이 굉장히 비싼데, 
특히 이는 음성인식에서 더 두드러지기 때문입니다.

음성 인식 데이터는 다른 도메인에 비해 annotation하기가 힘들어 상대적으로 더 적다고 볼 수 있는데, 
이 말은 즉, 만국 공통어라고 불리는 영어, 중국어등이 아니면 다른 나라 언어의 경우 더욱 데이터가 부족하다는 겁니다. 

따라서 딥러닝 방법으로 음성인식기를 만들기란 굉장히 어려운데, 여기서 2번이 시사하는 바는 만약 우리가 소수민족의 언어에 대한 음성인식기를 만든다고 해도, 그들이 뱉은 음성 데이터를 엄청 모아 SSL 방법으로 사전학습한 뒤 10분 정도의 pair 데이터만 있다면 그럴싸한 음성인식기를 만들어 낼 수 있다는 부분에서 굉장히 중요한 부분이라고 할 수 있을 것 같습니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

## <mark style='background-color: #fff5b1'> Why Self-Supervised Learning? </mark>

자 이제 본격적으로 자기지도학습에 대해서 이야기 해보도록 하겠습니다. 왜 이게 중요할까요?
앞서 말했지만, 다시 한번 말씀드리자면 딥러닝에서 사용되는 지도학습 방법론을 위한 데이터셋을 구축하기에 비용이 너무 많이 들기 때문입니다. 

![ssl2](https://user-images.githubusercontent.com/48202736/108098887-44341980-70c7-11eb-8027-1caa7b7545f5.png)

예를 들어 이미지 분류를 예로 들어보겠습니다. '이미지<->정답' pair가 10000개 있다고 생각해 보겠습니다.
이런 pair데이터들을 보통 annotation되어 있다고 이야기 합니다.

어쨌든 지도 학습을 생각해보겠습니다. 
우리가 네트워크를 '이미지<->정답' pair를 가지고 학습을 할 경우, 이미지를 잘 표현하는 특징을 추출하는 Representation(보통 Encoder가 추출)을 학습을 통해 배우게 되고, 
이를 최종적으로 분류층(Classifier)이 분류를 하게 됩니다.

우리는 하지만 annotation이 없는 개, 고양이 사진들도 엄청 많이 가지고 있을겁니다.
바로 이러한 '널려있는 annotation 되어있지 않은 이미지들만 가지고 Representation을 잘 학습할 수는 없을까?'라는 생각을 토대로 고안된 방법이 자기지도학습입니다.

![ssl4](https://user-images.githubusercontent.com/48202736/108098893-45fddd00-70c7-11eb-91b6-6521e62ebdbb.png)

다른 말로 이는 데이터들을 스스로 annotation 한다고 생각할 수도 있습니다.

어쨌든 이렇게 방대한 annotation되지 않은 입력 데이터들만을 가지고 스스로 정답이라고 정의한것을 학습하면서 Representation를 Encoder 네트워크가 배우게 되고
이렇게 학습된 (보통 사전 학습(pre-training) 했다고 표현합니다) 네트워크를 내가 원하는 task에 해당하는 소량의 데이터로(여유가 된다면 이번에도 대량의 데이터로) 지도학습을 해 주면 (이를 미세조정(fine-tuning) 한다고 합니다.) 우리는 소량의 데이터만을 가지고 지도학습을 한 경우보다 훨씬 좋은 성능의 네트워크를 얻을 수 있게 되는겁니다.

![ssl5](https://user-images.githubusercontent.com/48202736/108098896-46967380-70c7-11eb-8134-7e932e7062f8.png)

지금까지는 이미지를 다루는 Computer Vision task에 대해서 예시로 들어봤지만, 아마 대부분은 이러한 SSL방법에 대해서 이미 익숙하실겁니다. 
바로 딥러닝을 접하고 이것저것 읽다보면 배우게 되는 Word2Vec이나 자연어처리 분야의 SOTA 기법들인 (BERT, GPT)등이 모두 이러한 방법론을 따르기 때문입니다.

![ssl1](https://user-images.githubusercontent.com/48202736/108098880-4302ec80-70c7-11eb-9ae2-af9e0ba1f5e1.png)

이제 이러한 SSL을 사용한 네트워크들에 대해서 대략적으로 알아보도록 하겠습니다. 






### <mark style='background-color: #dcffe4'> Word2Vec </mark>

2013년에 구글에서 제안된 Word2Vec은 뉴럴네트워크를 사용해서 자기지도학습 방법으로 Word Representation을 학습하는 방법을 제안한 논문입니다.
이는 2018년에 제시되어 현재까지도 대부분의 자연어처리 (Natural Language Processing, NLP) task의 Encoder단에서 Representation을 뽑아주는 feature extractor로 쓰이는 [BERT](https://arxiv.org/pdf/1810.04805)와 유사한 역할을 하는(어떻게 학습을 통해 Representation을 얻어내는가는 다릅니다), 그러니까 이런 방법론의 시초? 라고 볼 수 있는 모델입니다.

<img width="1126" alt="word2vec" src="https://user-images.githubusercontent.com/48202736/108096719-a8a1a980-70c4-11eb-833d-c210d9136d7b.png">
*Fig. Word2Vec model architectures from Google 2013. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.*

Word2Vec 논문에서 제안된 방법은 주변 단어들을 통해서 타깃이 되는 가운데 단어 하나를 예측하는 CBOW(Continuous Back Of Words) 방법과 Skip-gram 방법 두 가지가 있으며,
이런식으로 학습 된 단어들은 유사한 의미(semantic)를 가지는 단어일 수록 같은 곳에 뭉치게 되는 현상을 보여주게 됩니다.  

(어떻게 뭉치는지에 대한 결과물은 Representation을 3차원으로 projection해서 보여주는 아래의 사이트를 통해 확인하실 수 있습니다.)

<img width="826" alt="word2vec_animation" src="https://user-images.githubusercontent.com/48202736/108102442-e524d380-70cb-11eb-8f88-50da23a07725.png">
*Fig. PCA Visualization of trained Word2Vec Network, 이미지 출처 : [link](https://projector.tensorflow.org/)*

이 글이 Word2Vec을 설명하는 글은 아니나 이야기가 나왔기 때문에, 이에 대해서 조금만 더 간략하게 설명하고 넘어가보겠습니다.

![skipgram](https://user-images.githubusercontent.com/48202736/108102464-ece47800-70cb-11eb-9e8a-c79ab4a24930.png)
*Fig. Skip-gram*

우선 Skip-gram은 위와 같이 생겼는데, 앞서 말씀드렸다 싶이, 주변의 단어들을 통해서 타겟 단어를 예측하는 Key idea 입니다.

![skip1](https://user-images.githubusercontent.com/48202736/108198913-958fe780-715f-11eb-83d9-6ed6beaf25bc.png)

예를 들어 'Jay was hit by a red bus in...' (이 이미지들을 직접 만든 Jay Alammar는 대체 왜 이런 예문을...) 라는 문장에서 주변 단어 'by,a,bus,in'를 통해서 'red'를 맞추는거죠.

여기에 중요한 것은 저희가 명시적으로 정답을 주어 학습하는게 아니고, 네트워크가 스스로 'by,a,bus,in'가 주어졌을때 정답은 'red'라는 즉 '주변 단어 좌2개 우2개로 가운데를 맞추자'라는 규칙을 통해 스스로
'입력-정답' pair를 만들어 학습한다는 것입니다.

디테일하게 보자면 아래와 같습니다.

![skip2](https://user-images.githubusercontent.com/48202736/108198918-96c11480-715f-11eb-8ca2-b7b9e504e97c.png)

![skip4](https://user-images.githubusercontent.com/48202736/108198924-99236e80-715f-11eb-999d-9b2aa4f70192.png)

이런식으로 '입력-정답' pair를 엄청나게 많이 만들어 내는 것이죠. (5칸짜리, 좌2개 우2개 입력, 가운데 정답 인것을 window라고 하며, 이를 sliding하면서 많은 데이터를 만들어냅니다.)

CBOW 방법도 한번 살펴보도록 하겠습니다.

![cbow](https://user-images.githubusercontent.com/48202736/108102472-ee15a500-70cb-11eb-9372-1e48b0ea7590.png)
*Fig. Continuous Back Of Words, CBOW*

이것은 반대로 가운데 한 단어를 통해서 주변 단어들을 예측하는 방법으로 아래와 같이 데이터를 구성할 수 있습니다. (역으로 되겠죠?)

![cbow1](https://user-images.githubusercontent.com/48202736/108198892-8d37ac80-715f-11eb-9dbe-63fc729b7a4d.png)

![cbow2](https://user-images.githubusercontent.com/48202736/108198894-8f017000-715f-11eb-84d8-506aa0ba859a.png)

여기서 무슨 방법이 더 좋다고는 확실하게 알려져 있지 않지만(만들어낼 수 있는 데이터 수 차이가 있을 수 있음), 이러한 방법들을 통해 우리는 단어 표현(Word Representation)을 학습할 수 있습니다.

학습을 하는 방법에 대해서는 아래와 같지만 앞서 말했듯 우리가 지금 Word2Vec만을 다루는 것이 아니기 때문에 수식적으로 접근하거나 디테일에 대해서 더 이야기 하지는 않겠습니다.

![word2vec_train](https://user-images.githubusercontent.com/48202736/108199006-ba845a80-715f-11eb-90e9-df757ffaa04e.png)

이렇게 학습이 된 경우 우리는 단어를 SSL방법으로 학습한 Representation으로 매핑해주는 Embedding Matrix를 얻을 수 있고, 예를들어 우리가 가지고 있는 데이터셋이 50000개의 사전 단어를 가지고 있어,
입력 단어가 $$ \mathbb{R}^{1 \times 50000} $$ 차원일 경우 Embedding Matrix가 $$ \mathbb{R}^{50000 \times 50} $$ 차원이라면, 
즉 우리가 학습을 통해 서로 유의미한 문장들끼리 뭉치게 할 단어 표현 벡터의 차원이 $$ \mathbb{R}^{1 \times 50} $$ 이 됩니다.

![word2vec_result](https://user-images.githubusercontent.com/48202736/108198996-b7896a00-715f-11eb-935e-1978c360b462.png)

우리가 얻은 단어 표현 벡터에 숫자를 입히면 아래와 같이 나타낼 수 있고,

![word2vec_result2](https://user-images.githubusercontent.com/48202736/108199002-b9532d80-715f-11eb-9be5-48f083afd033.png)

잘 학습이 되었다는 가정하에, 아래와 같이 'king - man + woman = queen'이 된다는 대량의 데이터를 annotation 없이 자기지도학습 방법으로 학습한 Word2Vec 네트워크의 놀라운 효과는 너무나 유명한 이야기 입니다. 

![word2vec_result3](https://user-images.githubusercontent.com/48202736/108199005-b9ebc400-715f-11eb-95df-927863e0003d.png)




### <mark style='background-color: #dcffe4'> BERT </mark>

<img width="981" alt="BERT" src="https://user-images.githubusercontent.com/48202736/108096746-adfef400-70c4-11eb-9829-05a58478be3a.png">
*Fig. Bidirectional Encoder Representation from Transformer, BERT from Google, 2018*


![BERT_jay1](https://user-images.githubusercontent.com/48202736/108104460-ae9c8800-70ce-11eb-8e69-522ab5617624.png)
![BERT_jay2](https://user-images.githubusercontent.com/48202736/108104469-b1977880-70ce-11eb-9dfa-dabb06a4337e.png)
![BERT_jay3](https://user-images.githubusercontent.com/48202736/108104473-b2c8a580-70ce-11eb-9f9a-68da2332a1cd.png)








### <mark style='background-color: #dcffe4'> Speech2Vec </mark>

<img width="1364" alt="speech2vec" src="https://user-images.githubusercontent.com/48202736/108096750-af302100-70c4-11eb-9cba-54aeefa418d8.png">
*Fig. The structures of Speech2Vec trained with skipgrams and cbow, respectively from Yu-An Chung(MIT CSAIL), 2018*




## <mark style='background-color: #fff5b1'> Contrastive Learning </mark>

### <mark style='background-color: #dcffe4'> Contrastive Predictive Coding </mark>

<img width="1230" alt="cpc" src="https://user-images.githubusercontent.com/48202736/108101659-e30e4500-70ca-11eb-96e5-30a79de0f5c3.png">
*Fig. Overview of Contrastive Predictive Coding(CPC) for Audio inputs from Deepmind, 2019*






### <mark style='background-color: #dcffe4'> SimCLR Framework </mark>








## <mark style='background-color: #fff5b1'> Former Wav2Vec Series</mark>

### <mark style='background-color: #dcffe4'> Wav2Vec </mark>

<img width="783" alt="wav2vec" src="https://user-images.githubusercontent.com/48202736/108096770-b5260200-70c4-11eb-9987-d7ee7dd73886.png">





### <mark style='background-color: #dcffe4'> VQ-Wav2Vec </mark>

<img width="1283" alt="vq-wav2vec" src="https://user-images.githubusercontent.com/48202736/108096787-b8b98900-70c4-11eb-884a-6f0f6718a1db.png">

<img width="1272" alt="vq-wav2vec_2" src="https://user-images.githubusercontent.com/48202736/108096795-ba834c80-70c4-11eb-9bda-d2daa78418a2.png">






## <mark style='background-color: #fff5b1'> Wav2Vec2.0 </mark>

<img width="1125" alt="wav2vec2" src="https://user-images.githubusercontent.com/48202736/108096805-bce5a680-70c4-11eb-8831-e621254e5ed0.png">








## <mark style='background-color: #fff5b1'> Further Study </mark>

### <mark style='background-color: #dcffe4'> Unsupervised Cross-Lingual Representation Learning </mark>

<img width="1330" alt="multilingual" src="https://user-images.githubusercontent.com/48202736/108098859-3b434800-70c7-11eb-82b3-537e077e8e61.png">






## <mark style='background-color: #fff5b1'> References </mark>

- 1.[The Illustrated Self-Supervised Learning from Amit Chaudhary](https://amitness.com/2020/02/illustrated-self-supervised-learning/)

- 2.[The Illustrated SimCLR Framework from Amit Chaudhary](https://amitness.com/2020/03/illustrated-simclr/)

- 3.[Self-Supervised Representation Learning from Lillian Weng](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html#why-self-supervised-learning)

- 4.[Learning Word Embedding from Lillian Weng](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#noise-contrastive-estimation-nce)

- 5.[The Illustrated Word2vec from Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)

- 6.[The Illustrated BERT, ELMo, and co. from Jay Alammar](https://jalammar.github.io/illustrated-bert/)
