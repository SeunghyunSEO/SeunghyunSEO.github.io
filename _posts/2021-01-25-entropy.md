---
title: Relative Entropy and Mutual Information
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Information Theory </mark>

```
비숍의 PRML 책을 보면, 정보 이론이 패턴인식과 머신 러닝 테크닉을 이해하는 데 있어서 중요한 역할을 하게 될 역할을 한다는 말을 합니다.
```


- <mark style='background-color: #fff5b1'> Relative Entropy and Mutual Information </mark>

```
이번에는 정보 이론의 중요 개념들을 패턴 인식에 어떻게 적용시킬 수 있는지를 살펴보게 될 것입니다.
```

자 우선, 알려지지 않은 분포 $$p(x)$$에 대해 먼저 생각해봅시다. 우리가 머신 러닝을 하는 이유는 실제 데이터 분포 $$p(x)$$를 찾는 것 입니다. 

여차저차 학습 데이터를 모델링해 분포 q(x)를 구할 수 있었다고 생각해봅시다.


만약 우리가 $$q(x)$$를 사용해 x의 값을 누군가에게 전달하기 위해 코드를 만든다고 하면, 
우리는 $$p(x)$$ 가 아닌 $$q(x)$$를 사용했기 때문에 추가적인 정보를 더 포함해서 수신자에게 전달해야 합니다.

이때 추가로 필요한 정보의 양은 다음과 같이 주어집니다.

<center>$$ KL(p||q) = - \int p(x) lnq(x) dx - (-\int p(x) lnp(x) dx) $$</center>
<center>$$ KL(p||q) = - \int p(x) ln\frac{q(x)}{p(x)} dx $$</center>

위의 $$KL(p||q)$$를 두 분포간의 상대 엔트로피 (relative entropy) 혹은 쿨백 라이블러 발산 (Kullback-Leibler divergence, KL divergence, KLD) 라고 합니다.

어떤 데이터 $$x$$(벡터) 에 대해 이 연속 변수들에 대해 정의된 밀도의 경우, 미분 엔트로피가 아래와 같이 주어졌던걸 생각해보면

두 분포(실제 분포, 실제가 아닌 분포)를 이용해서 데이터를 전달하기 위해 필요한 정보량의 차가 위 수식처럼 주어지는지 알 수 있을겁니다.

<center>$$ cf) H[x] = - \int p(x) lnp(x) dx $$</center>



- <mark style='background-color: #fff5b1'> References </mark>

1. [Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)
