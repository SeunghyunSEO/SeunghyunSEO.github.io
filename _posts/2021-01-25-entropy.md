---
title: Relative Entropy and Mutual Information
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

이번 글은 [Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/) 를 상당히 많이 참고하였습니다 :)

- <mark style='background-color: #fff5b1'> Information Theory </mark>

```
비숍의 PRML 책을 보면, 정보 이론이 패턴인식과 머신 러닝 테크닉을 이해하는 데 있어서 확률론, 결정 이론과 함께 중요한 역할을 하게 될 역할을 한다는 말을 합니다.
```

이산 확률 변수 $$x$$에 대해 생각해 봅시다. 

![image](https://user-images.githubusercontent.com/48202736/105871613-d606b480-603c-11eb-865f-2048a81bb0ca.png)

- <mark style='background-color: #dcffe4'> 정보량 </mark>

이 변수가 특정 값을 가지고 있는 것을 확인했을 때 전해지는 정보량은 얼마만큼일까? 

x의 값을 학습하는 데 있어서 정보의 양은 '놀라움의 정도'라고 생각 할 수 있을겁니다.

즉, 매우 일어날 가능성이 높은 사건이 일어났다는 소식을 전해 들었을 때 보다 일어나기 힘든 사건이 발생했다는 사실을 전해 들었을 때 더 많은 정보를 전달받게 된다는 것이죠.


따라서 앞으로 우리가 쓸 정보량은 데이터 x의 발생 확률을 의미하는 분포 $$p(x)$$에 종속적인 함수가 됩니다.

<center>$$ h(x) = - log_2 p(x) $$<center>
  
위의 수식은 확률 값이 0~1 사이의 값이기 때문에 음의 값을 가질 수 없으며, 확률이 0에 가까워질수록 무한대에 가까운 값을 가지게 됩니다. 그리고 로그의 밑이 2인 것은 정보 이론 학계의 관습 이라고 합니다. 이렇게 할 경우 h(x)의 단위는 비트가 된다고 합니다.('이진 비트')


- <mark style='background-color: #dcffe4'> 엔트로피 (Entropy) </mark>

이번에는 누군가 어떤 확률 변수의 값을 다른 사람에게 전송하고자 하는 상황을 가정해 보겠습니다. 전송에 필요한 정보량의 평균치는 p(x)에 대한 정보량의 기대값으로 구할 수 있습니다.

<center>$$ H[x] = - \sum_2 p(x)log_2 p(x) $$<center>
  
이 값을 바로 엔트로피라고 부르고, 확률 밀도 $$p(x)$$가 베르누이 분포를 따를 경우 엔트로피는 아래와 같습니다.

![image](https://user-images.githubusercontent.com/48202736/105872548-e1a6ab00-603d-11eb-80d3-0cbf5874b534.png)





- <mark style='background-color: #fff5b1'> Relative Entropy and Mutual Information </mark>

```
이번에는 정보 이론의 중요 개념들을 패턴 인식에 어떻게 적용시킬 수 있는지를 살펴보게 될 것입니다.
```

자 우선, 알려지지 않은 분포 $$p(x)$$에 대해 먼저 생각해봅시다. 우리가 머신 러닝을 하는 이유는 실제 데이터 분포 $$p(x)$$를 찾는 것 입니다. 

여차저차 학습 데이터를 모델링해 분포 q(x)를 구할 수 있었다고 생각해봅시다.


만약 우리가 $$q(x)$$를 사용해 x의 값을 누군가에게 전달하기 위해 코드를 만든다고 하면, 
우리는 $$p(x)$$ 가 아닌 $$q(x)$$를 사용했기 때문에 추가적인 정보를 더 포함해서 수신자에게 전달해야 합니다.

이때 추가로 필요한 정보의 양은 다음과 같이 주어집니다.

<center>$$ KL(p \parallel  q) = - \int p(x) lnq(x) dx - (-\int p(x) lnp(x) dx) $$</center>
<center>$$ KL(p \parallel  q) = - \int p(x) ln\frac{q(x)}{p(x)} dx $$</center>

위의 $$KL(p \parallel  q)$$를 두 분포간의 상대 엔트로피 (relative entropy) 혹은 쿨백 라이블러 발산 (Kullback-Leibler divergence, KL divergence, KLD) 라고 합니다.

어떤 데이터 $$x$$(벡터) 에 대해 이 연속 변수들에 대해 정의된 밀도의 경우, 미분 엔트로피가 아래와 같이 주어졌던걸 생각해보면

두 분포(실제 분포, 실제가 아닌 분포)를 이용해서 데이터를 전달하기 위해 필요한 정보량의 차가 위 수식처럼 주어지는지 알 수 있을겁니다.

<center>$$ cf) H[x] = - \int p(x) lnp(x) dx $$</center>

이 식은 대칭적이지 않으며, 따라서 $$KL(p \parallel  q) \not\equiv  KL(q \parallel  p)$$ 입니다.

또한 $$KL(p \parallel  q)=0$$ 일 때는  $$KL(p \parallel q) \geqslant  0$$ 에서  $$p(x) = q(x)$$ 인 것과 동치인데 이것의 증명은 생략하도록 하겠습니다.

결론은 KLD를 두 분포 p(x)와 q(x)가 얼마나 다른지의 척도로 사용할 수 잇다는 것입니다.

종합해보면 밀도를 추정하는 것 (예를 들어 알지 못하는 확률 분포를 모델링하는 문제)와 전송하는 데이터를 압축하는 데는 밀접한 연관이 있음을 알 수 있습니다.

왜냐하면 실제 분포에 대해서 알고 있을 때 가장 효율적인 압축이 가능하기 때문입니다.

실제 분포와 다른 분포를 바탕으로 모델링이 이루어졌을 경우에는 압축 자체가 덜 효율적이고, 평균적으로 두 분포 사이의 KLD 만큼의 정보가 추가적으로 전송되어야 하는 것입니다.


예를 들어 우리가 모델링하고자 하는 알려지지 않은 분포 $$p(x)$$로부터 데이터가 만들어지는 상황을 가정해 봅시다.

학습 가능한 파라메터 $$\theta$$에 대해 종속적인 매개변수 분포 $$q(x \mid \theta)$$ (likelihood네요, 예를 들자면 다변량 가우시안 분포 등)을 이용해서 $$p(x)$$를 추정하고자 할 수 있을겁니다.

이 때 파라메터를 추정하는 방법은 두 분포 $$p(x)$$와 $$q(x \mid \theta)$$ 사이의 KLD를 최소화 하도록 하는 $$\theta$$를 찾는 것입니다.


하지만 우리는 실제 데이터 분포 $$p(x)$$는 아예 모르는 상태이고 (그러니까 어디서 샘플링했는지는 모르는 겁니다), 학습 데이터들만 가지고 있습니다. ($$x_n, \space n=1,2,...,n$$)

그렇기 때문에 우리는 KL term을 근사시켜야 합니다.

<center>$$ KL(p \parallel q) = - \int p(x) lnq(x) dx - (-\int p(x) lnp(x) dx) $$</center>

<center>$$ KL(p \parallel q) \simeq  - \frac{1}{N} \sum_{n=1}^{N} \{ -ln q(x_n \mid \theta) + lnp(x_n) \} $$</center>

(데이터 포인트들의 합으로 $$p(x)$$에 대한 기대값을 구한 것이죠)

여기서 우변의 두 번째 항은 추정하고자 하는 $$\theta$$와 독립이고, 첫 번째 항인 $$q(x \mid \theta)$$ 하에서 $$\theta$$의 음의 로그 가능도 함수 (Negative Log Likelihood, NLL)을 최소화 하는 것에 해당합니다.


즉 KLD를 최소화 하는 것이 Maximum Likelihood 문제를 푸는 것과 동일하다는 것을 알 수 있습니다. 



- <mark style='background-color: #fff5b1'> References </mark>

1. [Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)
