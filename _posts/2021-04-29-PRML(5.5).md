---
title: 5.5 Regularization in Neural Networks
categories: Brief_Review_for_PRML
tag: [PRML,MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---



이번 챕터의 주제는 "뉴럴 네트워크에서의 정규화" 입니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

머신 러닝을 한다는 것은 결국, 데이터의 분포를 잘 모방하는, 입력 $$x$$ 로부터 $$y$$ 의 출력 분포를 모델링하는 Mapping Function, $$\hat{y}=f(x)$$을 학습 하는 겁니다. 분포를 잘 모델링 해냈다면, 보지 못한 데이터가 들어왔을 때 (unseen data) 적절한 분포를 예측하게 되겠죠. 앞선 1,2,3장에서 우리는 최대 우도 측정 (Maximum Likelihood Estimation, MLE)을 통해서 출력 분포를 모델링 하는 것이 얼마나 쉽게 과적합 (overfitting)을 일으키는지를 봐 왔습니다. 

$$
\hat{y}=f(x)
$$

우리는 이러한 Mapping Function의 종류로 고전적인 머신 러닝 방법들부터 (입력 자체를 매핑하거나, 기저 함수를 사용하거나) 층을 조금 더 깊게 쌓아 표현력을 높힌 신경망 (Neural Network) 까지 사용할 수 있다는 걸 배웠습니다. 여기서 Mapping Function 의 입력 - 출력 유닛의 숫자는 보통 데이터 집합의 차원수에 따라 결정되지만, 은닉 유닛 (Hidden Unit)들의 숫자는 $$M$$은 사실 자유 매개 변수 이며, 과적합을 피해 최적의 예측 성능을 내도록 조절할 수 있습니다. 우리는 과적합이 일어나지 않는 선에서, 일반화 성능을 (generalization, 일반화란 학습 이후 unseen data를 만났을 때 얼마나 좋은 예측을 하느냐를 나타낸다.) 최대한으로 낼 수 있는 어떠한 $$M$$이 존재할거라는 믿음을 가질 수 있습니다.  

### <mark style='background-color: #dcffe4'> 5.5.1 일관된 가우시안 사전 분포 (Consistent Gaussian priors) </mark>

### <mark style='background-color: #dcffe4'> 5.5.2 조기 종료 (Early stopping) </mark>

### <mark style='background-color: #dcffe4'> 5.5.3 불변성 (Invariances) </mark>

### <mark style='background-color: #dcffe4'> 5.5.4 탄젠트 전파 (Tangent Propagation) </mark>


### <mark style='background-color: #dcffe4'> 5.5.5 변환된 데이터를 이용한 훈련 (Training with transformed data) </mark>


### <mark style='background-color: #dcffe4'> 5.5.6 합성곱 신경망 (Convolutional networks) </mark>


### <mark style='background-color: #dcffe4'> 5.5.7 약한 가중치 공유 (Soft weight sharing) </mark>
