---
title: 5.5 Regularization in Neural Networks
categories: Brief_Review_for_PRML
tag: [PRML,MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---



이번 챕터의 주제는 "뉴럴 네트워크에서의 정규화" 입니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

머신 러닝을 한다는 것은 결국, 데이터의 분포를 잘 모방하는, 입력 $$x$$ 로부터 $$y$$ 의 출력 분포를 모델링하는 Mapping Function, $$\hat{y}=f(x)$$을 학습 하는 겁니다. 분포를 잘 모델링 해냈다면, 보지 못한 데이터가 들어왔을 때 (unseen data) 적절한 분포를 예측하게 되겠죠. 앞선 1,2,3장에서 우리는 최대 우도 측정 (Maximum Likelihood Estimation, MLE)을 통해서 출력 분포를 모델링 하는 것이 얼마나 쉽게 과적합 (overfitting)을 일으키는지를 봐 왔습니다. 

$$
\hat{y}=f(x)
$$

이러한 

### <mark style='background-color: #dcffe4'> 5.5.1 일관된 가우시안 사전 분포 (Consistent Gaussian priors) </mark>

### <mark style='background-color: #dcffe4'> 5.5.2 조기 종료 (Early stopping) </mark>

### <mark style='background-color: #dcffe4'> 5.5.3 불변성 (Invariances) </mark>

### <mark style='background-color: #dcffe4'> 5.5.4 탄젠트 전파 (Tangent Propagation) </mark>


### <mark style='background-color: #dcffe4'> 5.5.5 변환된 데이터를 이용한 훈련 (Training with transformed data) </mark>


### <mark style='background-color: #dcffe4'> 5.5.6 합성곱 신경망 (Convolutional networks) </mark>


### <mark style='background-color: #dcffe4'> 5.5.7 약한 가중치 공유 (Soft weight sharing) </mark>
