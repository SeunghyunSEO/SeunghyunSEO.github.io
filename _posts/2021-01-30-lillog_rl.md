---
title: A (Long) Peek into Reinforcement Learning - Lil'log
categories: ReinforcementLearning
tag: [RL]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Lilian Weng의 블로그 포스트 번역 </mark>

[[Original post written in english](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#common-approaches)]

이 글은 OpenAI의 머신 러닝 연구자인 [lilian Weng](https://scholar.google.com/citations?user=dCa-pW8AAAAJ)의 [blog(Lil'log)](https://lilianweng.github.io/lil-log/) 글을 번역한 것입니다.

![image](https://user-images.githubusercontent.com/48202736/106112753-fa73a580-6190-11eb-961b-d598ee0902a3.png)
{: style="width: 40%;" class="center"}
*(Lilian Weng)*

굉장히 글을 잘 쓰셔서 꼭 옮겨적으면서 다시 한 번 개념들을 이해하고 다른 분들께 전달하고 싶어서 번역을 하게 되었습니다.


릴리안의 포스트를 번역하는것은 아래와 같이 그녀에게 이메일로 알려주면 된다고 하는데, 이 글은 아직 비공식 글이기 때문에 우선 번역을 ...해보도록 하겠습니다. (아무도 안볼듯 ㅎ)

```
Q: Can I translate your posts to another language?

A: Yes and my pleasure! But please email me in advance and please keep the original post link on top (rather than in tiny font at the end, and yes I do see that case and that makes me feel sad).
```

하지만 이 포스트는 제가 릴리안의 글을 똑같은 포맷으로 그대로 가져와 번역만 한 것이 아니고, 읽으면서 더 자료가 필요하다 싶은 부분은 자료를 추가하고, 아닌 부분은 줄이는 등 수정을 할 것이기 때문에 
뭔가 읽다가 불편하시다면 릴리안의 오리지널 포스트를 읽으시길 추천드립니다.


그러면 시작하도록 하겠습니다 :)

## <mark style='background-color: #fff5b1'> What is Reinforcement Learning? </mark>

자, 어떤 알려지지 않은 환경에 ```agent```(로봇이라고 생각하면 될듯)가 놓여져 있다고 생각합시다, 그리고 이 agent는 환경과 상호작용을 통해서 어떠한 ```rewards```(보상)을 얻을 수 있다고 합시다. <br>
이 로봇은 결과적으로 누적 보상을 최대화 하는방향으로 액션을 취해야 할 것입니다. (여기서 누적 보상이라고 하는 이유는, 현재 보상만 최대화 하는 방향으로 행동하는게 미래에는 안좋을 수 있기 때문이죠, 예를 들어 우리가 지금 강화학습을 공부하는게 고통스러울 지언정, 미래엔 도움이 될 것이고 (그렇겠죠...?), 지금 랩탑을 덮고 피시방에서 롤 하러 가는건 당장은 해피할 지 몰라도 미래에는...) <br>
실제, real world에서 위에서 얘기한 것들은 가령 게임에서 고득점을 얻거나 상대방을 이기는 것 등을 목적으로 하는 로봇들을 예시로 들 수 있을겁니다.      


![image](https://user-images.githubusercontent.com/48202736/106113304-afa65d80-6191-11eb-8486-3b9f8fba7cb7.png)
{: style="width: 70%;" class="center"}
*Fig. 1. 그림이 전달하는 바는 명확합니다, 우리가 현재 상황에서 어떤 액션을 취하면 환경에 그만한 변화가 생긱고, 그렇기 때문에 생기는 reward를 agent가 받게 되는거죠.*

강화학습 (Reinforcement Learning, RL)의 목적은 시행 착오(trial and error, 이는 모든 머신러닝의 기본적인 매커니즘이죠)를 통해서(실험 환경에서의 시행 착오) '이런 상황에서 이런 행동을 했더니 이렇던데?' 같은 feedback 통해 가장 좋은 전략(strategy)을 학습하는 겁니다. <br>
Optimal한 전략을 가지고 있으면, agent는 future rewards를 최대화 하는 방향으로 환경에 적응할 수 있을 겁니다. <br>



### <mark style='background-color: #fff5b1'> Key Concepts </mark>

자 이제 RL에서 쓰이는 중요한 concept, notation에 대해 알아보도록 합시다 :)


우선 agent가 행동하는(acting)곳은 바로 ```environment```(환경)입니다. <br>
agent가 한 특정 행동에 대해서 어떻게 환경이 반응 하는지는 ```model``` 로부터 정의되는데요, 이 모델은 우리가 알 수도 있고 모를 수도 있습니다. <br>
agent는 환경의 수많은 ```states```(상태) ($$s \in \mathcal{S}$$)들 중 하나에 있을겁니다. (격자 구조 미로라면 미로의 한 칸, 바둑을 두는 상황이라면 바둑이 놓여진 어느 한 상황) <br>
그리고 그 주어진 상태에서 취할 수 있는 수많은 ```actions```(행동) ($$a \in \mathcal{A}$$) 중 하나를 취할겁니다. 그러면 당연히 상태가 변하겠죠? (미로에서 움직였으면 상태가 변하는거고, 현재 상황의 바둑판에 돌을 두면 그 다음 state가 되는거죠)<br>
근데 agent가 어느 state에 도착할지는 ```transition probabilities between states``` ($$P$$)에 의해서 결정됩니다. (어떤 상태 $$s_{11}$$에서 $$s_{12}$$,$$s_{13}$$...등으로 갈 확률을 나타내는 테이블이 있어요)<br>
그리고 일단 액션을 취하면 envirionment가 '니가 취한 행동의 대가야'라며 ```reward``` ($$r \in \mathcal{R}$$)를 피드백으로 줍니다.<br>


The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP). Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL. This is not the focus of this post though. 
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm. Most of the following content serves the scenarios when the model is unknown.

The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **<span style="color:#e01f1f">the goal to maximize the total rewards</span>**. Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. In other words, the value function quantifies how good a state is. Both policy and value functions are what we try to learn in reinforcement learning.


**<span style="color:#e01f1f">the goal to maximize the total rewards</span>**

![image](https://user-images.githubusercontent.com/48202736/106113925-57bc2680-6192-11eb-9427-f78d9dc6407f.png)
{: style="width: 100%;" class="center"}
*Fig. 2. Summary of approaches in RL based on whether we want to model the value, policy, or the environment. (Image source: reproduced from David Silver's RL course [lecture 1](https://youtu.be/2pWv7GOvuf0).)*


The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $$t=1, 2, \dots, T$$. During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy. Let's label the state, action, and reward at time step t as $$S_t$$, $$A_t$$, and $$R_t$$, respectively. Thus the interaction sequence is fully described by one **episode** (also known as "trial" or "trajectory") and the sequence ends at the terminal state $$S_T$$:

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$


Terms you will encounter a lot when diving into different categories of RL algorithms:
- **Model-based**: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.
- **Model-free**: No dependency on the model during learning.
- **On-policy**: Use the deterministic outcomes or samples from the target policy to train the algorithm.
- **Off-policy**: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.


#### Model: Transition and Reward

The model is a descriptor of the environment. With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. The model has two major parts, transition probability function $$P$$ and reward function $$R$$.

Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r. This is known as one **transition** step, represented by a tuple (s, a, s', r).

The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r. We use $$\mathbb{P}$$ as a symbol of "probability".

$$
P(s', r \vert s, a)  = \mathbb{P} [S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a]
$$

Thus the state-transition function can be defined as a function of $$P(s', r \vert s, a)$$:

$$
P_{ss'}^a = P(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

The reward function R predicts the next reward triggered by one action:

$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$


#### Policy

Policy, as the agent's behavior function $$\pi$$, tells us which action to take in state s. It is a mapping from state s to action a and can be either deterministic or stochastic:
- Deterministic: $$\pi(s) = a$$.
- Stochastic: $$\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]$$.


#### Value Function

Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward. The future reward, also known as **return**, is a total sum of discounted rewards going forward. Let's compute the return $$G_t$$ starting from time t:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

The discounting factor $$\gamma \in [0, 1]$$ penalize the rewards in the future, because:
- The future rewards may have higher uncertainty; i.e. stock market.
- The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;).
- Discounting provides mathematical convenience; i.e., we don't need to track future steps forever to compute return.
- We don't need to worry about the infinite loops in the state transition graph.

The **state-value** of a state s is the expected return if we are in this state at time t, $$S_t = s$$:

$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]
$$

Similarly, we define the **action-value** ("Q-value"; Q as "Quality" I believe?) of a state-action pair as:

$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]
$$

Additionally, since we follow the target policy $$\pi$$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value:

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

The difference between action-value and state-value is the action **advantage** function ("A-value"):

$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$


#### Optimal Value and Policy

The optimal value function produces the maximum return:

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

The optimal policy achieves optimal value functions:

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

And of course, we have $$V_{\pi_{*}}(s)=V_{*}(s)$$ and $$Q_{\pi_{*}}(s, a) = Q_{*}(s, a)$$.

