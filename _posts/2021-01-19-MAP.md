---
title: Maximum A Posterior, MAP
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- 최대사후확률 (Maximum A Posterior)란 무엇인가?

지난 글에서 MLE에 대해서 간단하게 살펴봤습니다. 이제 MAP에 대해서 이야기해보도록 하겠습니다. 

마찬가지로 다음의 수식과 정의에 대해서 익숙해지셔야 합니다. 

1. $$likelihood : p(x\mid\theta)$$ 

2. $$posterior \propto likelihood \times prior : p(\theta \mid x) \propto p(x \mid \theta)p(\theta)$$



- MAP

지난 글에서 MLE의 문제점에 대해 이야기하면서 MAP를 잠깐 언급했었습니다.

![image](https://user-images.githubusercontent.com/48202736/105208721-584e2f00-5b8c-11eb-9ddd-97d9ca2244d3.png)

MLE에서 한발 더 나아가 likelihood에 prior를 곱해 파라메터의 값들에 대한 정보를 주어 posterior를 구하고 이것을 maximize하는 단 하나의 솔루션을 구하는게 MAP 라고 할 수 있습니다.


이럴 경우 데이터가 적을 때는 prior의 영향력이 커서 추정 값이 너무 편향되지 않게 잡아주고, 데이터가 많을 경우에는 likelihood의 영향력이 커져 결국 데이터로부터 얻은 정보를 최대한 활용하게 된다고 볼 수 있는 장점이 있는 방법입니다.



이해를 돕기위해 그림을 첨부했습니다. 
아래를 보시면 맨 오른쪽에 있는 posterior는 likelihood와 prior를 곱한 것의 분포입니다.

![image](https://user-images.githubusercontent.com/48202736/105036806-5f980e80-5aa0-11eb-8638-e7a875dba603.png)



- Prior란?

prior는 말그대로 사전 지식입니다. 이전에 MLE에는 prior가 없었는데 prior가 있다는 것의 의미는 무엇일까요? 바로 추정하고자 하는 파라메터(가우시안의 경우 mean, variance)에 대해서 '실험은 안해봤지만 그냥 일반적으로 보니까 mean,variance가 값이 1일 확률이 가장 높던데?' 같은 정보를 주는겁니다. prior는 바로 (추정하고자 하는 확률 분포의)파라메터에 대한 사전 확률 분포(데이터를 보기 전에 임의로 주는)인거죠.


Univarite Normal Distribution (Gaussian Distribution)의 수식과 그림
![image](https://user-images.githubusercontent.com/48202736/105276366-9119f280-5be4-11eb-9e93-7ed5a1f432e4.png)
![image](https://user-images.githubusercontent.com/48202736/105276373-95dea680-5be4-11eb-87f2-f20cd78b4c3d.png)

Normal Inverse Gamma Distribution의 수식과 그림
![image](https://user-images.githubusercontent.com/48202736/105276386-9d05b480-5be4-11eb-87e4-9a0a7cc9ef1a.png)
![image](https://user-images.githubusercontent.com/48202736/105276384-9b3bf100-5be4-11eb-9053-6cd26135c930.png)

위의 두 분포는 Conjugate Distributions이다.



이때 likelihood가 Normal Distribution(가우시안 분포)를 따른다면 prior로는 conjugate distribution인 Normal Inverse Gamma Distribution을 따르는 것이 좋습니다.
왜냐면 아래의 수식처럼 둘을 곱할건데 conjugate family인 분포로 모델링을 하게 되면 수학적으로 굉장히 쉽게 계산이 되기 때문이죠



$$posterior \propto likelihood \times prior : p(\theta \mid x) \propto p(x \mid \theta)p(\theta)$$



- MLE vs MAP



maximum likelihood를 사용하는 빈도적 확률 관점(frequent) 과 prior를 추가해 posterior를 사용하는 베이지안 확률 관점(MAP가 bayes' rule에서 기인함) 중 어떤 것이 더 상대적으로 우수한지에 대해서는 끊임없이 논쟁이 있다고 합니다. 


여기서 베이지안 접근법에 대해 널리 알려진 비판 중 하나는 바로 사전 분포가 실제 사전의 믿음을 반영하기 보다는 수학적인 편리성을 위해 선택하는 것이 아니냐 라는 것이라고 합니다.


위에서 말한 prior를 likelihood의 conjugate distribution으로 설정하는것이 주관이 포함된게 아니냐는 것인데, 
그렇기 때문에 MAP, bayesian을 사용할 때 Jeffreys Prior 등의 주관이 들어가지 않은, 무정보적(non-informative) prior를 사용하기도 한다고 합니다. 


- 수식으로 보는 MAP

사족이 많았는데, 이제 수식으로 MAP를 알아보도록 하겠습니다.
우리가 하고 싶은것은 posterior를 maximize 하는 것입니다.

<center>$$\hat{\Theta}=argmax_\theta[Pr(\theta \mid x_{1...I})]$$</center>

여기서 잘 알려진 Bayes' Rule을 사용하면 다음과 같이 나타낼 수 있습니다.

<center>$$Pr(\theta \mid x_{1...I})=\frac{Pr(x_{1...I} \mid \theta)Pr(\theta)}{Pr(x_{1...I})}$$</center>

<center>$$\hat{\Theta}=argmax_\theta[\frac{Pr(x_{1...I} \mid \theta)Pr(\theta)}{Pr(x_{1...I})}]$$</center>

<center>$$\hat{\Theta}=argmax_\theta[\frac{\prod_{i=1}^{I}Pr(x_i \mid \theta)Pr(\theta)}{Pr(x_{1...I})}]$$</center>

여기서 분모에 있는 것은 Evidence 라고도 하는데, 우리가 추정하고자 하는 파라메터와 관련이 없으므로 떼어놓고 생각할 수 있습니다.

