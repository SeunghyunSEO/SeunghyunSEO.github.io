---
title: (미완) Lecture 8 - Deep RL with Q-Functions

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)

Lecture 8의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=7-D8RL3D6CI&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=32)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 다룰 내용은 앞서 배운 Value-based 방법론인 `Q-Learning`을 사용한 practical Deep RL 알고리즘 (Deep Q-Learning) 입니다.

![slide1](/assets/images/CS285/lec-8/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap: Q-learning </mark>

Q-iteration에 대한 Recap으로 강의를 시작합니다.

Value-based 강화학습 알고리즘들은 policy를 따로 명시적으로 (explicitly) 나타내지 않아도 됐죠.
하지만 Transition Probability를 알아야 하는 대부분의 Model-based Value-based 알고리즘들이 현실 세계에 잘 적용되지 않는다는 문제를 개선하기 위해서
강의의 마지막엔 $$V(s)$$ 를 $$Q(s,a)$$로 대체한 Model-free Value-based 알고리즘, 그중에서도 `Fitted Q-Iteration`, `Q-Learning` 에 대해서 알아봤습니다.
Q-Learning 은 optimal policy를 얻는 것이 보장되지 않은 알고리즘이었으나 `이를 개선하면 충분히 practical한 알고리즘을 만들어낼 수 있음`을 시사했는데요, 오늘 알아볼 내용이 바로 이것입니다.

![slide2](/assets/images/CS285/lec-8/slide2.png)
*Slide. 2.*

슬라이드의 왼쪽에는 Batch-mode Q-Iteration과 Online-mode Q-Iteration이 나와있고, 오른쪽에는 이 강의 내내 사용한 Deep RL의 Anatomy가 Q-Learning에 대해 fitting 되어있습니다.

간단히 리뷰해보면 `General Fitted Q-Iteration`은

```
for 학습 데이터를 샘플링 함 (s,a,s',r). (max operator 덕분에 `Off-Policy`로 데이터 샘플 가능하며, 얼마나 많이 transition할지는 정하기 나름)
  for i in range(K) : << (K도 정하기 나름)
    (s,a,s',r) tuple을 이용해서 y_i, 즉 타겟을 구함. 
    파라메터를 한 스텝 업데이트.
```

였는데, 위에서 말한 각 step을 몇 번 반복할지를 의미하는 하이퍼파라메터들을 모두 1로 설정하면 `Online Q-Iteration`이 되는데 이를 `Q-Learning`이라고 합니다.
(딱 한번만 $$s_i$$에서 $$a_i$$를 취하고, 이로부터 발생하는 $$s'_i,r_i$$를 얻어서 2,3번 스텝으로 파라메터를 딱 한스텝만 업데이트 합니다.)








### <mark style='background-color: #dcffe4'> Problems of Q-Learning  </mark>

![slide3](/assets/images/CS285/lec-8/slide3.png)
*Slide. 3.*

일반적인 Q-Learning의 문제점은 뭐였을까요?

*Slide. 3.*에서 보이는 step 3가 `gradient descent`처럼 보이지만 사실은 그렇지 않다는게 문제였죠.
다시 써보면 원래의 Q-Learning 업데이트 수식이 아래와 같았지만,

$$
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) - y_i )
$$

$$y_i$$에 step 2수식을 대입하면 아래와 같이 전개할 수 있고,

$$
\begin{aligned}
&
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( \color{red}{ Q_{\phi}(s_i,a_i) } - y_i )
& \\

&
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( \color{red}{ r(s_i,a_i) + \gamma max_a Q_{\phi} (s'_i,a'_i) } - y_i )
& \\
\end{aligned}
$$

Target Value인 `max operator` 부분에 gradient가 흐르지 않습니다. (max operator 덕분에 off-policy 알고리즘으로 발전할 수 있었긴 하지만요)
그래서 `chain rule`을 적용하기 힘들기 때문에, `residual gradient algorithm`이라고 불리는 방법을 사용하긴 하지만 별로 잘 작동하지는 않았습니다.

Online Q-Learning에는 또 하나의 문제점이 있었는데요, 바로 한 번에 하나의 transition만 샘플하면 sequential transition들이 highly correlated 되어 있다는 것이었는데요, 즉 time-step $$t$$에서 본 state, $$s_t$$와 $$s_{t+1}$$이 굉장히 유사할거라는 겁니다. 
그 말인 즉, step 3에서 gradient step을 한스텝 진행할 때도 문제를 야기한다고 하는데요, 곧 솔루션과 함께 제대로 다룬다고 합니다.




이제 `Correlation Problem`에 대해서 제대로 얘기를 해보도록 하겠습니다.

![slide4](/assets/images/CS285/lec-8/slide4.png)
*Slide. 4.*

*Slide. 4.*에는 알고리즘이 2 step이 되었는데요, 큰 변화는 없고 target을 evaluation하는 부분을 step 3에 넣었을 뿐 똑같습니다.
Correlation Problem이란 t번째 state, $$s_t$$와 t+1번째 $$s_{t+1}$$이 굉장히 유사하거나 유사하지 않더라도 관계가 깊다는 겁니다.
그리고 Optimization 하는 과정이 MSE Loss를 사용한 Supervised Regression 문제 같이 보이지만 Target Value는 계속해서 변화합니다.

어떤 일련의 Trajectory에 대해서 생각해보도록 하겠습니다.

![correlation1](/assets/images/CS285/lec-8/correlation1.png){: widht="80%"}
*Fig. Trajectory 1-1*

위의 Trajectory에 대해서 $$s_t,a_t,r_t,s_{t+1}$$ 을 여러 세트로 해서 최적화를 한다고 하면 $$s_1,s_2,s_3$$ 간에 서로 비슷한 transition을 하기 떄문에 네트워크는 굉장히 이 transition에 대해서 locally over-fitting 하게 될겁니다.

![correlation2](/assets/images/CS285/lec-8/correlation2.png){: widht="80%"}
*Fig. Trajectory 1-2*

마찬가지로 over-fitting 하겠죠?

![correlation3](/assets/images/CS285/lec-8/correlation3.png){: widht="80%"}
*Fig. Trajectory 1-3*

그리고 새로운 Trajectory에 대해서 네트워크는 굉장히 좋지 못한 Q값을 예측하게 될겁니다.


![slide5](/assets/images/CS285/lec-8/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-8/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-8/slide7.png)
*Slide. 7.*






## <mark style='background-color: #fff5b1'> Target Networks </mark>

![slide9](/assets/images/CS285/lec-8/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-8/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-8/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-8/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-8/slide13.png)
*Slide. 13.*




## <mark style='background-color: #fff5b1'> A General View of Q-Learning </mark>

![slide15](/assets/images/CS285/lec-8/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-8/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-8/slide17.png)
*Slide. 17.*






## <mark style='background-color: #fff5b1'> Improving Q-Learning </mark>

![slide19](/assets/images/CS285/lec-8/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-8/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-8/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-8/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-8/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-8/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-8/slide25.png)
*Slide. 25.*




## <mark style='background-color: #fff5b1'> Q-Learning with Continuous Actions </mark>

![slide27](/assets/images/CS285/lec-8/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-8/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-8/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-8/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-8/slide31.png)
*Slide. 31.*




## <mark style='background-color: #fff5b1'> Implementation Tips and Examples </mark>

![slide33](/assets/images/CS285/lec-8/slide33.png)
*Slide. 33.*

![slide34](/assets/images/CS285/lec-8/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-8/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-8/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-8/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-8/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-8/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-8/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-8/slide41.png)
*Slide. 41.*



### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)









