---
title: Lecture 8 - Deep RL with Q-Functions

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)

Lecture 8의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=7-D8RL3D6CI&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=32)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 다룰 내용은 앞서 배운 Value-based 방법론인 `Q-Learning`을 사용한 practical Deep RL 알고리즘 (Deep Q-Learning) 입니다.

![slide1](/assets/images/CS285/lec-8/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap: Q-learning </mark>

Q-iteration에 대한 Recap으로 강의를 시작합니다.

Value-based 강화학습 알고리즘들은 policy를 따로 명시적으로 (explicitly) 나타내지 않아도 됐죠.
하지만 Transition Probability를 알아야 하는 대부분의 Model-based Value-based 알고리즘들이 현실 세계에 잘 적용되지 않는다는 문제를 개선하기 위해서
강의의 마지막엔 $$V(s)$$ 를 $$Q(s,a)$$로 대체한 Model-free Value-based 알고리즘, 그중에서도 `Fitted Q-Iteration`, `Q-Learning` 에 대해서 알아봤습니다.
Q-Learning 은 optimal policy를 얻는 것이 보장되지 않은 알고리즘이었으나 `이를 개선하면 충분히 practical한 알고리즘을 만들어낼 수 있음`을 시사했는데요, 오늘 알아볼 내용이 바로 이것입니다.

![slide2](/assets/images/CS285/lec-8/slide2.png)
*Slide. 2.*

슬라이드의 왼쪽에는 Batch-mode Q-Iteration과 Online-mode Q-Iteration이 나와있고, 오른쪽에는 이 강의 내내 사용한 Deep RL의 Anatomy가 Q-Learning에 대해 fitting 되어있습니다.

간단히 리뷰해보면 `General Fitted Q-Iteration`은

```
for 학습 데이터를 샘플링 함 (s,a,s',r). (max operator 덕분에 `Off-Policy`로 데이터 샘플 가능하며, 얼마나 많이 transition할지는 정하기 나름)
  for i in range(K) : << (K도 정하기 나름)
    (s,a,s',r) tuple을 이용해서 y_i, 즉 타겟을 구함. 
    파라메터를 한 스텝 업데이트.
```

였는데, 위에서 말한 각 step을 몇 번 반복할지를 의미하는 하이퍼파라메터들을 모두 1로 설정하면 `Online Q-Iteration`이 되는데 이를 `Q-Learning`이라고 합니다.
(딱 한번만 $$s_i$$에서 $$a_i$$를 취하고, 이로부터 발생하는 $$s'_i,r_i$$를 얻어서 2,3번 스텝으로 파라메터를 딱 한스텝만 업데이트 합니다.)








### <mark style='background-color: #dcffe4'> Problems of Q-Learning  </mark>

![slide3](/assets/images/CS285/lec-8/slide3.png)
*Slide. 3.*

일반적인 Q-Learning의 문제점은 뭐였을까요?

*Slide. 3.*에서 보이는 step 3가 `gradient descent`처럼 보이지만 사실은 그렇지 않다는게 문제였죠.
다시 써보면 원래의 Q-Learning 업데이트 수식이 아래와 같았지만,

$$
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) - y_i )
$$

$$y_i$$에 step 2수식을 대입하면 아래와 같이 전개할 수 있고,

$$
\begin{aligned}
&
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) - \color{red}{  y_i } )
& \\

&
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) -  \color{red}{ [r(s_i,a_i) + \gamma max_a Q_{\phi} (s'_i,a'_i)] } )
& \\
\end{aligned}
$$

Target Value인 `max operator` 부분에 gradient가 흐르지 않습니다. (max operator 덕분에 off-policy 알고리즘으로 발전할 수 있었긴 하지만요)
그래서 `chain rule`을 적용하기 힘들기 때문에, `residual gradient algorithm`이라고 불리는 방법을 사용하긴 하지만 별로 잘 작동하지는 않았습니다.

Online Q-Learning에는 또 하나의 문제점이 있었는데요, 바로 한 번에 하나의 transition만 샘플하면 sequential transition들이 highly correlated 되어 있다는 것이었는데요, 즉 time-step $$t$$에서 본 state, $$s_t$$와 $$s_{t+1}$$이 굉장히 유사할거라는 겁니다. 
그 말인 즉, step 3에서 gradient step을 한스텝 진행할 때도 문제를 야기한다고 하는데요, 곧 솔루션과 함께 제대로 다룬다고 합니다.




이제 `Correlation Problem`에 대해서 제대로 얘기를 해보도록 하겠습니다.

![slide4](/assets/images/CS285/lec-8/slide4.png)
*Slide. 4.*

*Slide. 4.*에는 알고리즘이 2 step이 되었는데요, 큰 변화는 없고 target을 evaluation하는 부분을 step 3에 넣었을 뿐 똑같습니다.
Correlation Problem이란 t번째 state, $$s_t$$와 t+1번째 $$s_{t+1}$$이 굉장히 유사하거나 유사하지 않더라도 관계가 깊다는 겁니다. 이게 왜 문제냐면 비슷한 입력과 결과를 내는 데이터를 매우 많이 학습하는것은 비효율적이고 네트워크가 오버피팅되는 등의 악영향을 줄 수 있기 때문입니다.

(+ 또하나의 문제로는 후에 다룰거지만 Optimization 하는 과정이 MSE Loss를 사용한 Supervised Regression 문제 같이 보이지만 Target Value가 계속 변화하는 문제가 있습니다.)

아무튼 어떤 일련의 Trajectory에 대해서 생각해보도록 하겠습니다.

![correlation1](/assets/images/CS285/lec-8/correlation1.png){: widht="80%"}
*Fig. Trajectory 1-1*

위의 Trajectory에 대해서 $$s_t,a_t,r_t,s_{t+1}$$ 을 여러 세트로 해서 최적화를 한다고 하면 $$s_1,s_2,s_3$$ 간에 서로 비슷한 transition을 하기 떄문에 네트워크는 굉장히 이 transition에 대해서 locally over-fitting 하게 될겁니다.

![correlation2](/assets/images/CS285/lec-8/correlation2.png){: widht="80%"}
*Fig. Trajectory 1-2*

마찬가지로 over-fitting 하겠죠?

![correlation3](/assets/images/CS285/lec-8/correlation3.png){: widht="80%"}
*Fig. Trajectory 1-3*

그리고 새로운 Trajectory에 대해서 네트워크는 굉장히 좋지 못한 Q값을 예측하게 될겁니다.

이는 Actor-Critic의 Online version에서도 겪었던 비슷한 문제이며, 우리는 이 문제를 해결하기 위해서 동기식, 비동기식 `Data Parallel`을 진행했습니다.
여러 Worker들을 사용해서 데이터를 뽑아서 다양성을 높히는 거였죠.
이러한 전략은 Q-Learning 자체가 직전의 policy로만 샘플할 데이터를 써도되지 않는, 이른 바 Off-Policy 알고리즘이기 때문에 더 잘 맞는다고 합니다.

![slide5](/assets/images/CS285/lec-8/slide5.png)
*Slide. 5.*

이런 correlated sample 문제를 해결하는 더 간단한 방법이 있는데요, 바로 `Replay Buffer`를 이용하는 겁니다.
Replay Buffer를 사용하는 아이디어는 RL에서 굉장히 오래됐으며 1990년대에 제안되었다고 합니다.
아이디어는 간단한데요, locally 비슷한 state sample들을 쓰지 않고 여러군데서 랜덤하게 뽑은 state들로 네트워크를 학습하자는 겁니다.

*Slide. 5.*에서 보시면 Full Fitted Q-Iteartion 에서 1번 데이터를 모으는 과정이 생략되었고 이전에 모아둔 data들이 모인 Replay Buffer에서 무작위로 가져오는걸로 변경됐습니다.

```
전형적인 Data-Driven Approach인 Deep Learning처럼 dataset에서 random batch를 뽑아 쓰는 것처럼 됐습니다.
```

![slide6](/assets/images/CS285/lec-8/slide6.png)
*Slide. 6.*

위의 슬라이드에서 보시면 이제 데이터를 그냥 배치를 원하는 만큼 샘플합니다.
(이 때 데이터들은 딥러닝 알고리즘들이 사용하는 데이터가 `iid`라고 가정하는 것과 같아진다고 합니다.)  

그리고 배치를 1이 아닌 4,8 이런식으로 여러번 하고 gradient를 합쳐서 업데이트 하면 low variance gradient를 얻을 수도 있습니다.

```
하지만 여전히 max term 때문에 정확한 gradient를 계산할 수는 없지만 그래도 correlation 문제는 이런식으로 해결이 가능하다고 합니다.
```

하지만 리플레이 버퍼를 사용할 때의 문제점이 아직 존재합니다.
"리플레이 버퍼에 넣을 데이터를 어디서 샘플하지? 혹은 시간이 지날수록 어떻게 더 유의미한 데이터를 넣을까?" 인데요, 
초기 policy는 랜덤하거나 매우 좋지않은 policy이기 때문에 ($$s,a,r,s'$$)이 별로 좋지 않을 수 있기 때문에, 학습이 진행될 수록
업데이트된 그럴싸한 policy를 사용해서 추가적으로 뽑은데이터를 넣거나 해야 수렴이 더 잘된다는 겁니다.

우리가 말한 전략을 그림으로 나타내면 아래와 같습니다.

![replay_buffer_update](/assets/images/CS285/lec-8/replay_buffer_update.png)
*Fig. How to update replay buffer*

간단합니다.
우리는 ($$s,a,r,s'$$) 같은 `transition data`를 엄청 가지고 있는데,

```
(s1,a1,r1,s2)
(s2,a2,r2,s3)
...
(s_{n-1},a_{n-1},r_{n-1},s_n)
```

학습을 조금 진행한 뒤에 이 버퍼에 최근 policy로 구한 transition data를 더 추가하는겁니다.

```
(s1,a1,r1,s2)
(s2,a2,r2,s3)
...
(s_{n-1},a_{n-1},r_{n-1},s_n)
...
(s_{2n-1},a_{2n-1},r_{2n-1},s_{2n})
```


![slide7](/assets/images/CS285/lec-8/slide7.png)
*Slide. 7.*

*Slide. 7.*는 방금까지 얘기한 내용을 한 슬라이드로 정리한 것입니다.

알고리즘이 그렇게 어려워 보이지는 않죠.
Q 함수를 근사한 뉴럴 네트워크를 업데이트하는 Value-based Objective와 Replay-Buffer 를 간단하게 적용하는게 전부입니다.
하지만 여기에 추가적으로 거대한 state space를 근사하는 네트워크와 Target Network를 사용하는 등 학습의 안정성과 효율성을 높히면 굉장히 놀라운 결과를 보여주게 되는데요, 그것이 바로 아직까지도 많이 사용되는 Value-Based Deep RL 알고리즘들의 근간이 되는  `Deep-Q-Network (DQN)` 입니다.

![dqn_illustration](/assets/images/CS285/lec-8/dqn_illustration.png)
*Fig. The deep Q-network (DQN). from [A Brief Survey of Deep Reinforcement Learning](https://arxiv.org/pdf/1708.05866)*

![2015_dqn](/assets/images/CS285/lec-8/2015_dqn.png)
*Fig. Schematic illustration of the convolutional neural network in Deep-Q-Network (DQN). from [2015, Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)*

`DQN`은 2013년에 제안된 딥러닝과 강화학습 결합한 네트워크이며,
`Atari`라는 비디오 게임에서 인간 수준의 퍼포먼스를 보여주며 심층 강화학습의 가능성을 보여준 간단하지만 강력한 네트워크입니다.
(이전에도 신경망과 강화학습을 결합한 Approach는 있었으나 잘 안되었으며, 2013년에 Arxiv에 게제된 DeepMind의 논문이 첫 성공 사례입니다.)

![2013_dqn](/assets/images/CS285/lec-8/2013_dqn.png)
*Fig. Screenshots from five Atari 2600 Games: (Left-to-right) Pong, Breakout, SpaceInvaders,  Seaquest, Beam Rider from [2013, Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)*



## <mark style='background-color: #fff5b1'> Target Networks </mark>

지금까지 Q-Learning의 몇가지 문제점에 대해 생각해보고 이를 해결하면서 practical한 알고리즘을 만들려고 노력했지만
아직 트롤링을 하는 term이 있죠, 이것 때문에 Replay Buffer를 사용했음에도 여전히 학습이 불안정한데요,

![slide9](/assets/images/CS285/lec-8/slide9.png)
*Slide. 9.*

바로 Objective의 두 번째 term인 max term 입니다.

$$
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) -  \color{red}{ [r(s_i,a_i) + \gamma max_a Q_{\phi} (s'_i,a'_i)] } )
$$

Q-Learning은 gradient descent 같이 보이지만 그건 아니고, 또한 `moving target`을 따라가야 하는 문제를 가지고 있습니다.
그러니까 예를 들어 MSE Loss를 쓰는 회귀문제에서 target이 움직이는거나 다름 없기 때문에 네트워크가 수렴하기 어렵습니다.

![slide10](/assets/images/CS285/lec-8/slide10.png)
*Slide. 10.*

그렇기 때문에 우리는 Q-Learning에 `어떤 Target Network`라고 하는 것을 사용해서 Full Batch Fitted Q-Iteration과 Online Q-Learning을 결합한(?) 방법을 써보려고 합니다.

![slide11](/assets/images/CS285/lec-8/slide11.png)
*Slide. 11.*

그러니까 문제는 Target이 움직이는건데, 왜 움직이는지를 생각해보면 원래 Q-Learning의 target은 $$Q_{ \color{red}{\phi}}(s',a')$$, 즉 $$\phi$$로 파라메터화된 네트워크의 output을 사용하는건데, update step에서 파라메터 $$\phi$$가 업데이트되기 때문에 그게 문제인겁니다. 이를 피하기 위해서 제안된 아이디어는 "$$\phi'$$를 써보면 어떨까?" 입니다. 그러니까 Q-Network, $$Q_{\phi}$$의 파라메터가 자꾸 변하니까 `Target Network`, $$Q_{\color{blue}{\phi}}$$를 따로 두자는 거죠. 

![target_network](/assets/images/CS285/lec-8/target_network.png)
*Fig.*

그 결과 알고리즘은 버퍼에서 데이터를 샘플해서 데이터에 대해서 Q-Network를 충분히 여러번 반복적으로 업데이트 하고 충분히 업데이트 했으면 Target Network를 다시 최근의 Q-Network로 업데이트해주고, 다시 또 Target은 고정을 하고... 를 반복하는 겁니다. (*Slide. 11.* 에서 1번과 4번의 $$\phi'$$가 핵심입니다.)

(여기서 잊지말아야 할 점은 앞서 말한것 처럼 리플레이 버퍼에 데이터를 추가해주는 과정도 있다는 겁니다.)

![target_network2](/assets/images/CS285/lec-8/target_network2.png)
*Fig.*

K는 1~4정도의 값으로 반복하고, N은 10000정도가 될 수 있다고 합니다.

![slide12](/assets/images/CS285/lec-8/slide12.png)
*Slide. 12.*

이제 여태까지 배운 것을 바탕으로 `Deep Q-Learning (Deep-Q-Network; DQN)`을 정의할 수 있는데요,
*Slide. 12.*에 잘 나와있습니다. 
슬라이드에는 두 가지 알고리즘이 있는데요 아래에 나와있는 "Classic" DQN이 위의 알고리즘의 k=1일 때의 special case입니다.


Target Network를 다루는 데에는 또 다른 방법이 있습니다.

![slide13](/assets/images/CS285/lec-8/slide13.png)
*Slide. 13.*

"Classic" DQN을 보면 5번의 update룰이 조금 위화감이 느껴질 수 있는데요, 
바로 우리가 Moving Target 문제를 해결하고자 $$\phi'$$로 고정된 네트워크로 타겟을 생성하고 이를 바탕으로 $$\phi$$를 업데이트 하고 있었는데 이게 어느순간 $$\phi$$로 확 변해버려서 결국엔 Moving Target처럼 느껴질 수 있다는 점입니다.

그러니까 K=1 (gradient step 반복 횟수), N=4 (배치 데이터 샘플 횟수, 실제론 매우 큼) 라고 생각할 때, N=4에서 5로 넘어가는 순간 갑자기 $$\phi$$가 되버리는게 문제가 될 수 있다는 겁니다 (실제로는 별로 큰문제는 아니랍니다).

그래서 $$\phi' \leftarrow \phi$$ 가 아니라 조금 더 자연스럽게 $$\phi' \leftarrow (\tau)\phi' + (1-\tau)\phi $$ 로 업데이트하는 방식을 쓰기도 한다고 하며, 이와 비슷한 방법론을 `Polyak Averaging`라고 한다고 합니다.

$$\phi' \leftarrow ( \color{red}{ \tau } )\phi' + ( 1 - \color{red}{ \tau } )\phi, \text{ where } \tau \text{ is large value, like } \color{red}{ 0.999 } $$

여기서 "NN은 선형적이지 않은데 linearly interpolation을 해도 되나?" 하는 의문점이 들 수 있는데요, old policy가 current policy의 set이라서 (유사해서) 별로 문제가 되지 않는다고(?) 합니다. (non-linear function들에 대한 linear interpolation이 궁금하시다면 `Polyak Averaging`을 살펴보라고 하십니다.) 








***

### <mark style='background-color: #dcffe4'> DQN: Algorithm and Pseudo Code </mark>

DQN을 제안한 원본 논문에서는 아래와 같이 알고리즘을 설명하고 있습니다.
강의 Slide와 크게 다르지 않으나 원본이 궁금하신 분이 계실 것 같아서 첨부해봤습니다.

![dqn_annotated](/assets/images/CS285/lec-8/dqn_annotated.png)
*Fig. Original DQN Algorithm from [2013, Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)*

이에 따른 수도코드도 아래와 같이 간단하게 작성해볼 수 있습니다.

![dqn_pseudo](/assets/images/CS285/lec-8/dqn_pseudo.png)
*Fig. Pseudo Code from [CS234 Lecture 6 : CNNs and Deep Q Learning](https://web.stanford.edu/class/cs234/slides/lecture6.pdf)*

+추가) DQN의 Tensorboard Graph 그림을 추가해봤습니다. (출처는 아래에)

![dqn-tensorboard-graph](/assets/images/CS285/lec-8/dqn-tensorboard-graph.png)
*Fig. DQN Tensorboard Graph from [Implementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym](https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html)*

***








## <mark style='background-color: #fff5b1'> A General View of Q-Learning </mark>

이번 subsection에서는 그동안 Q-Iteration, Fitted Q-Iteration, Q-Learning, Online Q-Learning ...등 다양한 버전으로 알아봤던
Q-Function based 알고리즘들을 하나의 General한 Framework로 표현해 보려고 합니다.

```
(사실 별로 내용은 없습니다.)
```

![slide15](/assets/images/CS285/lec-8/slide15.png)
*Slide. 15.*

*Slide. 15.*의 상단에는 Replay Buffer와 Target Network를 사용하는 일반적인 Q-Learning이 나와있고 여기서 N=1, K=1이면 DQN이 됩니다. 
하단에 나와있는 알고리즘도 위와 배치만 다를뿐 같은 의미를 갖습니다.

![slide16](/assets/images/CS285/lec-8/slide16.png)
*Slide. 16.*

Replay Buffer는 유한 차원의 사이즈를 가지고 있기 때문에, 오래된 데이터는 실제로는 주기적으로 버린다고 합니다. 
100만개의 transition이 담겨져 있으면 하나 추가될 때 마다 가장 오래된 걸 하나 버리는거죠.

그리고 ($$s,a,r,s'$$) tuple을 버퍼에 집어넣는걸 `process 1`이라고 하며, 
`process 2`는 Target Network를 Polyak Averaging으로 업데이트하거나 그냥 $$\phi$$를 카피하는 식으로 업데이트 하는 과정입니다.
여기서 $$\phi'$$로 target을 만드는 데 사용되고, $$\phi$$는 실제 Q 값을 추론 하는데 사용됩니다.
Current parameter, $$\phi$$는 추가적으로 process 1의 데이터를 Epsilon Greedy 방법으로 모으는데 사용 됩니다. 
`process 3`는 main process로 버퍼에서 데이터를 불러와서 Target Network로 타겟을 만들고 Current Q Network로 추론해서 둘을 가지고 Q-Network를 학습하는 과정입니다.


![slide17](/assets/images/CS285/lec-8/slide17.png)
*Slide. 17.*

Online Q-Learning은 세 가지 process를 동시에 하고 (버퍼 사이즈가 1이라고 생각해도 될듯) DQN은 replay buffer가 굉장히 크며 process 1,3은 동시에 진행되고 process 2는 여러번 루프를 돌게 됩니다. 





## <mark style='background-color: #fff5b1'> Improving Q-Learning </mark>

이번 subsection에서 다룰 내용은 Q-Learning을 실제 구현해서 문제 해결에 사용할 때, 성능을 높힐 수 있는 방법입니다.

강의는 `"Are the Q-values accurate?"`라는 질문으로 시작되는데요,

![slide19](/assets/images/CS285/lec-8/slide19.png)
*Slide. 19.*

Q-Function은 어떤 state에서 어떤 action을 했을때 미래에 내가 얻을 수 있는 보상들의 합이죠. 
*Slide. 19.*의 상단에 나와있는 그림을 확대해보면,

![dqn_paper_figure1](/assets/images/CS285/lec-8/dqn_paper_figure1.png)

그래프의 x축은 Training Epoch이고 y축은 위의 두개는 episode당 average reward 이며, 아래 두개는 Average Q-Value 입니다. 이는 Atari 게임을 play하는 Agent를 DQN으로 학습시켰을 때의 결과이며, 학습이 진행될수록 평균 Q-Value와 Reward 둘 다 증가하는 것을 볼 수 있습니다.

그 다음으로 우리는 실제 게임에서 매 state (게임 화면) 마다 우리가 가진 네트워크가 Value, $$V(s)$$를 어떻게 예측하는지를 살펴볼 수 있는데요, 아래의 그림은 Atari게임 중 "Breakout (벽돌 깨기)"를 플레이할 때 게임 진행 과정 동안 Value가 어떻게 변하는지를 나타냅니다.

![dqn_paper_figure2](/assets/images/CS285/lec-8/dqn_paper_figure2.png)


벽돌 깨기를 실제로 해보신 분들은 아시겠지만,
그림 3번에서처럼 벽돌의 한쪽을 공략한 뒤 그 뒤로 공을 넘겨버리면 천장을 다 부숴버리면서 좋은 점수를 얻을 수 있는데,
잘 학습된 네트워크는 3번 state에서 공을 벽돌 천장 위로 보내는 action이 좋다는 걸 알고, 즉 Q(s,a(천장위로)) 가 높기 때문에 그 행동을 취하게 됩니다.
4번에서는 천장 위로 공을 보내는 행위가 직접적으로 벽돌을 깨는 행위는 아닌데도 가장 높은 Value를 리턴합니다.

알려준 적은 없지만 DQN으로 사람같이 플레이 하게 된거죠.


그 다음은 Pong이라는 게임에서의 결과인데요,

![dqn_paper_figure3](/assets/images/CS285/lec-8/dqn_paper_figure3.png)

테니스나 탁구 처럼 랠리를 하면서 상대방을 이기는게 목표인 게임입니다.
2번의 순간에 위로 올라가야 한다는 걸 Q(s,a(위))가 가장 높은 값을 리턴함으로써 알려주네요.

하지만 이렇게 Q값이 실제 값을 반영한다는 것만 보여주는게 강의 목표는 아니겠죠? 
문제는 위와같이 성공적으로 학습이 된 경우가 아니라면 Q값은 실제값을 잘 반영하지 못한다는 데 있습니다.

![slide20](/assets/images/CS285/lec-8/slide20.png)
*Slide. 20.*

DQN으로 학습한 결과는 실제 Agent가 가지는 실제로 리턴받을 True Value와 굉장히 동떨어져 있는데요,
분명 Q가 학습을 거듭할수록 높아지고 좋아보이는데, 문제가 있어 보이네요.
이를 `Overestimation Problem`이라고 합니다.

```
여기서 얻는 Reward Value는 discount된 값이고,
실제 값은 Trajectory를 따라가서 정말로 얻게되는 값이며,
Value Estimate이라는 네트워크 출력값은 step 1에서의 Value를 얘기한다고 합니다.
step 1에서 Value를 취하는게 우리는 discounted total reward라고 정의했었으니까요.

만약 Q-Network가 Value를 잘 예측하도록 학습되었다면 이 값은 비슷하게 나올거라고 합니다.
```

(그림에 파란색은 `Double DQN` 이라고 되어있는데, 이는 당연히 DQN의 이런 Overestimation 문제를 해결하기 위해 제시되었겠죠?
바로 밑에서 배울겁니다.)


![slide21](/assets/images/CS285/lec-8/slide21.png)
*Slide. 21.*

왜 Q-Learning 알고리즘은 Overestimation 하는 걸까요? 이는 굉장히 직관적이고 명확하다고 할 수 있다고 하는데요,
Q-Learning의 Target Value를 계산하는 과정은 아래와 같았습니다.

$$
y_j = r_j + \gamma max_{a'_j} Q_{\phi'}(s'_j,a'_j)
$$

바로 여기서 max operator가 overestimation 문제를 야기한다고 하는데요,
이를 이해하기 위해서 어떤 random variable X1, X2가 있다고 생각해보도록 하겠습니다.
X1과 X2는 둘 다 normally distributed random variable 이라고 가정하면, 이 변수들은 true value에 noise가 조금 섞여있는 거라고 볼 수 있습니다. 

그럴경우 간단하게 아래의 수식이 성립함을 알 수 있다고 하는데요,

$$
\mathbb{E} [max(X1,X2)] \geq max( \mathbb{E}[X1], \mathbb{E}[X2] )
$$

왜 이 수식이 성립하는지에 대해서 생각해보자면, max(X1,X2)의 경우 X1, X2중에서 noise가 더 낀 값을 리턴하게 되는데, 만약 X1, X2 분포가 zero-mean normal distribution 이라고 생각해볼 경우 각 변수가 양수, 음수일 확률은 모두 0.5인데 그 말인 즉 둘 다 음수일 경우는 0.25, 하나라도 양수일 경우는 0.75 가 됩니다. 그리고 하나라도 양수면 max를 취했을 때 양수가 나오기 때문에, $$\mathbb{E}[X1]=0,\mathbb{E}[X2]=0$$에 max를 취해서 나오는 0이라는 값보다 양수를 리턴하는 $$\mathbb{E}[max(X1,X2)]$$가 더 크다는 겁니다.

이를 Q-Learning에 대해서 생각해보면, Q-Learning의 Target Value도 사실은 실제 Q값에 noise가 낀 것이라고 볼 수 있는데,

$$
Q_{\phi'}(s',a') \text{ is not perfect, it looks "noisy" }
$$

여기에 max를 취한다는 것은 위에서 말한 것을 생각해 볼 때, 만약 action 수가 4라면 $$\color{red}{max} (Q(s,a_1),Q(s,a_2),Q(s,a_3),Q(s,a_4) )$$ 를 하는 것인데, 각각의 $$Q(s,a')$$이 실제 값보다 더 노이즈가 낀 값이라고 볼 수 있습니다. 즉 위와 똑같이 생각해보면 max로 평가를 거듭할수록 positive error를 계속해서 선택해 나가기 때문에 실제 값과 동 떨어지는 (실제 값보다 더 큰 값으로 유추) 경우가 발생한다는 겁니다.

즉 max operator를 쓰는 Q-Learning은 구조적으로 거듭된 반복으로 Q-Value를 `overestimate (과대평가)` 할 수 밖에 없다는 결론에 다다르게 된다고 합니다.

그렇다면 어떻게 이 문제를 해결할 수 있을까요?

우리가 $$max_{a'} Q_{\phi'}(s',a')$$ 을 구할 때의 수식에서 

$$max_{a'} Q_{\phi'}(s',a') = Q_{\phi'}(s',arg max_{a'} Q_{\phi'} (s',a') )$$ 

액션을 선택하는 매커니즘 속에 존재하는 noise를 decorrelate 할 수 있다면 해결할 수 있다고 얘기합니다.



### <mark style='background-color: #dcffe4'> Double Q-Learning and Double-DQN </mark>

과연 뭐가 문제였을까요? 

$$max_{a'} Q_{\phi'}(s',a') = Q_{\phi'}(s',arg max_{a'} Q_{\phi'} (s',a') )$$ 

에서 $$Q_{\phi'}$$ 부분이 문제를 야기한다고 하는데요, 이를 해결하기 위해서 (noise decorrelation) $$Q_{\phi'}$$로 다른 Q-Network를 사용하자는 방법론이 2010년 NIPS에 처음 제시됐고 ([Double Q-learning](https://papers.nips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf)), 이를 Deep-Q-Learning (DQN)으로 확장한 것이 DeepMind가 AAAI 2016에 발표한 [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461) 입니다.


![slide22](/assets/images/CS285/lec-8/slide22.png)
*Slide. 22.*

key idea는 *Slide. 22.*에도 나와있는 것 처럼 다음과 같습니다.

```
Dont' use the same network to choose the action and evaluate value (Q)
```

슬라이드의 수식을 잘 보시면 target이 다음과 같이 표현된 걸 알 수 있습니다.

$$
\begin{aligned}

&
y_j = r_j + \gamma max_{a'_j} Q_{\phi'} (s'_j,a'_j)
& \scriptstyle{ \text{target value in Q-Learning} } \\

&
Q_{\phi'}(s,a) \leftarrow r + \gamma Q_{\phi'} (s', arg max_{a'} Q_{\phi'} (s',a') )
& \scriptstyle{ \text{the max term in target value above} } \\

&
\color{red}{ Q_{\phi_A} }(s,a) \leftarrow r + \gamma \color{blue}{ Q_{\phi_B} } (s', arg max_{a'} \color{red}{ Q_{\phi_A} } (s',a') )
& \\
 
&
\color{blue}{ Q_{\phi_B} } (s,a) \leftarrow r + \gamma \color{red}{ Q_{\phi_A} } (s', arg max_{a'} \color{blue}{ Q_{\phi_B} } (s',a') )
& \\ 

\end{aligned}
$$

$$Q_{\phi_A}$$는 target value를 평가할 때 $$Q_{\phi_B}$$의 값을 사용합니다. 주의할 점은 action을 선택할 때는 $$Q_{\phi_A}$$를 사용한다는 점이고, $$Q_{\phi_B}$$는 이와 반대로 하면 됩니다. 

이렇게 하면 우리가 문제삼았던 아래의 수식에서

$$max_{a'} Q_{\phi'}(s',a') = Q_{\phi'}(s',arg max_{a'} Q_{\phi'} (s',a') )$$ 

두 번 noise가 추가되는 부분을 서로 관련이 없다고 생각할 수 있기 때문에 noise들이 설령 껴있다고 하더라도 둘은 서로 출처가 다른 noise가 됩니다.
원리는 아래의 경우를 예로 들 때 

$$
\color{red}{ Q_{\phi_A} }(s,a) \leftarrow r + \gamma \color{blue}{ Q_{\phi_B} } (s', arg max_{a'} \color{red}{ Q_{\phi_A} } (s',a') ) 
$$

$$Q_{\phi_A}$$를 통해 positive noise가 낀 액션을 골랐어도 $$Q_{\phi_B}$$가 낮은 값을 리턴하기 때문에 보완이 된다는 겁니다.
즉 `self-correcting`이 된다고 합니다.


![slide23](/assets/images/CS285/lec-8/slide23.png)
*Slide. 23.*

실제로 `Double Q-Learning`을 구현할 때는 다른 $$Q_A,Q_B$$를 또 추가하는 것이 아니라 원래도 Current Network, Target Network를 두 개 가지고 있기 때문에 이 네트워크들을 사용하면 된다고 합니다.

$$
\begin{aligned}
&
y = r + \gamma Q_{\phi'} (s', arg max_{a'} Q_{\phi'}(s',a'))
& \scriptstyle{ \text{ standard Q-Learning } } \\

&
y = r + \gamma Q_{\phi'} (s', arg max_{a'} \color{red}{ Q_{\phi} }(s',a'))
& \scriptstyle{ \text{ double Q-Learning } } \\

\end{aligned}
$$

즉 moving target 문제를 피하기 위해서 Target Network로 여전히 Q값을 평가하긴 하지만 액션을 고르기 위해서는 Current Network를 쓰게 된 겁니다.
$$\phi$$와 $$\phi'$$이 비슷해지지 않는이상 decorrelated 되게 된다고 합니다.

물론 이렇게 함으로써 moving target 문제가 좀 생긴다고 하며, $$Q'$$는 가장 최근의 $$Q$$로 고정된 네트워크이기 때문에 둘은 완전 decorrelated 하지 않고,
이는 완벽한 해법은 아니라고 하지만 `practical`하게는 이렇게 쓴다고 합니다.






### <mark style='background-color: #dcffe4'> Dueling-DQN </mark>

* 추가적으로 강의에서 언급하지는 않았지만 Double DQN이 제안된 2015~2016년에 이름이 비슷한 `Dueling-DQN (DDQN)`이라는 방법론도 제안됐는데 이는 $$Q(s,a) = V(s) + A(s,a)$$ 라는 특성을 이용해서 직접적으로 Q를 예측하지 않고 V와 A를 예측하는 네트워크를 2개 둬서 간접적으로 Q를 예측하는 방법론 입니다. 이렇게 함으로써 기존의 DQN을 의 더 효율적이고 안정적이게 학습할 수 있다고 합니다.

![dueling_dqn1](/assets/images/CS285/lec-8/dueling_dqn1.png){: width="80%"}
*Fig. DQN (위) vs DDQN (아래)*

![dqn_algorithm](/assets/images/CS285/lec-8/dqn_algorithm.png)
*Fig. DQN Algorithm from [2013, Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)*

![dueling_dqn2](/assets/images/CS285/lec-8/dueling_dqn2.png)
*Fig. Dueling-DQN(DDQN) from [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/pdf/1511.06581)*






### <mark style='background-color: #dcffe4'> Q-Learning with Multi-Step Returns</mark>

그 다음으로 알아볼 것은 `Multi-Step Return Q-Learning` 입니다. 이는 Actor-Critic에서 살펴봤던 방법과 비슷합니다.

![slide24](/assets/images/CS285/lec-8/slide24.png)
*Slide. 24.*

Q-Learning target이 다음과 같죠.

$$
y_{j,t} = r_{j,t} + \gamma max_{a_{j,t+1}} Q_{\phi'}(s_{j,t+1},a_{j,t+1})
$$

Q-Learning을 학습할 때 중요한 요소는 $$Q_{\phi'}$$이 구린 학습 초기에는 reward, $$r$$ 이고 $$Q_{\phi'}$$는 노이즈처럼 인식되다가 나중에 Q함수가 좋게 학습되고, 이 값이 커지면 $$Q_{\phi'}$$로 바뀌는 구조입니다.

Actor-Critic에 대해서 다시 한 번 생각해보면, Actor-Critic은 reward와 다음 state의 value값을 더해 action과 weighted sum하는 구조로 variance를 줄였으나 V값이 좋지못하면 학습에 어려움을 겪는 baised 모델이었고, Monte Carlo 방법으로 reward를 sum해서 사용하는 그냥 Policy Gradient방법은 unbaised 하지만 variance가 큰 구조였습니다. 

***

- Actor-Critic의 다양한 Advantage Function Recap

$$
\begin{aligned}
&
\hat{A}_{\color{red}{C}}^{\pi} (s_t,a_t) = r(s_t,a_t) + \gamma \hat{V}_{\phi}^{\pi} (s_{t+1}) - \hat{V}_{\phi}^{\pi} (s_{t}) 
& \\

&
\hat{A}_{\color{red}{MC}}^{\pi} (s_t,a_t) = \sum_{t=t'}^{\infty} \gamma^{t'-t} r(s_{t'},a_{t'}) - \hat{V}_{\phi}^{\pi} (s_{t}) 
& \\

&
\hat{A}_{\color{red}{n}}^{\pi} (s_t,a_t) = \sum_{t=t'}^{t+n} \gamma^{t'-t} r(s_{t'},a_{t'}) - \hat{V}_{\phi}^{\pi} (s_{t}) + \color{blue}{ \gamma^n \hat{V}_{\phi}^{\pi} (s_{t+n}) }
& \\

\end{aligned}
$$

***

Q-Learning은 기본적으로 한 스텝만으로 target을 정하는 이른 바 `one step back up` 방법으로 maximum bias 이며 minimum variance 한 특성을 지니고 있는데요, 이를 Actor-Critic에서 처럼 

$$
\begin{aligned}
&
y_{j,t} = r_{j,t} + \gamma max_{a_{j,t+1}} Q_{\phi'}(s_{j,t+1},a_{j,t+1})
& \scriptstyle{ \text{ og Q-Learning target } } \\

&
y_{j,t} = \sum_{t'=t}^{t+N-1} \gamma^{t-t'} r_{j,t'} + \gamma^N max_{a_{j,t+N}} Q_{\phi'}(s_{j,t+N},a_{j,t+N})
& \scriptstyle{ \text{ Multi-Step target like Actor-Critic  } } \\

\end{aligned}
$$

(당연히 여기서 N이 1이면 standard form이 됩니다.)

이는 여러번 reward값을 더하고 N step뒤의 state, action에 기반한 Q값을 사용하기 때문에 `N-step Estimator`라고도 부르며 one-step에서 n-step이 됐기 때문에 variance는 조금 높아졌으며 bias는 낮아지는 효과가 생깁니다 (bias variance trade-off).


![slide25](/assets/images/CS285/lec-8/slide25.png)
*Slide. 25.*

하지만 N-Step Return을 사용할 경우 몇 가지 문제점이 있을 수 있는데요,
그 중 하나는 N번이나 transition을 진행하기 때문에 off-policy sample을 사용할 경우 더이상 return값이 정확(?)하지 않다는 겁니다. 

이를 해결하는 방법은 세 가지 정도가 있을 수 있는데요,

- Ignore the problem
- Cut the trace - dynamically choose N to get only on-policy data
- Importance sampling

더 자세한 내용은 [Safe and Efficient Off-Policy Reinforcement Learning](https://arxiv.org/pdf/1606.02647) 논문을 찾아보시면 좋을 것 같습니다.









## <mark style='background-color: #fff5b1'> Q-Learning with Continuous Actions </mark>

우리는 그동안 discrete action space인 경우의 Q-Learning에 대해서만 생각해 봤는데요, 과연 continuous action space인 경우에 대해서는 어떻게 해야 할까요?

![discrete_vs_continuous1](/assets/images/CS285/lec-8/discrete_vs_continuous1.png){: width="90%"}
*Fig. Discrete Action Space vs Continuous Action Space*

![discrete_vs_continuous2](/assets/images/CS285/lec-8/discrete_vs_continuous2.png)
*Fig. 일반적으로 Discete case는 마지막에 Softmax Layer를 통과시켜 Categorical Distribution을 출력하고, Continuous case는 Multivariate Gaussian Distribution을 추론하는 것으로 생각해볼 수 있겠습니다.*




### <mark style='background-color: #dcffe4'> Option 1 : Optimization </mark>

Discrete action space를 사용하는 경우 우리는 가능한 action들의 경우에 대해서 Q값이 가장 높은 action을 선택하는 argmax 방법을 사용했습니다.

![slide27](/assets/images/CS285/lec-8/slide27.png)
*Slide. 27.*

하지만 선택 가능한 action의 수가 무수히 많으면 (continuous) 이런 방법을 쓰는건 쉽지 않겠죠.
이를 해결하기 위한 방법이 몇가지 있는데요, 그 중 첫번째는 `Stochastic Optimization`을 이용하는 겁니다.

![slide28](/assets/images/CS285/lec-8/slide28.png)
*Slide. 28.*

이는 continuous action 들에 대한 max 연산을 approximate 하는 방법인데요,
바로 ramdon하게 샘플링 해서 얻은 action들, 즉 discrete하게 된 action들을 max 해서 사용하는 겁니다.

일반적으로는 별로 정확하지는 않을 수 있지만 action space가 작다거나 하면 꽤 나쁘지 않은 선택이라고 합니다.

(`Cross-Entropy Meothd (CEM)`이나 `CMA-ES` 등의 improved method가 존재하는데 궁금하신 분들은 관련 자료를 찾아보시기 바랍니다.)





### <mark style='background-color: #dcffe4'> Option 2 : Normalized Adantage Functions (NAF) </mark>

두 번째 옵션은 [Continuous Deep Q-Learning with Model-based Acceleration](https://arxiv.org/pdf/1603.00748) 라는 논문의 방법론 입니다.

![slide29](/assets/images/CS285/lec-8/slide29.png)
*Slide. 29.*

일반적으로 뉴럴 네트워크로 근사한 함수는 최적화 하기가 쉽지 않은데요, 이보다 단순한 형태인 `2차식 형태 (quadratic form)`로 Q-Function을 나타내는 겁니다. 이렇게하면 2차식이 일반적으로 concave한 형태가 되기 때문에 optimal solution을 closed-form 형태로 구할 수 있으므로 이런 방법으로 max값을 찾겠다는 겁니다. 

`Normalized Advantage Function (NAF)`에서는 어떤 뉴럴 네트워크가 state, $$s$$를 인풋으로 받아 $$\mu,P,V$$로 표현되는 벡터나 매트릭스의 세 가지 qunatity를 출력값으로 뱉습니다. 
그러니까 이 네트워크는 어떤 state의 함수라도 표현이 가능한 state에 대해서는 완전히 비선형적 (non-linear) 인 함수지만 출력 값으로 뱉은 qunatity들을 가지고 action에 대한 Q함수는 언제나 2차식 형태로 표현할 수 있고 언제나 closed-form으로 해당 state에서의 max인 Q를 리턴하는 action을 구해낼 수 있습니다.

```
어떻게 벡터와 매트릭스만으로 2차식을 만들어내는지는 어떤 함수를 테일러 전개했을 때 2차 항 까지만 표현하는 방법을 생각하시면 될 것 같습니다. 
```

이럴 경우 $$arg max_a Q_{\phi}(s,a) = \mu_{\phi}(s)$$로 표현할 수 있습니다.

당연히 원래 Q함수는 뉴럴네트워크로 Q함수를 직접 리턴하는 함수로 디자인 됐지만, 
지금은 2차식 형태로 표현하기로 했기 때문에 representational power를 잃게되는 단점이 있지만 효율적으로 연속적인 행동 공간에 놓여진 문제를 Q-Learning으로 풀 수 있습니다.

![naf_algorithm](/assets/images/CS285/lec-8/naf_algorithm.png){: width="80%"}
*Fig. NAF algorithm*

(참고하시면 좋을 것 같아 `NAF` 논문의 알고리즘을 첨부했습니다.)





### <mark style='background-color: #dcffe4'> Option 3 : Deep Deterministic Poliy Gradient (DDPG) </mark>

마지막 옵션은 `approximate maximizer`를 학습하는 방법으로 [Continuous control with deep reinforcement learning](https://arxiv.org/pdf/1509.02971) 라는 논문에서 소개된 방법입니다. 

이는 첫 번째 옵션과 비슷하지만 조금 다르게 아예 네트워크 하나를 더 두고 이를 통해서 maximization을 한다고 합니다.

위의 논문은 `Deep Deterministic Policy Gradient (DDPG)`라고도 불리는데요, 이름에서도 알 수 있듯 이는 "Deterministic" Actor-Critic 알고리즘이라고 생각할 수도 있지만, continuous action space를 다루는 Q-Learning 이라고 접근하는게 더 개념적으로 심필하다고 합니다.

![slide30](/assets/images/CS285/lec-8/slide30.png)
*Slide. 30.*

다시 한 번 $$max_a Q_{\phi}(s,a)$$는 아래와 같이 표현할 수 있죠.

$$
max_a Q_{\phi}(s,a) = Q_{\phi} (s, arg max_a Q_{\phi}(s,a))
$$

DDPG의 핵심 아이디어는 다음의 역할을 하는 $$\theta$$로 파라메터화 된 네트워크를 학습하는 겁니다.

$$\mu_{\theta} (s) \approx arg max_a Q_{\phi}(s,a)$$

option 1에서 수많은 action들을 전부 헤아릴 수 없으니 몇개 랜덤 샘플해서 최적화를 했던 것과 유사하나, 그것을 근사하는 직접적인 네트워크가 추가된 것이기 때문에 option 1과 비슷하다는 것이고, 여기서 $$\mu_{\theta}$$가 policy 나 다름 없는데 이게 argmax operator의 역할을 하기 때문에 이런 방법론이 `deterministic policy` gradient algorithm이라고도 불리는 겁니다.

어떻게 $$\theta$$ 네트워크를 학습할까요? 바로 아래의 최적화 문제를 오차 역전파로 풀면 됩니다.

$$
\theta \leftarrow arg max_{\theta} Q_{\phi} (s, \mu_{\theta}(s))
$$

이렇게 해서 우리의 continuous action space에서의 target은 아래와 같아집니다.

$$
y_j = r_j + \gamma Q_{\theta'}(s'_j, \mu_{\theta}(s'_j)) \approx r_j + \gamma Q_{\theta'} (s'_j, arg max_{a'} Q_{\theta'} (s'_j,a'_j)) 
$$



실제 알고리즘은 아래와 같은데,

![slide31](/assets/images/CS285/lec-8/slide31.png)
*Slide. 31.*

주의할 점은 네트워크가 2개이기 때문에 $$\theta,\phi$$를 둘 다 업데이트 해줘야 한다는 것이고,
같은 내용이지만 원본을 참고하시면 더 좋을 것 같아 `DDPG` 논문의 알고리즘을 첨부했습니다.

![ddpg_algorithm](/assets/images/CS285/lec-8/ddpg_algorithm.png)
*Fig. DDPG algorithm*

+ 추가적으로 강의에서 언급하진 않았지만 NFQCA나 TD3, SAC등의 기법들도 있다고 하니 관심있으시면 더 찾아보시면 좋을 것 같습니다.

- TD3 (Twin Delayed DDPG) : [Addressing Function Approximation Error in Actor-Critic Methods](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf)
- SAC (Soft Actor-Critic) : [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf)
- ㅁ




***

### <mark style='background-color: #dcffe4'> Flowcharts of NAF and DDPG </mark>

NAF와 DDPG를 잘 나타낸 Flow Chart를 발견해서 알고리즘을 이해하는데 도움이 될 것 같아 첨부합니다.

![naf_flowchart](/assets/images/CS285/lec-8/naf_flowchart.png){: width="80%"}
*Fig. NAF Flowchart*

![ddpg_flowchart](/assets/images/CS285/lec-8/ddpg_flowchart.png)
*Fig. DDPG Flowchart*

(출처 : [Deep reinforcement learning-continuous action control DDPG, NAF](https://www.programmersought.com/article/58923675061/))

***






## <mark style='background-color: #fff5b1'> Implementation Tips and Examples </mark>

이번 강의의 마지막 부분에서 다룰 내용은 실제 Q-Learning 알고리즘을 구현할 때 practical한 tip과 몇가지 Q-Learning을 사용한 논문 예시들 입니다.

### <mark style='background-color: #dcffe4'> Implementation Tips </mark>

![slide33](/assets/images/CS285/lec-8/slide33.png)
*Slide. 33.*

Q-Learning은 실제로 사용하기에 Policty Gradient보다 더 까다롭다고 (finicky) 하는데요, 학습하는 데 있어서 stability가 항상 문제가 된다고 합니다. 

Sergey 교수님은 우선 DQN 같은 알고리즘을 구현했으면 쉬운 task부터 학습, 테스트를 하면서 bug가 있지는 않은지? 잘 구현했는지?를 체크해보고 그 다음에 하이퍼 파라메터를 튜닝하고 실제 문제에 적용해 보는 것을 추천한다고 합니다.

![dqn_implementation1](/assets/images/CS285/lec-8/dqn_implementation1.png)
*Figure from [Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952)*



![slide34](/assets/images/CS285/lec-8/slide34.png)
*Slide. 34.*








### <mark style='background-color: #dcffe4'> Some Papers and Examples </mark>

![slide35](/assets/images/CS285/lec-8/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-8/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-8/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-8/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-8/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-8/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-8/slide41.png)
*Slide. 41.*











## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [Medium : (Deep) Q-learning, Part1: basic introduction and implementation](https://medium.com/@qempsil0914/zero-to-one-deep-q-learning-part1-basic-introduction-and-implementation-bb7602b55a2c)

- [Medium : Deep Q-Learning, Part2: Double Deep Q Network, (Double DQN)](https://medium.com/@qempsil0914/deep-q-learning-part2-double-deep-q-network-double-dqn-b8fc9212bbb2)

- [Deep reinforcement learning-continuous action control DDPG, NAF](https://www.programmersought.com/article/58923675061/)




