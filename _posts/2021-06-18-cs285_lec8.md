---
title: (미완) Lecture 8 - Deep RL with Q-Functions

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)

Lecture 8의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=7-D8RL3D6CI&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=32)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 다룰 내용은 앞서 배운 Value-based 방법론인 `Q-Learning`을 사용한 practical Deep RL 알고리즘 (Deep Q-Learning) 입니다.

![slide1](/assets/images/CS285/lec-8/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap: Q-learning </mark>

Q-iteration에 대한 Recap으로 강의를 시작합니다.

Value-based 강화학습 알고리즘들은 policy를 따로 명시적으로 (explicitly) 나타내지 않아도 됐죠.
하지만 Transition Probability를 알아야 하는 대부분의 Model-based Value-based 알고리즘들이 현실 세계에 잘 적용되지 않는다는 문제를 개선하기 위해서
강의의 마지막엔 $$V(s)$$ 를 $$Q(s,a)$$로 대체한 Model-free Value-based 알고리즘, 그중에서도 `Fitted Q-Iteration`, `Q-Learning` 에 대해서 알아봤습니다.
Q-Learning 은 optimal policy를 얻는 것이 보장되지 않은 알고리즘이었으나 `이를 개선하면 충분히 practical한 알고리즘을 만들어낼 수 있음`을 시사했는데요, 오늘 알아볼 내용이 바로 이것입니다.

![slide2](/assets/images/CS285/lec-8/slide2.png)
*Slide. 2.*

슬라이드의 왼쪽에는 Batch-mode Q-Iteration과 Online-mode Q-Iteration이 나와있고, 오른쪽에는 이 강의 내내 사용한 Deep RL의 Anatomy가 Q-Learning에 대해 fitting 되어있습니다.

간단히 리뷰해보면 `General Fitted Q-Iteration`은

```
for 학습 데이터를 샘플링 함 (s,a,s',r). (max operator 덕분에 `Off-Policy`로 데이터 샘플 가능하며, 얼마나 많이 transition할지는 정하기 나름)
  for i in range(K) : << (K도 정하기 나름)
    (s,a,s',r) tuple을 이용해서 y_i, 즉 타겟을 구함. 
    파라메터를 한 스텝 업데이트.
```

였는데, 위에서 말한 각 step을 몇 번 반복할지를 의미하는 하이퍼파라메터들을 모두 1로 설정하면 `Online Q-Iteration`이 되는데 이를 `Q-Learning`이라고 합니다.
(딱 한번만 $$s_i$$에서 $$a_i$$를 취하고, 이로부터 발생하는 $$s'_i,r_i$$를 얻어서 2,3번 스텝으로 파라메터를 딱 한스텝만 업데이트 합니다.)








### <mark style='background-color: #dcffe4'> Problems of Q-Learning  </mark>

![slide3](/assets/images/CS285/lec-8/slide3.png)
*Slide. 3.*

일반적인 Q-Learning의 문제점은 뭐였을까요?

*Slide. 3.*에서 보이는 step 3가 `gradient descent`처럼 보이지만 사실은 그렇지 않다는게 문제였죠.
다시 써보면 원래의 Q-Learning 업데이트 수식이 아래와 같았지만,

$$
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) - y_i )
$$

$$y_i$$에 step 2수식을 대입하면 아래와 같이 전개할 수 있고,

$$
\begin{aligned}
&
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) - \color{red}{  y_i } )
& \\

&
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) -  \color{red}{ [r(s_i,a_i) + \gamma max_a Q_{\phi} (s'_i,a'_i)] } )
& \\
\end{aligned}
$$

Target Value인 `max operator` 부분에 gradient가 흐르지 않습니다. (max operator 덕분에 off-policy 알고리즘으로 발전할 수 있었긴 하지만요)
그래서 `chain rule`을 적용하기 힘들기 때문에, `residual gradient algorithm`이라고 불리는 방법을 사용하긴 하지만 별로 잘 작동하지는 않았습니다.

Online Q-Learning에는 또 하나의 문제점이 있었는데요, 바로 한 번에 하나의 transition만 샘플하면 sequential transition들이 highly correlated 되어 있다는 것이었는데요, 즉 time-step $$t$$에서 본 state, $$s_t$$와 $$s_{t+1}$$이 굉장히 유사할거라는 겁니다. 
그 말인 즉, step 3에서 gradient step을 한스텝 진행할 때도 문제를 야기한다고 하는데요, 곧 솔루션과 함께 제대로 다룬다고 합니다.




이제 `Correlation Problem`에 대해서 제대로 얘기를 해보도록 하겠습니다.

![slide4](/assets/images/CS285/lec-8/slide4.png)
*Slide. 4.*

*Slide. 4.*에는 알고리즘이 2 step이 되었는데요, 큰 변화는 없고 target을 evaluation하는 부분을 step 3에 넣었을 뿐 똑같습니다.
Correlation Problem이란 t번째 state, $$s_t$$와 t+1번째 $$s_{t+1}$$이 굉장히 유사하거나 유사하지 않더라도 관계가 깊다는 겁니다. 이게 왜 문제냐면 비슷한 입력과 결과를 내는 데이터를 매우 많이 학습하는것은 비효율적이고 네트워크가 오버피팅되는 등의 악영향을 줄 수 있기 때문입니다.

(+ 또하나의 문제로는 후에 다룰거지만 Optimization 하는 과정이 MSE Loss를 사용한 Supervised Regression 문제 같이 보이지만 Target Value가 계속 변화하는 문제가 있습니다.)

아무튼 어떤 일련의 Trajectory에 대해서 생각해보도록 하겠습니다.

![correlation1](/assets/images/CS285/lec-8/correlation1.png){: widht="80%"}
*Fig. Trajectory 1-1*

위의 Trajectory에 대해서 $$s_t,a_t,r_t,s_{t+1}$$ 을 여러 세트로 해서 최적화를 한다고 하면 $$s_1,s_2,s_3$$ 간에 서로 비슷한 transition을 하기 떄문에 네트워크는 굉장히 이 transition에 대해서 locally over-fitting 하게 될겁니다.

![correlation2](/assets/images/CS285/lec-8/correlation2.png){: widht="80%"}
*Fig. Trajectory 1-2*

마찬가지로 over-fitting 하겠죠?

![correlation3](/assets/images/CS285/lec-8/correlation3.png){: widht="80%"}
*Fig. Trajectory 1-3*

그리고 새로운 Trajectory에 대해서 네트워크는 굉장히 좋지 못한 Q값을 예측하게 될겁니다.

이는 Actor-Critic의 Online version에서도 겪었던 비슷한 문제이며, 우리는 이 문제를 해결하기 위해서 동기식, 비동기식 `Data Parallel`을 진행했습니다.
여러 Worker들을 사용해서 데이터를 뽑아서 다양성을 높히는 거였죠.
이러한 전략은 Q-Learning 자체가 직전의 policy로만 샘플할 데이터를 써도되지 않는, 이른 바 Off-Policy 알고리즘이기 때문에 더 잘 맞는다고 합니다.

![slide5](/assets/images/CS285/lec-8/slide5.png)
*Slide. 5.*

이런 correlated sample 문제를 해결하는 더 간단한 방법이 있는데요, 바로 `Replay Buffer`를 이용하는 겁니다.
Replay Buffer를 사용하는 아이디어는 RL에서 굉장히 오래됐으며 1990년대에 제안되었다고 합니다.
아이디어는 간단한데요, locally 비슷한 state sample들을 쓰지 않고 여러군데서 랜덤하게 뽑은 state들로 네트워크를 학습하자는 겁니다.

*Slide. 5.*에서 보시면 Full Fitted Q-Iteartion 에서 1번 데이터를 모으는 과정이 생략되었고 이전에 모아둔 data들이 모인 Replay Buffer에서 무작위로 가져오는걸로 변경됐습니다.

```
전형적인 Data-Driven Approach인 Deep Learning처럼 dataset에서 random batch를 뽑아 쓰는 것처럼 됐습니다.
```

![slide6](/assets/images/CS285/lec-8/slide6.png)
*Slide. 6.*

위의 슬라이드에서 보시면 이제 데이터를 그냥 배치를 원하는 만큼 샘플합니다.
(이 때 데이터들은 딥러닝 알고리즘들이 사용하는 데이터가 `iid`라고 가정하는 것과 같아진다고 합니다.)  

그리고 배치를 1이 아닌 4,8 이런식으로 여러번 하고 gradient를 합쳐서 업데이트 하면 low variance gradient를 얻을 수도 있습니다.

```
하지만 여전히 max term 때문에 정확한 gradient를 계산할 수는 없지만 그래도 correlation 문제는 이런식으로 해결이 가능하다고 합니다.
```

하지만 리플레이 버퍼를 사용할 때의 문제점이 아직 존재합니다.
"리플레이 버퍼에 넣을 데이터를 어디서 샘플하지? 혹은 시간이 지날수록 어떻게 더 유의미한 데이터를 넣을까?" 인데요, 
초기 policy는 랜덤하거나 매우 좋지않은 policy이기 때문에 ($$s,a,r,s'$$)이 별로 좋지 않을 수 있기 때문에, 학습이 진행될 수록
업데이트된 그럴싸한 policy를 사용해서 추가적으로 뽑은데이터를 넣거나 해야 수렴이 더 잘된다는 겁니다.

우리가 말한 전략을 그림으로 나타내면 아래와 같습니다.

![replay_buffer_update](/assets/images/CS285/lec-8/replay_buffer_update.png)
*Fig. How to update replay buffer*

간단합니다.
우리는 ($$s,a,r,s'$$) 같은 `transition data`를 엄청 가지고 있는데,

```
(s1,a1,r1,s2)
(s2,a2,r2,s3)
...
(s_{n-1},a_{n-1},r_{n-1},s_n)
```

학습을 조금 진행한 뒤에 이 버퍼에 최근 policy로 구한 transition data를 더 추가하는겁니다.

```
(s1,a1,r1,s2)
(s2,a2,r2,s3)
...
(s_{n-1},a_{n-1},r_{n-1},s_n)
...
(s_{2n-1},a_{2n-1},r_{2n-1},s_{2n})
```


![slide7](/assets/images/CS285/lec-8/slide7.png)
*Slide. 7.*

*Slide. 7.*는 방금까지 얘기한 내용을 한 슬라이드로 정리한 것입니다.

알고리즘이 그렇게 어려워 보이지는 않죠.
Q 함수를 근사한 뉴럴 네트워크를 업데이트하는 Value-based Objective와 Replay-Buffer 를 간단하게 적용하는게 전부입니다.
하지만 여기에 추가적으로 거대한 state space를 근사하는 네트워크와 Target Network를 사용하는 등 학습의 안정성과 효율성을 높히면 굉장히 놀라운 결과를 보여주게 되는데요, 그것이 바로 아직까지도 많이 사용되는 Value-Based Deep RL 알고리즘들의 근간이 되는  `Deep-Q-Network (DQN)` 입니다.

![dqn_illustration](/assets/images/CS285/lec-8/dqn_illustration.png)
*Fig. The deep Q-network (DQN). from [A Brief Survey of Deep Reinforcement Learning](https://arxiv.org/pdf/1708.05866)*

![2015_dqn](/assets/images/CS285/lec-8/2015_dqn.png)
*Fig. Schematic illustration of the convolutional neural network in Deep-Q-Network (DQN). from [2015, Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)*

`DQN`은 2013년에 제안된 딥러닝과 강화학습 결합한 네트워크이며,
`Atari`라는 비디오 게임에서 인간 수준의 퍼포먼스를 보여주며 심층 강화학습의 가능성을 보여준 간단하지만 강력한 네트워크입니다.

![2013_dqn](/assets/images/CS285/lec-8/2013_dqn.png)
*Fig. Screenshots from five Atari 2600 Games: (Left-to-right) Pong, Breakout, SpaceInvaders,  Seaquest, Beam Rider from [2013, Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)*



## <mark style='background-color: #fff5b1'> Target Networks </mark>

지금까지 Q-Learning의 몇가지 문제점에 대해 생각해보고 이를 해결하면서 practical한 알고리즘을 만들려고 노력했지만
아직 트롤링을 하는 term이 있죠, 이것 때문에 Replay Buffer를 사용했음에도 여전히 학습이 불안정한데요,

![slide9](/assets/images/CS285/lec-8/slide9.png)
*Slide. 9.*

바로 Objective의 두 번째 term인 max term 입니다.

$$
\phi \leftarrow \phi - \alpha \frac{d Q_{\phi}}{d \phi} (s_i,a_i) ( Q_{\phi}(s_i,a_i) -  \color{red}{ [r(s_i,a_i) + \gamma max_a Q_{\phi} (s'_i,a'_i)] } )
$$

Q-Learning은 gradient descent 같이 보이지만 그건 아니고, 또한 `moving target`을 따라가야 하는 문제를 가지고 있습니다.
그러니까 예를 들어 MSE Loss를 쓰는 회귀문제에서 target이 움직이는거나 다름 없기 때문에 네트워크가 수렴하기 어렵습니다.

![slide10](/assets/images/CS285/lec-8/slide10.png)
*Slide. 10.*

그렇기 때문에 우리는 Q-Learning에 `어떤 Target Network`라고 하는 것을 사용해서 Full Batch Fitted Q-Iteration과 Online Q-Learning을 결합한(?) 방법을 써보려고 합니다.

![slide11](/assets/images/CS285/lec-8/slide11.png)
*Slide. 11.*

그러니까 문제는 Target이 움직이는건데, 왜 움직이는지를 생각해보면 원래 Q-Learning의 target은 $$Q_{ \color{red}{\phi}}(s',a')$$, 즉 $$\phi$$로 파라메터화된 네트워크의 output을 사용하는건데, update step에서 파라메터 $$\phi$$가 업데이트되기 때문에 그게 문제인겁니다. 이를 피하기 위해서 제안된 아이디어는 "$$\phi'$$를 써보면 어떨까?" 입니다. 그러니까 Q-Network, $$Q_{\phi}$$의 파라메터가 자꾸 변하니까 `Target Network`, $$Q_{\color{blue}{\phi}}$$를 따로 두자는 거죠. 

![target_network](/assets/images/CS285/lec-8/target_network.png)
*Fig.*

그 결과 알고리즘은 버퍼에서 데이터를 샘플해서 데이터에 대해서 Q-Network를 충분히 여러번 반복적으로 업데이트 하고 충분히 업데이트 했으면 Target Network를 다시 최근의 Q-Network로 업데이트해주고, 다시 또 Target은 고정을 하고... 를 반복하는 겁니다. (*Slide. 11.* 에서 1번과 4번의 $$\phi'$$가 핵심입니다.)

(여기서 잊지말아야 할 점은 앞서 말한것 처럼 리플레이 버퍼에 데이터를 추가해주는 과정도 있다는 겁니다.)

![target_network2](/assets/images/CS285/lec-8/target_network2.png)
*Fig.*

K는 1~4정도의 값으로 반복하고, N은 10000정도가 될 수 있다고 합니다.

![slide12](/assets/images/CS285/lec-8/slide12.png)
*Slide. 12.*

이제 여태까지 배운 것을 바탕으로 `Deep Q-Learning (Deep-Q-Network; DQN)`을 정의할 수 있는데요,
*Slide. 12.*에 잘 나와있습니다. 
슬라이드에는 두 가지 알고리즘이 있는데요 아래에 나와있는 "Classic" DQN이 위의 알고리즘의 k=1일 때의 special case입니다.


Target Network를 다루는 데에는 또 다른 방법이 있습니다.

![slide13](/assets/images/CS285/lec-8/slide13.png)
*Slide. 13.*

"Classic" DQN을 보면 5번의 update룰이 조금 위화감이 느껴질 수 있는데요, 
바로 우리가 Moving Target 문제를 해결하고자 $$\phi'$$로 고정된 네트워크로 타겟을 생성하고 이를 바탕으로 $$\phi$$를 업데이트 하고 있었는데 이게 어느순간 $$\phi$$로 확 변해버려서 결국엔 Moving Target처럼 느껴질 수 있다는 점입니다.

그러니까 K=1 (gradient step 반복 횟수), N=4 (배치 데이터 샘플 횟수, 실제론 매우 큼) 라고 생각할 때, N=4에서 5로 넘어가는 순간 갑자기 $$\phi$$가 되버리는게 문제가 될 수 있다는 겁니다 (실제로는 별로 큰문제는 아니랍니다).

그래서 $$\phi' \leftarrow \phi$$ 가 아니라 조금 더 자연스럽게 $$\phi' \leftarrow (\tau)\phi' + (1-\tau)\phi $$ 로 업데이트하는 방식을 쓰기도 한다고 하며, 이와 비슷한 방법론을 `Polyak Averaging`라고 한다고 합니다.

$$\phi' \leftarrow ( \colr{red}{ \tau } )\phi' + ( 1 - \colr{red}{ \tau } )\phi, \text{ where } \tau \text{ is large value, like } \colr{red}{ 0.999 } $$

여기서 "NN은 선형적이지 않은데 linearly interpolation을 해도 되나?" 하는 의문점이 들 수 있는데요, old policy가 current policy의 set이라서 (유사해서) 별로 문제가 되지 않는다고(?) 합니다. (non-linear function들에 대한 linear interpolation이 궁금하시다면 `Polyak Averaging`을 살펴보라고 하십니다.) 


***

### <mark style='background-color: #dcffe4'> DQN: Algorithm and Pseudo Code </mark>

DQN을 제안한 원본 논문에서는 아래와 같이 알고리즘을 설명하고 있습니다.
강의 Slide와 크게 다르지 않으나 원본이 궁금하신 분이 계실 것 같아서 첨부해봤습니다.

![dqn_annotated](/assets/images/CS285/lec-8/dqn_annotated.png)
*Fig. Original DQN Algorithm from [2013, Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)*

이에 따른 수도코드도 아래와 같이 간단하게 작성해볼 수 있습니다.

![dqn_pseudo](/assets/images/CS285/lec-8/dqn_pseudo.png)
*Fig. Pseudo Code from [CS234 Lecture 6 : CNNs and Deep Q Learning](https://web.stanford.edu/class/cs234/slides/lecture6.pdf)*

***






## <mark style='background-color: #fff5b1'> A General View of Q-Learning </mark>

![slide15](/assets/images/CS285/lec-8/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-8/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-8/slide17.png)
*Slide. 17.*






## <mark style='background-color: #fff5b1'> Improving Q-Learning </mark>

![slide19](/assets/images/CS285/lec-8/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-8/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-8/slide21.png)
*Slide. 21.*








### <mark style='background-color: #dcffe4'> Double-DQN (DDQN) </mark>

[Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461)

![slide22](/assets/images/CS285/lec-8/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-8/slide23.png)
*Slide. 23.*








### <mark style='background-color: #dcffe4'> Dueling-DQN </mark>

![dueling_dqn1](/assets/images/CS285/lec-8/dueling_dqn1.png){: width="80%"}
*Fig.*

![dqn_algorithm](/assets/images/CS285/lec-8/dqn_algorithm.png)
*Fig. DQN Algorithm from [2013, Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)*

![dueling_dqn2](/assets/images/CS285/lec-8/dueling_dqn2.png)
*Fig. Dueling-DQN(DDQN) from [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/pdf/1511.06581)*









### <mark style='background-color: #dcffe4'> Q-Learning with Multi-Step Returns</mark>

![slide24](/assets/images/CS285/lec-8/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-8/slide25.png)
*Slide. 25.*










## <mark style='background-color: #fff5b1'> Q-Learning with Continuous Actions </mark>

### <mark style='background-color: #dcffe4'> Option 1 : Optimization </mark>

![slide27](/assets/images/CS285/lec-8/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-8/slide28.png)
*Slide. 28.*





### <mark style='background-color: #dcffe4'> Option 2 : Normalized Adantage Functions (NAF) </mark>

[Continuous Deep Q-Learning with Model-based Acceleration](https://arxiv.org/pdf/1603.00748)

![slide29](/assets/images/CS285/lec-8/slide29.png)
*Slide. 29.*

![naf_algorithm](/assets/images/CS285/lec-8/naf_algorithm.png){: width="80%"}
*Fig. NAF algorithm*




### <mark style='background-color: #dcffe4'> Option 3 : Deep Deterministic Poliy Gradient (DDPG) </mark>

[Continuous control with deep reinforcement learning](https://arxiv.org/pdf/1509.02971)

![slide30](/assets/images/CS285/lec-8/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-8/slide31.png)
*Slide. 31.*

![ddpg_algorithm](/assets/images/CS285/lec-8/ddpg_algorithm.png)
*Fig. DDPG algorithm*










## <mark style='background-color: #fff5b1'> Implementation Tips and Examples </mark>

### <mark style='background-color: #dcffe4'> Implementation Tips </mark>

![slide33](/assets/images/CS285/lec-8/slide33.png)
*Slide. 33.*

![slide34](/assets/images/CS285/lec-8/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-8/slide35.png)
*Slide. 35.*










### <mark style='background-color: #dcffe4'> Examples </mark>

![slide36](/assets/images/CS285/lec-8/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-8/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-8/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-8/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-8/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-8/slide41.png)
*Slide. 41.*








## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)









