---
title: () Lecture 8 - Deep RL with Q-Functions

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)

Lecture 8의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=7-D8RL3D6CI&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=32)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 다룰 내용은 앞서 배운 Value-based 방법론인 `Q-Learning`을 사용한 practical Deep RL 알고리즘 (Deep Q-Learning) 입니다.

![slide1](/assets/images/CS285/lec-8/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap: Q-learning </mark>

Q-iteration에 대한 Recap으로 강의를 시작합니다.

Value-based 강화학습 알고리즘들은 policy를 따로 명시적으로 (explicitly) 나타내지 않아도 됐죠.
하지만 Transition Probability를 알아야 하는 대부분의 Model-based Value-based 알고리즘들이 현실 세계에 잘 적용되지 않는다는 문제를 개선하기 위해서
강의의 마지막엔 $$V(s)$$ 를 $$Q(s,a)$$로 대체한 Model-free Value-based 알고리즘, 그중에서도 `Q-Iteration`, `Q-Learning` 에 대해서 알아봤습니다.
Q-Learning 은 optimal policy를 얻는 것이 보장되지 않은 알고리즘이었으나 `이를 개선하면 충분히 practical한 알고리즘을 만들어낼 수 있음`을 시사했는데요, 오늘 알아볼 내용이 바로 이것입니다.

![slide2](/assets/images/CS285/lec-8/slide2.png)
*Slide. 2.*

슬라이드의 왼쪽에는 Batch-mode Q-Iteration과 Online-mode Q-Iteration이 나와있고, 오른쪽에는 이 강의 내내 사용한 Deep RL의 Anatomy가 Q-Learning에 대해 fitting 되어있는데 이를 요약하자면 

- 1.`Orange Box` : 학습 데이터를 policy를 직접 돌려 구하는데, 학습 데이터를 만드는 과정에서 우리가 학습하는 policy가 아니라 광범위한 다른 policy를 사용하는 경우를 `Off-Policy Algorithm`이라고 함.
- 2.`Green Box` : $$(s,a,s',r)$$ tuple을 이용해서 $$y_i$$, 즉 타겟을 구함. 
- 3.`Blue Box` : 파라메터 업데이트

였죠.

![slide3](/assets/images/CS285/lec-8/slide3.png)
*Slide. 3.*

![slide4](/assets/images/CS285/lec-8/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-8/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-8/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-8/slide7.png)
*Slide. 7.*






## <mark style='background-color: #fff5b1'> Target Networks </mark>

![slide9](/assets/images/CS285/lec-8/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-8/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-8/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-8/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-8/slide13.png)
*Slide. 13.*




## <mark style='background-color: #fff5b1'> A General View of Q-Learning </mark>

![slide15](/assets/images/CS285/lec-8/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-8/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-8/slide17.png)
*Slide. 17.*






## <mark style='background-color: #fff5b1'> Improving Q-Learning </mark>

![slide19](/assets/images/CS285/lec-8/slide19.png)
*Slide. 19.*

![slide20](/assets/images/CS285/lec-8/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-8/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-8/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-8/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-8/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-8/slide25.png)
*Slide. 25.*




## <mark style='background-color: #fff5b1'> Q-Learning with Continuous Actions </mark>

![slide27](/assets/images/CS285/lec-8/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-8/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-8/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-8/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-8/slide31.png)
*Slide. 31.*




## <mark style='background-color: #fff5b1'> Implementation Tips and Examples </mark>

![slide33](/assets/images/CS285/lec-8/slide33.png)
*Slide. 33.*

![slide34](/assets/images/CS285/lec-8/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-8/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-8/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-8/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-8/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-8/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-8/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-8/slide41.png)
*Slide. 41.*



### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)









