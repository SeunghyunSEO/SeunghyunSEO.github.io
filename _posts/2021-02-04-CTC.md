---
title: 2006, ICML, Connectionist Temporal Classification - Labelling Unsegmented Sequence Data With Recurrent Neural Networks
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Connectionist Temporal Classification, CTC </mark>

CTC는 무엇일까요? 간단하게 말해서 음성인식같은 task에서 입력이 되는 음성의 시퀀스길이와 이 되는(디코딩 되는) 받아쓰기(dictation)한 텍스트의 길이가 맞지 않아 발생하는 
Miss-Alignment 문제를 해결하기 위해 2006년 [Alex Graves](https://www.cs.toronto.edu/~graves/)라는 딥마인드의 세계적인 석학에 의해 2006년에 제안된 기법입니다.

![image](https://user-images.githubusercontent.com/48202736/106895390-7fcbfc80-6733-11eb-803c-da984525e0b2.png)
*Fig. 1. Alex Graves의 홈페이지에 있는 그의 사진*

그는 Toronto 대학 출신의, 세계적인 석학 [Geoffrey E. Hinton](http://www.cs.toronto.edu/~hinton/)의 제자이며, 


CTC 이외에도 ([A novel connectionist system for unconstrained handwriting recognition](PDF), OCR논문), ([Practical Variational Inference for Neural Networks](https://www.cs.toronto.edu/~graves/nips_2011.pdf), 베이지안 방법론에 쓰이는 VI), ([Sequence transduction with recurrent neural networks](https://arxiv.org/pdf/1211.3711), E2E음성인식의 모델의 큰 축 중 하나인 RNN Transducer), ([Generating sequences with recurrent neural networks](https://arxiv.org/pdf/1308.0850)), ([Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf)) 등등의 굵직한 음성인식(ASR), 활자인식(OCR) 논문에 참여하거나 


음성 합성 ([Wavenet: A generative model for raw audio](https://arxiv.org/pdf/1609.03499)), ([Parallel wavenet: Fast high-fidelity speech synthesis](https://arxiv.org/pdf/1711.10433)) 논문들, 


그리고 굵직한 강화학습 논문들 ([Playing atari with deep reinforcement learning](https://arxiv.org/pdf/1312.5602)), ([Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)) 에도 참여하는 등 굵직한 업적을 남기고 있습니다...(정말 대단합니다)


아무튼 CTC는 그가 쓴 논문들에서도 알 수 있다 싶이, 음성인식, 활자인식 등에서 출력값을 어떻게 align 해서 뽑을거냐를 해결한 방법론이고, 

음성인식에서는 아직까지도 크게 CTC, 즉 CTC loss를 통해서 시퀀스 모델링을 했느냐, 아니면 Attentio Mechanism을 사용한 Seq2Seq(Encoder-Decoder 계열) 했느냐로 
나눠서 생각할 정도로 영향력이 어마어마한 논문입니다. (+ RNN Transducer) 


자 이제 그럼 앞으로, 언급햇던 Miss Alignment 문제가 무엇인지, 그리고 CTC가 무엇인지에 대해서 깊이 파고들어가 보도록 하겠습니다.

- <mark style='background-color: #fff5b1'> Miss Alignment Problem </mark>

Alignment의 의미는 뭘까요? 사전을 찾아보면 '가지런함'이라고 나와있습니다.

네 맞습니다 음성인식을 할 때 예를들어 제가 "This is spectrogram"이라고 음성을 녹음했다고 해보겠습니다.

이 음성이 2초라고 해도, 예를들어 음성을 녹음하는 sampling rate를 16000으로 하면 32000길이의 샘플이 생기고, 이를 2D feature map으로 변환하기 위해서
적당한 window size로 Short Time Fourier Transform(STFT)을 통해 아래와 같이 스펙트로그램으로 만들었다고 생각해보겠습니다. 

<img width="742" alt="align1" src="https://user-images.githubusercontent.com/48202736/106981507-75e2e180-67a5-11eb-9a2b-e7cf47e94019.png">
*Fig. 1. 일반적인 음성인식에서의 alignment가 맞지 않는 상황*

하지만 아무리 STFT로 시간축의 길이(Temporal Resolution) 줄여도 200,300이 넘어가는 벡터들이 존재하고, 
우리가 찾고자 하는 정답 sequence는 "T h i s   i s   a   s p e c t r o g r a m"으로 10개가 좀 넘네요

과연 300개에 해당하는 벡터중에 어디서부터 어디까지가 "T"이고 어디서부터 어디까지가 "h"일까요?
아니면 단어 단위로 생각해서 300개에 해당하는 벡터중에 어디서부터 어디까지가 "This"가 될까요?

이를 알기란 쉽지 않습니다.


차라리 300개 길이에 해당하는 입력값에 해당하는 정답이 300개 주어지면 모를까요. "TTTTTThhhhhhhiiiiiiiiissssss ...." 
이런경우에 그냥 정답이랑 비교하는 분류문제를 풀면 되겠죠? 각 벡터 하나당 정답(알파벳 A~Z 소문자+대문자+공백 = 26개+26개+1개)을 softmax확률 분포로 나타내서 정답 원핫 토큰과 비교하면되니까요.

하지만 일반적으로 그렇지 않기 때문에 (unsegmented) 우리는 이 입력과 출력 사이를 align해줄 aligner가 필요한데, 여기서 이 역할을 해주는게 CTC라고 할 수 있습니다.


이처럼 입력 시퀀스 길이가 N일때 출력 시퀀스 길이 M이라고 하면 이 둘의 길이가 맞지 않는 문제를 Miss Alignment 문제라고 할 수 있고, 대부분의 Sequence Classification task 에서 겪는 문제입니다.

<center>$$ N \neq M $$</center>


- <mark style='background-color: #fff5b1'> Sequence Generation Task </mark>

CTC가 적용될 수 있는 문제들은 다양한데요, Sequence를 만들어내는 태스크라면 전부 사용이 가능합니다.

(다만 입력길이가 출력 길이보다 길어야 한다던가 하는 제약사항이? 있습니다.)

- <mark style='background-color: #dcffe4'> Automatic Speech Recognition (ASR) </mark> 

<img width="822" alt="distill_asr" src="https://user-images.githubusercontent.com/48202736/106986492-cca0e900-67ae-11eb-977c-f30623b2cbab.png">
*Fig. 2. 음성 인식(Automatic Speech Recognition, ASR)*

![DeepSpeech2](https://user-images.githubusercontent.com/48202736/106983159-c740a000-67a8-11eb-9221-70fddc14c0c1.png)
 {: style="width: 60%;" class="center"}
*Fig. 3. CTC 계열 음성인식에서 유명한 모델 중 하나인 DeepSpeec2*

음성인식에서 주로 사용된다는 말은 입아프니 더 안하도록 하겠습니다.

- <mark style='background-color: #dcffe4'> Optical Character Recognition(OCR) </mark>

<img width="732" alt="distill_ocr" src="https://user-images.githubusercontent.com/48202736/106986497-cf9bd980-67ae-11eb-8a3a-5275775221e9.png">
*Fig. 4. 활자 인식(Optical Character Recognition, OCR)*

![OCR](https://user-images.githubusercontent.com/48202736/106983312-04a52d80-67a9-11eb-994e-e3a102d86697.png)
*Fig. 5. 활자인식을 위한 CRNN Model, 마지막에 CTC가 사용된다.*

마찬가지로 활자인식도 alignment가 맞지 않기 때문에 CTC Loss를 통해서 최적의 정답을 구해낼 수 있습니다.

(이 외에도 기계번역(Neural Machine Translation, NMT)이나 질문응답(QA,VQA 등) 에도 쓰이는지 까지는 잘 본적이 없어서 모르겠습니다 ㅎ)

- <mark style='background-color: #fff5b1'> CTC </mark>

이제 CTC에 대해 본격적으로 설명해보려고 합니다.

사실 CTC는 굉장히 어려운? 알고리즘입니다. (제가 처음 공부할때도 몇번을 다시 봤는지 모르겠습니다...)


사실 CTC는 코드상에서는 아래처럼 굉장히 간단하게 사용할 수 있습니다.

```
>>> # Target are to be padded
>>> T = 50      # Input sequence length
>>> C = 20      # Number of classes (including blank)
>>> N = 16      # Batch size
>>> S = 30      # Target sequence length of longest target in batch (padding length)
>>> S_min = 10  # Minimum target length, for demonstration purposes
>>>
>>> # Initialize random batch of input vectors, for *size = (T,N,C)
>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
>>>
>>> # Initialize random batch of targets (0 = blank, 1:C = classes)
>>> target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)
>>>
>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
>>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
>>> ctc_loss = nn.CTCLoss()
>>> loss = ctc_loss(input, target, input_lengths, target_lengths)
>>> loss.backward()
```
*Pytorch 공식 문서에 있는 CTC loss Example*
(파이토치 공식 코드는 근데 사실 별로 안좋다고 알려져?(들은것) 같습니다. 지금은 수정됐는지 모르겠지만 일반적으로 wrap-ctc라던가 하는 다른 잘 알려진 오픈소스 코드를 사용하곤 합니다.)

Cross Entropy 정의하는 것 처럼 정의해주고 입력값만 넣어주면 알아서 됩니다...


사실 Cross Entropy가 어떤 의미를 가지는지 모르고 사용하는 게 일반적이기 때문에 'CTC를 이렇게 복잡한데 꼭 알아야 하냐?' 하는 분들이 계시다면 중간에 탈출을 하셔도 됩니다...


하지만 이를 끝까지 이해하는것은 굉장히 도움이 될 것이라고 확신하기 때문에 최대한 쉽고 직관적이게 예시를 들어가며 설명을 하려고 노력해보도록 하겠습니다.

- <mark style='background-color: #dcffe4'> CTC Loss 까지 build up</mark>

우리에게 길이가 t짜리인 입력 시퀀스(음성) x가 주어졌다고 합시다. (벡터가 t개 있는겁니다.)

이제 우리는 각 벡터들에 대해서 분류 문제를 풀겁니다. (이 벡터는 'a'다 이 벡터는 'p'다 ... 알파벳 개수가 곧 분포가 됩니다.)

그렇다면 t번째 벡터 $$y_t$$ 의 분포 중 k번째(a,b,c.... A,B,...공백 까지 하면 53개겠죠?) 알파벳의 확률값은 아래와 같이 나타낼 수 있습니다.

<center>$$ Pr(k,t|x) = \frac{exp(y_t^k)}{\sum_{k'}exp(y_t^k)} $$</center>

굉장히 평범한, 다중분류할 때 사용하는, 출력의 합이 1이 되게 해주는 소프트맥스 함수입니다.


이를 그림으로 나타내면 아래와 같습니다.

<img width="926" alt="ctc1" src="https://user-images.githubusercontent.com/48202736/106981529-8004e000-67a5-11eb-8314-97c9170829a5.png">
*Fig. 6. CTC의 첫 번째 과정*

여기서 x가 중간에 어디를 들렀다가, 거기서 출력된 값을 softmax에 통과시켜 위의 수식 값이 나오는건데 다들 아시겠지만 이는 Recurrent Neural Network(RNN) 계열 네트워크입니다.

2014년에 ICML에서 *Graves*가 제시한 논문에서는 LSTM을 사용했고, 이는 더 나아가 트랜스포머 네트워크를 써도 상관 없습니다.


이제 우리는 정답 sequence에 대해 가능한 어떤 path 중 하나($$a$$)에 대한 확률값을 아래처럼 정의할 수 있습니다.

<center>$$ Pr(a|x) = \prod_{t=1}^{T}Pr(a_t,t|x) $$</center>

자, 일단 가능한 '정답 sequence에 대해 가능한 어떤 path 중 하나($$a$$)'가 뭔지 감이 안옵니다.

예시를 들어보도록 하겠습니다. "This is spectrogram"은 설명하기에 너무 길기 때문에

<img width="742" alt="align1" src="https://user-images.githubusercontent.com/48202736/106981507-75e2e180-67a5-11eb-9a2b-e7cf47e94019.png">

제가 ```Hello```라는 음성을 뱉었다고 하겠습니다.

여기서 ```Hello```가 정답이 되겠죠, 근데 여기에 ```_``` 라는 특별한 'blank token'을 정의해서,  섞어서 가능한 경우의수를 모두 만들어볼까요? (이는 $$\epsilon$$으로 표현하기도 합니다.)

```
_HELLO
_H_ELLO
_H_E_LLO
...
```

네 그렇습니다 아무리 짧아도 이거를 다 쓸수는 없군요...

자 이제 우리는 또 ```_HELLO``` 라는 하나의 가능한 경우에 대해서 만약 제가 뱉은 음성의 길이가 300이라면 300만큼 늘려서 또 생각할 수 있습니다.

```
_HHHHEEEELLLO... (입력 길이 t 까지)
_HHHHEELLLOOOOOO... (입력 길이 t 까지)
...
```

등등이 될 수 있겠죠?

하지만 위의 경우의수 (path)를 생각할 때 몇가지 규칙이 존재합니다.

```
1. Transitions can only go to the right or lower right directions.
2. There must be at least one blank element between the same elements.
3. Non-blank elements cannot be skipped.
4. The starting point must be from the first two elements, the ending point must be one of the last two elements.
```

즉,

> 1. transition은 오른쪽이나 아래 방향으로만 가능 <br>
> 2. 같은 요소 사이에는 blank가 최소한 하나 존재해야 함<br>
> 3. non-blank인 요소는 건너뛸 수 없음<br>
> 4. 스타팅 포인트는 반드시 blank나, 정답의 맨 첫번째 캐릭터요소 중 하나여야하고, 마지막도 유사함.<br>

이를 그림으로 나타내면 아래와 같습니다.

![ctc_rule1](https://user-images.githubusercontent.com/48202736/106988569-75514780-67b3-11eb-8976-d475f0d15067.png)
*Fig. 7. valid한 path인 blue line들과 invalid한 transition들인 red line들. valid한 경우에 가능한 시작점과 끝점은 녹색 박스로 표시되어 있음 *

![ctc_rule2](https://user-images.githubusercontent.com/48202736/106988571-771b0b00-67b3-11eb-94b5-04568fd1d951.png)
*Fig. 8. "apple"의 경우 가능한 모든 path*


자 이제 감이 오시나요? 위에서 설명한 '정답 sequence에 대해 가능한 어떤 path 중 하나($$a$$)'란 그림에서의 가능한 path들이 되는겁니다.

아래의 수식을 다시 볼까요? 아래의 수식은 입력 x가 주어졌을 때 ```_HHHHEEEELLLO...``` 가 나올 확률이라는 겁니다.

<center>$$ Pr(k,t|x) = \frac{exp(y_t^k)}{\sum_{k'}exp(y_t^k)} $$</center>

<center>$$ Pr(a|x) = \prod_{t=1}^{T}Pr(a_t,t|x) $$</center>

근데 확률값을 보면 우리가 $$Pr(a \vert x)$$ 이 식은 음성 입력 x가 주어졌을때 path a가 출력값으로 나올 확률 인데 
왜 오른쪽 수식에 보면 각각의 토큰들을 전부 곱하는지 의아하실겁니다.

그 이유는 이 출력 토큰값들이 전부 독립(i.i.d)이라고 가정했기 때문에, 확률에서 독립일 경우 확률값을 곱해서 위의식을 나타낼 수 있기 때문입니다.

한번 예시로 계산해볼까요?

```H_E_LL_L_O```의 경우에 대해서 (길이는 신경쓰지 말아주세요) 계산해보면

```
0.6*0.2*0.3*0.4*0.3*0.4*0.6*0.2*0.3*0.4 = 2.48832e-05 
```

가 됩니다. (숫자를 막 넣었더니 엄청 낮네요ㅋㅋㅋ;;)<br><br>

이제 거의 다 왔습니다 ㅎㅎ


마지막으로 우리가 할 것은 이 가능한 path들의 확률 분포값을 전부 더한것을 최대화 하는 겁니다.


x가 주어졌을 때 뭐라도 어떤 path라도 좋으니, 모로가도 서울로만 가면되니까요? 


이를 수식으로 나타내면 다음과 같습니다.

<center>$$ Pr(y|x) = \sum_{a \in S} Pr(a|x) $$</center>

여기서 S는 가능한 path들을 의미하고 우리는 위와같은 likelihood를 결국 구해냈습니다.

이를 그림으로 나타내면 아래와 같습니다.

![medium_ctc1](https://user-images.githubusercontent.com/48202736/106989699-0aedd680-67b6-11eb-820f-07e1db57613b.png)
*Fig. 9. 가능한 모든 path의 확률 값을 더한 최종 likelihood*

위와같이 확률값을 전부 더하는거죠

> 여기서 잠시, 위 수식에서 나온 set S에 대해서 잠깐 설명드리도록 하겠습니다.<br>

```
_HHHHEEEELLLO... (입력 길이 t 까지)
_HHHHEELLLOOOOOO... (입력 길이 t 까지)
...
```

우리는 위와같은 가능한 path들이 있는데

결국에 이 모든 path들은 ```HELLO```라는 정답에 적절히 ```_```를 섞은 뒤 길이를 이렇게 저렇게 규칙에 따라 늘린것이기 때문에

정답은 하나로 같습니다. ```HELLO```인거죠.

여기서 이런 path들을 모두 하나의 y=```HELLO```로 만들어주는 후 처리를

<center>$$ y = B(a) $$</center>

로 표현할 수 있습니다.

B함수가 바로 적당히 ```HHHH_EEE_L_L_O```를 규칙에 맞게 중복되는것은 줄여주고 (de-duplicated) 빼줘서 정답으로 만들어주는것이죠.

그래서 우리는 아래의 식을 

<center>$$ Pr(y|x) = \sum_{a \in S} Pr(a|x) $$</center>

이렇게도 표현할 수 있습니다.

<center>$$ Pr(y|x) = \sum_{a \in B^{-1}(y)} Pr(a|x) $$</center>

- <mark style='background-color: #dcffe4'>  CTC Loss </mark>

자 이제 정말 거의 다했습니다, 우리는 likelihood를 정의했으며, 일반적으로 Maximum likelihood는 negative log likelihood로 만들어 이를 최소화 하는 문제라고 할 수 있으니, 

예를들어 우리가 원하는 정답이 $$y^{\ast}$$라고 하면 최종적인 CTC loss는 아래와 같이 되고 우리는 이걸 최소화 하면 됩니다.

<center>$$ CTC(x) = -logPr(y^{\ast}|x) $$</center>

그리고 당연히 우리는 closed form solution따위는 존재하지 않으니 iterative 하게 loss를 줄이는(likelihood를 높히는) 방향으로 optimization을 통해 네트워크 파라메터를 업데이트하면 되는 겁니다.

![medium_ctc2](https://user-images.githubusercontent.com/48202736/106989707-0de8c700-67b6-11eb-9384-eeb8ca538ef7.png)
*Fig. 10. CTC의 학습 과정*

- <mark style='background-color: #dcffe4'> Decoding </mark>

마지막으로 우리는 학습이 된 네트워크의 파라메터를 통해 최종적으로 출력되는, 벡터에 해당하는 문자들에 대한 확률 분포에서 가장 높은 값들만(argmax) 취할 수 있습니다. 
그리고 아까의 B함수를 통해서 'blank token'과 중복되는 문자들을 지워주면 우리는 원하는 답을 구할 수 있습니다. (이를 Decoding 이라고 합니다)


(사실 이렇게 안하고 beam search를 하는 등 다른 좋은 테크닉이 더많습니다만, 우선은 이렇게 묘사하겠습니다.)

<center>$$ h(x) = \arg \max_{y \in L^{\leq T}} p(y|x).  $$</center>

<img width="367" alt="prefix_decoding" src="https://user-images.githubusercontent.com/48202736/106990727-2e198580-67b8-11eb-8bc0-0fbc8839873b.png">
*Fig. 11. 논문에서 묘사한 CTC의 Prefix Decoding 그림*

<img width="421" alt="ctc2" src="https://user-images.githubusercontent.com/48202736/106981546-8430fd80-67a5-11eb-80e8-5cc099b7f926.png">
*Fig. 12. CTC의 Prefix Decoding의 다른 그림 예시, <b>가 blank token, 이며 이 decoding의 결과는 'cat'이 된다*

하지만, 여기서 이 글이 끝나면 좋겠 사실 그렇지 않습니다...

- <mark style='background-color: #dcffe4'> Dynamic programming </mark>



- <mark style='background-color: #fff5b1'> CTC의 단점 </mark>



- <mark style='background-color: #fff5b1'> Seq2Seq with Attention </mark>



- <mark style='background-color: #fff5b1'> Hybrid Model </mark>



- <mark style='background-color: #fff5b1'> References </mark>

- [2006, ICML, Connectionist Temporal Classification - Labelling Unsegmented Sequence Data With Recurrent Neural Networks](https://dl.acm.org/doi/10.1145/1143844.1143891)

- [2014, ICML, Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf)

- 1.[Attention in end-to-end Automatic Speech Recognition](https://medium.com/intel-student-ambassadors/attention-in-end-to-end-automatic-speech-recognition-9f9e42718d21)

- 2.[An Intuitive Explanation of Connectionist Temporal Classification](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c)

- 3.[Distill Blog : Sequence Modeling With CTC](https://distill.pub/2017/ctc/)

- 4.[Multi-Digit Sequence Recognition With CRNN and CTC Loss Using PyTorch Framework](https://medium.com/swlh/multi-digit-sequence-recognition-with-crnn-and-ctc-loss-using-pytorch-framework-269a7aca2a6)

- 5.[ Natural Language Processing with Deep Learning CS224N/Ling284 - Lecture 12: End-to-end models for Speech Processing](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/cs224n-2017-lecture12.pdf)

- 6.[https://blog.naver.com/sogangori/221183469708](https://blog.naver.com/sogangori/221183469708)

- 7.[https://gogyzzz.blogspot.com/2018/08/ctc.html?m=1](https://gogyzzz.blogspot.com/2018/08/ctc.html?m=1)
