---
title: (미완) Connectionist Temporal Classification (CTC) Algorithm
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true

comments: true
---

본 글은 CTC 원 논문인 [2006, ICML, Connectionist Temporal Classification - Labelling Unsegmented Sequence Data With Recurrent Neural Networks](https://dl.acm.org/doi/10.1145/1143844.1143891)와 [2014, ICML, Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf), 여러 블로그와 대학 강의 (Reference 참고)를 참고하여 작성하였습니다. 


목차는 다음과 같습니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


- <mark style='background-color: #fff5b1'> Connectionist Temporal Classification, CTC </mark>

`Connectionist Temporal Classification (CTC)`는 무엇일까요? 정확히는 CTC Loss 로 이는 간단히 말해서 `음성 인식`같이 입력이 되는 음성의 시퀀스 길이와 출력 되는(디코딩 되는) 정답 텍스트의 길이가 맞지 않아 발생하는 `Miss-Alignment` 문제를 해결하기 위해 2006년 [Alex Graves](https://www.cs.toronto.edu/~graves/)라는 딥마인드의 세계적인 석학에 의해 2006년에 제안된 기법입니다.





![image](https://user-images.githubusercontent.com/48202736/106895390-7fcbfc80-6733-11eb-803c-da984525e0b2.png)
*Fig. 1. Alex Graves의 홈페이지에 있는 그의 사진*


***

```
아래는 제가 개인적으로 좋아하는 Alex Graves가 얼마나 대단한 연구자인지에 대한 잡담이기 때문에 넘어가셔도 됩니다 :)
```

Alex Graves는 Toronto 대학 출신의, 세계적인 석학 [Geoffrey E. Hinton](http://www.cs.toronto.edu/~hinton/)의 제자이며, 
CTC 이외에도 ([A novel connectionist system for unconstrained handwriting recognition](PDF), OCR논문), ([Practical Variational Inference for Neural Networks](https://www.cs.toronto.edu/~graves/nips_2011.pdf), 베이지안 방법론에 쓰이는 VI), ([Sequence transduction with recurrent neural networks](https://arxiv.org/pdf/1211.3711), E2E음성인식의 모델의 큰 축 중 하나인 RNN Transducer), ([Generating sequences with recurrent neural networks](https://arxiv.org/pdf/1308.0850)), ([Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf)) 등등의 굵직한 음성인식(ASR), 활자인식(OCR) 논문에 참여하거나 
음성 합성 ([Wavenet: A generative model for raw audio](https://arxiv.org/pdf/1609.03499)), ([Parallel wavenet: Fast high-fidelity speech synthesis](https://arxiv.org/pdf/1711.10433)) 논문들, 
그리고 굵직한 강화학습 논문들 ([Playing atari with deep reinforcement learning](https://arxiv.org/pdf/1312.5602)), ([Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)) 에도 참여하면서 머신러닝계에 대단한 족적을 남기고 있는 연구자입니다. 

***


아무튼 CTC는 그가 쓴 논문들에서도 알 수 있다 싶이, `음성 인식 (Automatic Speech Recognition, ASR)`, `활자 인식 (Optical Character Recognition)` 등의 task에서 어떻게 miss alignment를 해해결해서 학습을 할 것인가? 의 한 해결책입니다. 음성인식에서는 아직까지도 크게 `CTC loss`를 통해서 학습을 했느냐, 아니면 모델링을 Attention Mechanism을 사용한 `Seq2Seq (Encoder-Decoder 계열)` 했느냐 아니면 `Transducer` 계열로 했느냐로 나눠서 생각할 정도로 영향력이 어마어마한 논문입니다.


자 이제 그럼 앞으로, 언급햇던 Miss Alignment 문제가 무엇인지, 그리고 CTC가 무엇인지에 대해서 깊이 파고들어가 보도록 하겠습니다.

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---





## <mark style='background-color: #fff5b1'> Miss Alignment Problem </mark>

Alignment의 의미는 뭘까요? 사전을 찾아보면 '가지런함'이라고 나와있습니다.
음성인식을 할 때 예를들어 제가 "This is spectrogram"이라고 음성을 녹음했다고 해보겠습니다.

이 음성이 2초라고 해도, 예를들어 음성을 녹음하는 sampling rate를 16000으로 하면 32000길이의 샘플이 생기고, 이를 2D feature map으로 변환하기 위해서
적당한 window size로 `Short Time Fourier Transform (STFT)`을 통해 아래와 같이 스펙트로그램으로 만들었다고 생각해보겠습니다. 

<img width="742" alt="align1" src="https://user-images.githubusercontent.com/48202736/106981507-75e2e180-67a5-11eb-9a2b-e7cf47e94019.png">
*Fig. 1. 일반적인 음성인식에서의 alignment가 맞지 않는 상황*

하지만 아무리 STFT 같은 알고리즘으로 시간축의 길이 (temporal resolution)를 줄여도 200,300개가 넘어가는 수의 벡터들이 존재하고, 
이에 대응되는 정답 sequence는 "T h i s   i s   a   s p e c t r o g r a m"으로 공백을 포함해서 10개가 좀 넘습니다.

과연 300개에 해당하는 벡터중에 어디서부터 어디까지가 "T"이고 어디서부터 어디까지가 "h"일까요?
아니면 단어 단위로 생각해서 300개에 해당하는 벡터중에 어디서부터 어디까지가 "This"가 될까요?
이를 알기란 쉽지 않습니다.(즉 어느 순간에 어느 정답이 나타나는지를 알 수가 없다는 겁니다.)


차라리 입력 Spectrogram이 300개 길이의 벡터들이고, 이 벡터들 각각에 label을 달 수 있다면 좋지 않을까요? "TTTTTThhhhhhhiiiiiiiiissssss ...." 
이런 경우에 그냥 정답이랑 비교하는 분류문제를 풀어서 각 벡터 하나당 정답(알파벳 A~Z 소문자+대문자+공백 = 26개+26개+1개)을 softmax확률 분포로 나타내서 정답 원핫 토큰과 비교할 수 있습니다.


`CTC`는 사실 이런 단순한 생각이 주가되는 방법론인데요,
위 이 입력과 출력 사이를 align해줄 aligner가 필요한데, 여기서 이 역할을 해주는게 CTC라고 할 수 있습니다.


이처럼 입력 시퀀스 길이가 N일때 출력 시퀀스 길이 M이라고 하면 이 둘의 길이가 맞지 않는 문제를 Miss Alignment 문제라고 할 수 있고, 대부분의 Sequence Classification task 에서 겪는 문제입니다.

$$ 
N \neq M 
$$








## <mark style='background-color: #fff5b1'> Sequence Generation Task </mark>

CTC가 적용될 수 있는 문제들은 다양한데요, Sequence를 만들어내는 태스크라면 전부 사용이 가능합니다.

(다만 입력길이가 출력 길이보다 길어야 한다던가 하는 제약사항이? 있습니다.)

### <mark style='background-color: #dcffe4'> Automatic Speech Recognition (ASR) </mark> 

<img width="822" alt="distill_asr" src="https://user-images.githubusercontent.com/48202736/106986492-cca0e900-67ae-11eb-977c-f30623b2cbab.png">
{: style="width: 60%;" class="center"}
*Fig. 2. 음성 인식(Automatic Speech Recognition, ASR)*

![DeepSpeech2](https://user-images.githubusercontent.com/48202736/106983159-c740a000-67a8-11eb-9221-70fddc14c0c1.png)
{: style="width: 60%;" class="center"}
*Fig. 3. CTC 계열 음성인식에서 유명한 모델 중 하나인 DeepSpeec2*

당연히 위의 그림처럼 `CTC loss`를 통해 디자인된 End-to-End ASR Network는 굉장히 많았고, 최근에도 많은 페이퍼들에서 CTC를 사용하고 있습니다.
사실 CTC가 굉장히 오래전에 제안된 방법론이고 단점으로 지적되는 것들이 있는데, 최근에 `Transformer`와 결합되어 사용되고, `언어 모델 (Language Model, LM)`의 성능이 좋아졌다던가, 대량의 데이터를 사용하다 보니 크게 문제가 안된다는 것 같아서 한번 `Seq2Seq 계열`이 인기였다가 다시 더 쓰이는 것 같네요.






### <mark style='background-color: #dcffe4'> Optical Character Recognition(OCR) </mark>

<img width="732" alt="distill_ocr" src="https://user-images.githubusercontent.com/48202736/106986497-cf9bd980-67ae-11eb-8a3a-5275775221e9.png">
{: style="width: 60%;" class="center"}
*Fig. 4. 활자 인식(Optical Character Recognition, OCR)*

![OCR](https://user-images.githubusercontent.com/48202736/106983312-04a52d80-67a9-11eb-994e-e3a102d86697.png)
*Fig. 5. 활자인식을 위한 CRNN Model, 마지막에 CTC가 사용된다.*

마찬가지로 활자 인식같은 task에서도도 alignment가 맞지 않기 때문에 CTC Loss를 통해서 최적의 정답을 구해낼 수 있다고 합니다.








## <mark style='background-color: #fff5b1'> CTC </mark>

이제 CTC에 대해 본격적으로 알아보려고 합니다.
사실 CTC는 생각보다 쉽지않은 알고리즘입니다. (제가 처음 공부할때도 몇번을 다시 봤는지 모르겠습니다...)
`likelihood`를 정의하고 이를 `maximize`하는 것 까지는 쉽지만 그 뒤로 `Dynamic Programming`을 사용하고, 네트워크로 CTC loss의 미분이 어떻게 전파되는지 등에 대한 것들이 특히 쉽지 않습니다.
이를 끝까지 이해하는것은 굉장히 도움이 될 것이라고 확신하기 때문에 어렵더라도 최대한 쉽고 직관적이게 예시를 들어가며 설명을 해보도록 하겠습니다.





### <mark style='background-color: #dcffe4'> Maximum likelihood </mark>

우리에게 길이가 t짜리인 입력 시퀀스(음성) x가 주어졌다고 합시다. (벡터가 t개 있는겁니다.)

이제 우리는 각 벡터들에 대해서 분류 문제를 풀겁니다. (이 벡터는 'a'다 이 벡터는 'p'다 ... 알파벳 개수가 곧 분포가 됩니다.)

그렇다면 t번째 벡터 $$y_t$$ 의 분포 중 k번째(a,b,c.... A,B,...공백 까지 하면 53개겠죠?) 알파벳의 확률값은 아래와 같이 나타낼 수 있습니다.

<center>$$ Pr(k,t|x) = \frac{exp(y_t^k)}{\sum_{k'}exp(y_t^k)} $$</center>

굉장히 평범한, 다중분류할 때 사용하는, 출력의 합이 1이 되게 해주는 소프트맥스 함수입니다.


이를 그림으로 나타내면 아래와 같습니다.

<img width="926" alt="ctc1" src="https://user-images.githubusercontent.com/48202736/106981529-8004e000-67a5-11eb-8314-97c9170829a5.png">
*Fig. 6. CTC의 첫 번째 과정*

여기서 x가 중간에 어디를 들렀다가, 거기서 출력된 값을 softmax에 통과시켜 위의 수식 값이 나오는건데 다들 아시겠지만 이는 Recurrent Neural Network(RNN) 계열 네트워크입니다.

2014년에 ICML에서 *Graves*가 제시한 논문에서는 LSTM을 사용했고, 이는 더 나아가 트랜스포머 네트워크를 써도 상관 없습니다.


이제 우리는 정답 sequence에 대해 가능한 어떤 path 중 하나($$a$$)에 대한 확률값을 아래처럼 정의할 수 있습니다.

<center>$$ Pr(a|x) = \prod_{t=1}^{T}Pr(a_t,t|x) $$</center>

자, 일단 가능한 '정답 sequence에 대해 가능한 어떤 path 중 하나($$a$$)'가 뭔지 감이 안옵니다.

예시를 들어보도록 하겠습니다. "This is spectrogram"은 설명하기에 너무 길기 때문에

<img width="742" alt="align1" src="https://user-images.githubusercontent.com/48202736/106981507-75e2e180-67a5-11eb-9a2b-e7cf47e94019.png">

제가 ```Hello```라는 음성을 뱉었다고 하겠습니다.

여기서 ```Hello```가 정답이 되겠죠, 근데 여기에 ```_``` 라는 특별한 'blank token'을 정의해서,  섞어서 가능한 경우의수를 모두 만들어볼까요? (이는 $$\epsilon$$으로 표현하기도 합니다.)

```
_HELLO
_H_ELLO
_H_E_LLO
...
```

네 그렇습니다 아무리 짧아도 이거를 다 쓸수는 없군요...

자 이제 우리는 또 ```_HELLO``` 라는 하나의 가능한 경우에 대해서 만약 제가 뱉은 음성의 길이가 300이라면 300만큼 늘려서 또 생각할 수 있습니다.

```
_HHHHEEEELLLO... (입력 길이 t 까지)
_HHHHEELLLOOOOOO... (입력 길이 t 까지)
...
```

등등이 될 수 있겠죠?

하지만 위의 경우의수 (path)를 생각할 때 몇가지 규칙이 존재합니다.

```
1. Transitions can only go to the right or lower right directions.
2. There must be at least one blank element between the same elements.
3. Non-blank elements cannot be skipped.
4. The starting point must be from the first two elements, the ending point must be one of the last two elements.
```

즉,

> 1. transition은 오른쪽이나 아래 방향으로만 가능 <br>
> 2. 같은 요소 사이에는 blank가 최소한 하나 존재해야 함<br>
> 3. non-blank인 요소는 건너뛸 수 없음<br>
> 4. 스타팅 포인트는 반드시 blank나, 정답의 맨 첫번째 캐릭터요소 중 하나여야하고, 마지막도 유사함.<br>

이를 그림으로 나타내면 아래와 같습니다.

![ctc_rule1](https://user-images.githubusercontent.com/48202736/106988569-75514780-67b3-11eb-8976-d475f0d15067.png)
*Fig. 7. valid한 path인 blue line들과 invalid한 transition들인 red line들. valid한 경우에 가능한 시작점과 끝점은 녹색 박스로 표시되어 있음 *

![ctc_rule2](https://user-images.githubusercontent.com/48202736/106988571-771b0b00-67b3-11eb-94b5-04568fd1d951.png)
*Fig. 8. "apple"의 경우 가능한 모든 path*


자 이제 감이 오시나요? 위에서 설명한 '정답 sequence에 대해 가능한 어떤 path 중 하나($$a$$)'란 그림에서의 가능한 path들이 되는겁니다.

아래의 수식을 다시 볼까요? 아래의 수식은 입력 x가 주어졌을 때 ```_HHHHEEEELLLO...``` 가 나올 확률이라는 겁니다.

<center>$$ Pr(k,t|x) = \frac{exp(y_t^k)}{\sum_{k'}exp(y_t^k)} $$</center>

<center>$$ Pr(a|x) = \prod_{t=1}^{T}Pr(a_t,t|x) $$</center>

근데 확률값을 보면 우리가 $$Pr(a \vert x)$$ 이 식은 음성 입력 x가 주어졌을때 path a가 출력값으로 나올 확률 인데 
왜 오른쪽 수식에 보면 각각의 토큰들을 전부 곱하는지 의아하실겁니다.

그 이유는 이 출력 토큰값들이 전부 독립(i.i.d)이라고 가정했기 때문에, 확률에서 독립일 경우 확률값을 곱해서 위의식을 나타낼 수 있기 때문입니다.

한번 예시로 계산해볼까요?

```H_E_LL_L_O```의 경우에 대해서 (길이는 신경쓰지 말아주세요) 계산해보면

```
0.6*0.2*0.3*0.4*0.3*0.4*0.6*0.2*0.3*0.4 = 2.48832e-05 
```

가 됩니다. (숫자를 막 넣었더니 엄청 낮네요ㅋㅋㅋ;;)<br><br>

이제 거의 다 왔습니다 ㅎㅎ


마지막으로 우리가 할 것은 이 가능한 path들의 확률 분포값을 전부 더한것을 최대화 하는 겁니다.


x가 주어졌을 때 뭐라도 어떤 path라도 좋으니, 모로가도 서울로만 가면되니까요? 


이를 수식으로 나타내면 다음과 같습니다.

<center>$$ Pr(y|x) = \sum_{a \in S} Pr(a|x) $$</center>

여기서 S는 가능한 path들을 의미하고 우리는 위와같은 likelihood를 결국 구해냈습니다.

이를 그림으로 나타내면 아래와 같습니다.

![medium_ctc1](https://user-images.githubusercontent.com/48202736/106989699-0aedd680-67b6-11eb-820f-07e1db57613b.png)
*Fig. 9. 가능한 모든 path의 확률 값을 더한 최종 likelihood*

위와같이 확률값을 전부 더하는거죠

> 여기서 잠시, 위 수식에서 나온 set S에 대해서 잠깐 설명드리도록 하겠습니다.<br>

```
_HHHHEEEELLLO... (입력 길이 t 까지)
_HHHHEELLLOOOOOO... (입력 길이 t 까지)
...
```

우리는 위와같은 가능한 path들이 있는데

결국에 이 모든 path들은 ```HELLO```라는 정답에 적절히 ```_```를 섞은 뒤 길이를 이렇게 저렇게 규칙에 따라 늘린것이기 때문에

정답은 하나로 같습니다. ```HELLO```인거죠.

여기서 이런 path들을 모두 하나의 y=```HELLO```로 만들어주는 후 처리를

<center>$$ y = B(a) $$</center>

로 표현할 수 있습니다.

B함수가 바로 적당히 ```HHHH_EEE_L_L_O```를 규칙에 맞게 중복되는것은 줄여주고 (de-duplicated) 빼줘서 정답으로 만들어주는것이죠.

그래서 우리는 아래의 식을 

<center>$$ Pr(y|x) = \sum_{a \in S} Pr(a|x) $$</center>

이렇게도 표현할 수 있습니다.

<center>$$ Pr(y|x) = \sum_{a \in B^{-1}(y)} Pr(a|x) $$</center>







### <mark style='background-color: #dcffe4'> Final Objective </mark>

자 이제 정말 거의 다했습니다, 우리는 likelihood를 정의했으며, 일반적으로 Maximum likelihood는 negative log likelihood로 만들어 이를 최소화 하는 문제라고 할 수 있으니, 

예를들어 우리가 원하는 정답이 $$y^{\ast}$$라고 하면 최종적인 CTC loss는 아래와 같이 되고 우리는 이걸 최소화 하면 됩니다.

<center>$$ CTC(x) = -logPr(y^{\ast}|x) $$</center>

그리고 당연히 우리는 closed form solution따위는 존재하지 않으니 iterative 하게 loss를 줄이는(likelihood를 높히는) 방향으로 optimization을 통해 네트워크 파라메터를 업데이트하면 되는 겁니다.

![medium_ctc2](https://user-images.githubusercontent.com/48202736/106989707-0de8c700-67b6-11eb-9384-eeb8ca538ef7.png)
*Fig. 10. CTC의 학습 과정*








### <mark style='background-color: #dcffe4'> Decoding methods </mark>

마지막으로 우리는 학습이 된 네트워크의 파라메터를 통해 최종적으로 출력되는, 벡터에 해당하는 문자들에 대한 확률 분포에서 가장 높은 값들만(argmax) 취할 수 있습니다. 
그리고 아까의 B함수를 통해서 'blank token'과 중복되는 문자들을 지워주면 우리는 원하는 답을 구할 수 있습니다. (이를 Decoding 이라고 합니다)


(사실 이렇게 안하고 beam search를 하는 등 다른 좋은 테크닉이 더많습니다만, 우선은 이렇게 묘사하겠습니다.)

<center>$$ h(x) = \arg \max_{y \in L^{\leq T}} p(y|x).  $$</center>

<img width="367" alt="prefix_decoding" src="https://user-images.githubusercontent.com/48202736/106990727-2e198580-67b8-11eb-8bc0-0fbc8839873b.png">
*Fig. 11. 논문에서 묘사한 CTC의 Prefix Decoding 그림*

<img width="421" alt="ctc2" src="https://user-images.githubusercontent.com/48202736/106981546-8430fd80-67a5-11eb-80e8-5cc099b7f926.png">
*Fig. 12. CTC의 Prefix Decoding의 다른 그림 예시, <b>가 blank token, 이며 이 decoding의 결과는 'cat'이 된다*

하지만, 여기서 이 글이 끝나면 좋겠으나 사실 그렇지 않습니다.

  
  
  
  
  
  
### <mark style='background-color: #dcffe4'> Dynamic programming </mark>

  
어떤 음성의 정답이 `cat`인 경우를 생각해보겠습니다.
가능한 path는 아래와 같습니다.
  
 
$$
p(\pi \vert x) = \prod_{t=1}^T y_{\pi_t}^t
$$
  
$$
p(l \vert x) = \sum_{\pi \in \Beta^{-1} (l) } p(\pi \vert x)  
$$
 
  
입니다.  
  
$$
p(l \vert x) = \sum_{\pi \in \Beta^{-1} (l) } p(\pi \vert x)  
$$

이므로
  
$$
\begin{aligned}
&
p("cat" \vert x) = \sum_{\pi \in \Beta^{-1} ("cat") } p(\pi \vert x)  
& \\
\end{aligned}
$$
  
  

## <mark style='background-color: #fff5b1'> CTC의 단점과 다른 End-to-End 기법들과의 비교 </mark>

  



## <mark style='background-color: #fff5b1'> References </mark>

- [2006, ICML, Connectionist Temporal Classification - Labelling Unsegmented Sequence Data With Recurrent Neural Networks](https://dl.acm.org/doi/10.1145/1143844.1143891)

- [2014, ICML, Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf)

- 1.[Attention in end-to-end Automatic Speech Recognition](https://medium.com/intel-student-ambassadors/attention-in-end-to-end-automatic-speech-recognition-9f9e42718d21)

- 2.[An Intuitive Explanation of Connectionist Temporal Classification](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c)

- 3.[Distill Blog : Sequence Modeling With CTC](https://distill.pub/2017/ctc/)

- 4.[Multi-Digit Sequence Recognition With CRNN and CTC Loss Using PyTorch Framework](https://medium.com/swlh/multi-digit-sequence-recognition-with-crnn-and-ctc-loss-using-pytorch-framework-269a7aca2a6)

- 5.[ Natural Language Processing with Deep Learning CS224N/Ling284 - Lecture 12: End-to-end models for Speech Processing](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/cs224n-2017-lecture12.pdf)

- 6.[https://blog.naver.com/sogangori/221183469708](https://blog.naver.com/sogangori/221183469708)

- 7.[https://gogyzzz.blogspot.com/2018/08/ctc.html?m=1](https://gogyzzz.blogspot.com/2018/08/ctc.html?m=1)

- 9.[S18 Lecture 14: Connectionist Temporal Classification (CTC)](https://www.youtube.com/watch?v=c86gfVGcvh4&t=4216s)

- 10.[S18 Lecture 14: Connectionist Temporal Classification (CTC) slide](http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Spring.2018/www/slides/lec14.recurrent.pdf)
