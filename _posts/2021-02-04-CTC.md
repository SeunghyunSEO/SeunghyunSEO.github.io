---
title: 2006, ICML, Connectionist Temporal Classification - Labelling Unsegmented Sequence Data With Recurrent Neural Networks
categories: Speech_Recognition
tag: [tmp]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Connectionist Temporal Classification, CTC </mark>

CTC는 무엇일까요? 간단하게 말해서 음성인식같은 task에서 입력이 되는 음성의 시퀀스길이와 출력이 되는(디코딩 되는) 받아쓰기(dictation)한 텍스트의 길이가 맞지 않아 발생하는 

Miss-Alignment 문제를 해결하기 위해 2006년 [Alex Graves](https://www.cs.toronto.edu/~graves/)라는 딥마인드의 세계적인 석학에 의해 2006년에 제안된 기법입니다.

![image](https://user-images.githubusercontent.com/48202736/106895390-7fcbfc80-6733-11eb-803c-da984525e0b2.png)
*Fig. 1. Alex Graves의 홈페이지에 있는 그의 사진*

그는 Toronto 대학 출신의, 세계적인 석학 [Geoffrey E. Hinton](http://www.cs.toronto.edu/~hinton/)의 제자이며, 


CTC 이외에도 ([A novel connectionist system for unconstrained handwriting recognition](PDF), OCR논문), ([Practical Variational Inference for Neural Networks](https://www.cs.toronto.edu/~graves/nips_2011.pdf), 베이지안 방법론에 쓰이는 VI), ([Sequence transduction with recurrent neural networks](https://arxiv.org/pdf/1211.3711), E2E음성인식의 모델의 큰 축 중 하나인 RNN Transducer), ([Generating sequences with recurrent neural networks](https://arxiv.org/pdf/1308.0850)), ([Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf)) 등등의 굵직한 음성인식(ASR), 활자인식(OCR) 논문에 참여하거나 


음성 합성 ([Wavenet: A generative model for raw audio](https://arxiv.org/pdf/1609.03499)), ([Parallel wavenet: Fast high-fidelity speech synthesis](https://arxiv.org/pdf/1711.10433)) 논문들, 


그리고 굵직한 강화학습 논문들 ([Playing atari with deep reinforcement learning](https://arxiv.org/pdf/1312.5602)), ([Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)) 에도 참여하는 등 굵직한 업적을 남기고 있습니다...(정말 대단합니다)


아무튼 CTC는 그가 쓴 논문들에서도 알 수 있다 싶이, 음성인식, 활자인식 등에서 출력값을 어떻게 align 해서 뽑을거냐를 해결한 방법론이고, 

음성인식에서는 아직까지도 크게 CTC, 즉 CTC loss를 통해서 시퀀스 모델링을 했느냐, 아니면 Attentio Mechanism을 사용한 Seq2Seq(Encoder-Decoder 계열) 했느냐로 
나눠서 생각할 정도로 영향력이 어마어마한 논문입니다. (+ RNN Transducer) 


자 이제 그럼 앞으로, 언급햇던 Miss Alignment 문제가 무엇인지, 그리고 CTC가 무엇인지에 대해서 깊이 파고들어가 보도록 하겠습니다.

- <mark style='background-color: #fff5b1'> Miss Alignment Problem </mark>

Alignment의 의미는 뭘까요? 사전을 찾아보면 '가지런함'이라고 나와있습니다.

네 맞습니다 음성인식을 할 때 예를들어 제가 "This is spectrogram"이라고 음성을 녹음했다고 해보겠습니다.

이 음성이 2초라고 해도, 예를들어 음성을 녹음하는 sampling rate를 16000으로 하면 32000길이의 샘플이 생기고, 이를 2D feature map으로 변환하기 위해서

적당한 window size를 사용해서 Short Time Fourier Transform(STFT) 해서 아래와 같이 스펙트로그램으로 만들었다고 생각해보겠습니다. 

<img width="742" alt="align1" src="https://user-images.githubusercontent.com/48202736/106981507-75e2e180-67a5-11eb-9a2b-e7cf47e94019.png">
*Fig. 1. 일반적인 음성인식에서의 alignment가 맞지 않는 상황*

하지만 아무리 STFT로 시간축의 Time Resolution 줄여도 200,300이 넘어가는 벡터들이 존재하고, 
우리가 찾고자 하는 정답 sequence는 "T h i s   i s   a   s p e c t r o g r a m"으로 10개가 좀 넘네요

과연 300개에 해당하는 벡터중에 어디서부터 어디까지가 "T"이고 어디서부터 어디까지가 "h"일까요?

아니면 단어 단위로 생각해서 300개에 해당하는 벡터중에 어디서부터 어디까지가 "This"가 될까요?

이를 알기란 쉽지 않습니다.


차라리 300개 길이에 해당하는 입력값에 해당하는 정답이 300개 주어지면 모를까요. "TTTTTThhhhhhhiiiiiiiiissssss ...." 
이런경우에 그냥 정답이랑 비교하는 분류문제를 풀면 되겠죠? 각 벡터 하나당 정답(알파벳 A~Z 소문자+대문자+공백 = 26개+26개+1개)을 softmax확률 분포로 나타내서 정답 원핫 토큰과 비교하면되니까요.

하지만 일반적으로 그렇지 않기 때문에 (unsegmented) 우리는 이 입력과 출력 사이를 align해줄 aligner가 필요한데, 여기서 이 역할을 해주는게 CTC라고 할 수 있습니다.


이처럼 입력 시퀀스 길이가 N일때 출력 시퀀스 길이 M이라고 하면 이 둘의 길이가 맞지 않는 문제를 Miss Alignment 문제라고 할 수 있고, 대부분의 Sequence Classification task 에서 겪는 문제입니다.

<center>$$ N \neq M $$</center>


- <mark style='background-color: #fff5b1'> Sequence Generation Task </mark>

CTC가 적용될 수 있는 문제들은 다양한데요, Sequence를 만들어내는 태스크라면 전부 사용이 가능합니다.

(다만 입력길이가 출력 길이보다 길어야 한다던가 하는 제약사항이? 있습니다.)

- <mark style='background-color: #dcffe4'> Automatic Speech Recognition (ASR) </mark> 

<img width="358" alt="distill_asr" src="https://user-images.githubusercontent.com/48202736/106984911-cbba8800-67ab-11eb-99f4-e29bc193bfa7.png">
*Fig. 2. 음성 인식(Automatic Speech Recognition, ASR)*

![DeepSpeech2](https://user-images.githubusercontent.com/48202736/106983159-c740a000-67a8-11eb-9221-70fddc14c0c1.png)
 {: style="width: 60%;" class="center"}
*Fig. 3. CTC 계열 음성인식에서 유명한 모델 중 하나인 DeepSpeec2*

음성인식에서 주로 사용된다는 말은 입아프니 더 안하도록 하겠습니다.

- <mark style='background-color: #dcffe4'> Optical Character Recognition(OCR) </mark>

<img width="328" alt="distill_ocr" src="https://user-images.githubusercontent.com/48202736/106984914-cd844b80-67ab-11eb-9dc6-a6cb1314639d.png">
*Fig. 4. 활자 인식(Optical Character Recognition, OCR)*

![OCR](https://user-images.githubusercontent.com/48202736/106983312-04a52d80-67a9-11eb-994e-e3a102d86697.png)
*Fig. 5. 활자인식을 위한 CRNN Model, 마지막에 CTC가 사용된다.*

마찬가지로 활자인식도 alignment가 맞지 않기 때문에 CTC Loss를 통해서 최적의 정답을 구해낼 수 있습니다.

(이 외에도 기계번역(Neural Machine Translation, NMT)이나 질문응답(QA,VQA 등) 에도 쓰이는지 까지는 잘 본적이 없어서 모르겠습니다 ㅎ)

- <mark style='background-color: #fff5b1'> CTC </mark>

이제 CTC에 대해 본격적으로 설명해보려고 합니다.

사실 CTC는 굉장히 어려운? 알고리즘입니다. (제가 처음 공부할때도 몇번을 다시 봤는지 모르겠습니다...)


사실 CTC는 코드상에서는 아래처럼 굉장히 간단하게 사용할 수 있습니다.

```
>>> # Target are to be padded
>>> T = 50      # Input sequence length
>>> C = 20      # Number of classes (including blank)
>>> N = 16      # Batch size
>>> S = 30      # Target sequence length of longest target in batch (padding length)
>>> S_min = 10  # Minimum target length, for demonstration purposes
>>>
>>> # Initialize random batch of input vectors, for *size = (T,N,C)
>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
>>>
>>> # Initialize random batch of targets (0 = blank, 1:C = classes)
>>> target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)
>>>
>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
>>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
>>> ctc_loss = nn.CTCLoss()
>>> loss = ctc_loss(input, target, input_lengths, target_lengths)
>>> loss.backward()
```
*Pytorch 공식 문서에 있는 CTC loss Example*
(파이토치 공식 코드는 근데 사실 별로 안좋다고 알려져?(들은것) 같습니다. 지금은 수정됐는지 모르겠지만 일반적으로 wrap-ctc라던가 하는 다른 잘 알려진 오픈소스 코드를 사용하곤 합니다.)

Cross Entropy 정의하는 것 처럼 정의해주고 입력값만 넣어주면 알아서 됩니다...


사실 Cross Entropy가 어떤 의미를 가지는지 모르고 사용하는 게 일반적이기 때문에 'CTC를 이렇게 복잡한데 꼭 알아야 하냐?' 하는 분들이 계시다면 중간에 탈출을 하셔도 됩니다...


하지만 이를 끝까지 이해하는것은 굉장히 도움이 될 것이라고 확신하기 때문에 최대한 쉽고 직관적이게 예시를 들어가며 설명을 하려고 노력해보도록 하겠습니다.


- <mark style='background-color: #fff5b1'> CTC의 단점 </mark>



- <mark style='background-color: #fff5b1'> Seq2Seq with Attention </mark>



- <mark style='background-color: #fff5b1'> Hybrid Model </mark>



- <mark style='background-color: #fff5b1'> References </mark>

- [2006, ICML, Connectionist Temporal Classification - Labelling Unsegmented Sequence Data With Recurrent Neural Networks](https://dl.acm.org/doi/10.1145/1143844.1143891)

- [2014, ICML, Towards End-To-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf)

- 1.[Attention in end-to-end Automatic Speech Recognition](https://medium.com/intel-student-ambassadors/attention-in-end-to-end-automatic-speech-recognition-9f9e42718d21)

- 2.[An Intuitive Explanation of Connectionist Temporal Classification](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c)

- 3.[Distill Blog : Sequence Modeling With CTC](https://distill.pub/2017/ctc/)

- 4.[Multi-Digit Sequence Recognition With CRNN and CTC Loss Using PyTorch Framework](https://medium.com/swlh/multi-digit-sequence-recognition-with-crnn-and-ctc-loss-using-pytorch-framework-269a7aca2a6)

- 5.[ Natural Language Processing with Deep Learning CS224N/Ling284 - Lecture 12: End-to-end models for Speech Processing](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/cs224n-2017-lecture12.pdf)

- 6.[https://blog.naver.com/sogangori/221183469708](https://blog.naver.com/sogangori/221183469708)

- 7.[https://gogyzzz.blogspot.com/2018/08/ctc.html?m=1](https://gogyzzz.blogspot.com/2018/08/ctc.html?m=1)
