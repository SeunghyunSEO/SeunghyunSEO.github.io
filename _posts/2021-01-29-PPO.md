---
title: 2017, Proximal Policy Optimization Algorithms
categories: Deep_Reinforcement_Learning_Paper_Review
tag: [DeepLearning]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Proximal Policy Optimization Algorithms (PPO) </mark>

```
Proximal Policy Optimization Algorithms
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov
OpenAI
{joschu, filip, prafulla, alec, oleg}@openai.com
```

![image](https://user-images.githubusercontent.com/48202736/106094048-b8d50180-6174-11eb-8e8f-02c31a3d12e5.png){: width="40%"}

이 논문은 2017년에 OpenAI에서 공개된 논문으로 입니다. 어떻게 하면 Policy Optimization을 잘 할 수 있는가에 대한 내용으로 굉장히 유명한 논문이라고 합니다.

사실 제가 딥러닝 연구를 하면서 항상 심층 강화 학습 (Deep Reinforcement Learning)에 대해 흥미가 있어 따로 공부를 하거나 프로젝트를 진행해보고 싶었지만 따로 시간을 내기가 쉽지 않았습니다.
그래서 이제부터는 심층 강화 학습 분야의 굵직한 논문들을 최대한 디테일하게 리뷰하도록 하려고 합니다. (가능하면 코드 포함?)


아마 아무것도 모르는 제가 이해하기 위해 쓰는 글이니, 심층 강화 학습을 처음 접하시는 여러분들도 이 논문이 제시하는게 뭔지 쉽게 아실 수 있을 겁니다. (그러기를 바랍니다...)


논문의 1저자인 [John Schulman](http://joschu.net/)은 2015년에 제안한 PPO의 전신이라고 할 수 있는 TRPO를 제안한 적도 있고 현재 OpenAI의 RL team을 co-leading하고 있다고 합니다.
(저자중에는 GPT로 유명한 [Alec Radford](https://scholar.google.com/citations?hl=en&user=dOad5HoAAAAJ&view_op=list_works&sortby=pubdate)도 있군요...! 음성인식, 기계번역, 강화학습 다 하시는 Deepmind의 [Alex Graves](https://www.cs.toronto.edu/~graves/)도 그렇고, 역시 고수들은 올라운더인가 봅니다...)

![image](https://user-images.githubusercontent.com/48202736/106094032-b2468a00-6174-11eb-9e46-4a4bd93a3aa9.png){: width="40%"}

(사진 : 똑똑하고 잘생긴 John Schulman)

PPO에 대한 오피셜 코드는 [OpenAI/baselines](https://github.com/openai/baselines)에서 보실 수 있고(텐서플로우), [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ppo.html) 패키지에서도 사용하실 수 있으니 한번 사용해보시는것도 좋을 것 같습니다. (Pytorch, Tensorflow 둘 다 있음)


- <mark style='background-color: #fff5b1'> 0. Abstract </mark>


- <mark style='background-color: #fff5b1'> 1. Introduction </mark>


- <mark style='background-color: #fff5b1'> 2. Background: Policy Optimization </mark>


- <mark style='background-color: #dcffe4'> Policy Gradient Methods </mark>
  
<center>$$ \hat{g} = \E $$</center>
  
  
- <mark style='background-color: #dcffe4'> Trust Region Methods </mark>


- <mark style='background-color: #fff5b1'> 3. Clipped Surrogate Objective </mark>


- <mark style='background-color: #fff5b1'> 4. Adaptive KL Penalty Coefficient </mark>


- <mark style='background-color: #fff5b1'> 5. Algorithm </mark>


- <mark style='background-color: #fff5b1'> 6. Experiments </mark>


- <mark style='background-color: #fff5b1'> 7. Conclusion </mark>
