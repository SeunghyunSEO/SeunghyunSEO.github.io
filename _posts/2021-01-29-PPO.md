---
title: 2017, Proximal Policy Optimization Algorithms
categories: Deep_Reinforcement_Learning_Paper_Review
tag: [DeepLearning]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Proximal Policy Optimization Algorithms (PPO) </mark>

```
Proximal Policy Optimization Algorithms
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov
OpenAI
{joschu, filip, prafulla, alec, oleg}@openai.com
```

![image](https://user-images.githubusercontent.com/48202736/106094048-b8d50180-6174-11eb-8e8f-02c31a3d12e5.png){: width="40%"}

```
PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. PPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.
```

이 논문은 2017년에 OpenAI에서 공개된 논문으로 입니다. 위에서 말한것 처럼 PPO가 제안된 배경은 TRPO처럼 '어떻게 이 네트워크가 현재 가지고 있는 데이터를 사용해 퍼포먼스가 저하되지 않는 선에서 최대한 policy를 update 할 수 있을까?' 입니다. 하지만 2015년에 제안된 PPO의 전신이라고 할 수 있는 TRPO 알고리즘이 문제를 복잡한 2차 미분으로 풀려고 했다는 문제점이 있었고, 이를 개선해서 좀 더 practical 하게 만든게 PPO라고 생각하시면 될 것 같습니다.


사실 제가 딥러닝 연구를 하면서 항상 심층 강화 학습 (Deep Reinforcement Learning)에 대해 흥미가 있어 따로 공부를 하거나 프로젝트를 진행해보고 싶었지만 따로 시간을 내기가 쉽지 않았습니다.
그래서 이제부터는 심층 강화 학습 분야의 굵직한 논문들을 최대한 디테일하게 리뷰하도록 하려고 합니다. (가능하면 코드 포함?)


아마 아무것도 모르는 제가 이해하기 위해 쓰는 글이니, 심층 강화 학습을 처음 접하시는 여러분들도 이 논문이 제시하는게 뭔지 쉽게 아실 수 있을 겁니다. (그러기를 바랍니다...)


논문의 1저자인 [John Schulman](http://joschu.net/)은 2015년에 제안한 PPO의 전신이라고 할 수 있는 TRPO를 제안한 적도 있고 현재 OpenAI의 RL team을 co-leading하고 있다고 합니다.
(저자중에는 GPT로 유명한 [Alec Radford](https://scholar.google.com/citations?hl=en&user=dOad5HoAAAAJ&view_op=list_works&sortby=pubdate)도 있군요...! 음성인식, 기계번역, 강화학습 다 하시는 Deepmind의 [Alex Graves](https://www.cs.toronto.edu/~graves/)도 그렇고, 역시 고수들은 올라운더인가 봅니다...)

![image](https://user-images.githubusercontent.com/48202736/106094032-b2468a00-6174-11eb-9e46-4a4bd93a3aa9.png){: width="40%"}

(사진 : 똑똑하고 잘생긴 John Schulman)

PPO에 대한 오피셜 코드는 [OpenAI/baselines](https://github.com/openai/baselines)에서 보실 수 있고(텐서플로우), [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ppo.html) 패키지에서도 사용하실 수 있으니 한번 사용해보시는것도 좋을 것 같습니다. (Pytorch, Tensorflow 둘 다 있음)


- <mark style='background-color: #fff5b1'> 0. Abstract </mark>


- <mark style='background-color: #fff5b1'> 1. Introduction </mark>


- <mark style='background-color: #fff5b1'> 2. Background: Policy Optimization </mark>


- <mark style='background-color: #dcffe4'> Policy Gradient Methods </mark>
  
<center>$$ \hat{g} = \hat{\mathbb{E}_t} [ \bigtriangledown_{\theta} log \pi_{\theta} (a_t \mid s_t) \hat{A_t}  ] $$</center>
  
<center>$$ L^{PG}(\theta) = \hat{\mathbb{E}_t} [ log \pi_{\theta} (a_t \mid s_t) \hat{A_t}  ] $$</center>
  
- <mark style='background-color: #dcffe4'> Trust Region Methods </mark>

<center>$$ maximize_{\theta} \hat{\mathbb{E}_t} [ \frac{\pi_{\theta} (a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)} \hat{A_t}  ] $$</center>

<center>$$ subject \space to \space \hat{\mathbb{E}_t}[ KL[ \pi_{\theta_{old}}(\cdot \mid s_t), \pi_{\theta}(\cdot \mid s_t) ] ] \leq \delta $$</center>


<center>$$ maximize_{\theta} \hat{\mathbb{E}_t} [ \frac{\pi_{\theta} (a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)} \hat{A_t} - \beta KL[ \pi_{\theta_{old}}(\cdot \mid s_t), \pi_{\theta}(\cdot \mid s_t) ] ] $$</center>


- <mark style='background-color: #fff5b1'> 3. Clipped Surrogate Objective </mark>

<center>$$ r_t(\theta) =  \frac{\pi_{\theta} (a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)} $$</center>

<center>$$ r_t(\theta_{old}) =  1 $$</center>

<center>$$ L^{CPI}(\theta) = \hat{\mathbb{E}_t} [ \frac{\pi_{\theta} (a_t \mid s_t)}{\pi_{\theta_{old}} (a_t \mid s_t)} \hat{A_t}  ] = \hat{\mathbb{E}_t} [r_t(\theta) \hat{A_t}] $$</center>

<img width="789" alt="스크린샷 2021-01-28 오후 3 13 55" src="https://user-images.githubusercontent.com/48202736/106097927-7e229780-617b-11eb-8079-f898b0391745.png">

<img width="878" alt="스크린샷 2021-01-28 오후 3 14 00" src="https://user-images.githubusercontent.com/48202736/106097932-7f53c480-617b-11eb-876a-6cccf335f53c.png">


- <mark style='background-color: #fff5b1'> 4. Adaptive KL Penalty Coefficient </mark>


- <mark style='background-color: #fff5b1'> 5. Algorithm </mark>

<img width="1125" alt="스크린샷 2021-01-28 오후 3 14 09" src="https://user-images.githubusercontent.com/48202736/106097936-8084f180-617b-11eb-90bd-fb2f011dbb43.png">

- <mark style='background-color: #fff5b1'> 6. Experiments </mark>

- <mark style='background-color: #fff5b1'> 7. Conclusion </mark>

