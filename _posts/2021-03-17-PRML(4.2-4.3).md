---
title: 4.2 - 4.3 Probablistic Generative and Discriminative Models for Classification
categories: Brief_Review_for_PRML
tag: [PRML,MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> 4.2 </mark>



### <mark style='background-color: #dcffe4'> Notation </mark>

- 사후 확률 (posterior) : \\( p(C\_k\|{\bf x}) \\) 
    - 임의의 데이터 \\( x \\) 가 주어졌을 때 이 데이터가 특정 클래스 \\( C\_k \\) 에 속할 확률
    - \\( x \\) 는 샘플 하나를 의미하며 벡터로 표기된 이유는 여러 개의 feature를 가지기 때문임
- 클래스-조건부 밀도 (class-conditional density) : \\( p({\bf x}\|C\_k) \\)
    - 가능도 함수(likelihood function)가 아니냐고 묻는 분이 계시는데, 아니다.
    - 특정 클래스에서 입력된 하나의 데이터 \\( x \\) 가 발현될 확률을 의미하며, 각각의 클래스별로 계산되기 때문에
        - 데이터를 클래스 단위로 나누어 놓고 나면 각 클래스에 대한 \\( p({\bf x}) \\) 를 의미하는 것과 마찬가지가 된다.
        - 보통 가능도 함수(*likelihood*) 등을 통해 얻어진 모수 값을 이용하여 분포의 모양을 선택한 뒤 \\( x \\) 의 확률 값을 구하게 된다.
           - \\(p({\bf x}\|\theta\_{ml}) \\)
    - 가능도 함수 (*likelihood*) : \\( p({\bf X}\|C\_k) = \prod p({\bf x}\_i\|C\_k) \\)
        - 주어진 샘플 데이터가 실제 발현될 확률값을 주로 사용하며, 로그를 붙이는게 일반적이다.
        - 샘플 데이터는 i.i.d 를 가정하므로 보통은 확률 곱으로 표현 가능하다.
        - 특정 분포(distribution)를 사용하는 경우 주로 모수 추정에 사용된다.
        - 모수 추정이 완료되면 클래스-조건부 밀도 등의 식에서 이를 모수 값으로 사용한다.


- 클래스-조건부 밀도 \\( p({\bf x}\|C\_k) \\) 를 이용한 접근 방식에 대해 자세히 알아보도록 하자.
    - Generative 모델은 사후 확률 \\( p(C\_k\|{\bf x}) \\) 를 직접 구하는 것이 아니라 간접적 사후 확률을 예측하는 모델이다.
    - 따라서 사후 확률 대신 클래스-조건부 밀도와 사전 확률 등으로 사후 확률을 예측한다.
    - 즉, 임의의 \\( x \\) 가 특정 클래스에 속할 확률 값을 확인하고 이 중 가장 큰 확률 값을 가지는 클래스로 \\( x \\) 가 속할 클래스를 결정할 수 있다.
    - 가장 먼저 **2-class** 문제를 살펴보자. 다음은 \\( x \\) 가 클래스 \\( C\_1 \\) 에 속할 확률을 모델링하는 식이다. (베이즈 룰)
    
$$p(C_1|{\bf x}) = \dfrac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_1)p(C_1)+p({\bf x}|C_2)p(C_2)}=\dfrac{1}{1+\exp(-a)} = \sigma\;(a) \qquad{(4.57)}$$
    
$$a=\ln{\dfrac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_2)p(C_2)}} \qquad{(4.58)}$$



### <mark style='background-color: #dcffe4'> Sigmoid Function </mark>

- \\( \sigma\;(a) \\) 는 로지스틱 시그모이드(logistic sigmoid) 함수이며, 다음과 같이 정의된다.

$$\sigma\;(a)=\dfrac{1}{1+\exp(-a)} \qquad{(4.59)}$$

- 수식이 왜 이렇게 전개되는지 궁금할수도 있는데, 생각보다 쉽다.

$$\alpha = p({\bf x}|C_1)p(C_1)$$

$$\beta = p({\bf x}|C_2)p(C_2)$$

$$\dfrac{\alpha}{\alpha+\beta}=\dfrac{1}{\frac{\alpha+\beta}{\alpha}} = \dfrac{1}{1+\frac{\beta}{\alpha}}=\dfrac{1}{1+\exp({-\ln(\frac{\alpha}{\beta})})}$$



- 그림에서 보듯 *sigmoid* 라는 용어는 함수 식이 \\( S \\) 자 형태를 취하기 때문에 붙여진 이름이다.
    - 이런 함수들을 가끔 *squashing function* 이라고도 부르는데,
    - \\( x \\) 축 영역의 모든 값에 대응되는 함수 출력 값이 특정 범위에만 존재하기 때문이다. (여기서는 0 ~ 1 사이)

- 시그모이드가 도입된 이유는,
    - 시그모이드 자체가 특정 값으로 수렴되는 성질이 있으며 (0~1 사이의 값) 
    - 따라서 이 값을 확률 값으로 고려를 해도 되기 때문이다.
    - 게다가 모든 점에서 연속이며 미분 가능하므로 수학적 전개에도 매우 편리하다.



### <mark style='background-color: #dcffe4'> Logit </mark>

- 다음으로 로지스틱 시그모이드의 역(inverse)은 다음과 같다.

$$a=\ln\left(\dfrac{\sigma}{1-\sigma}\right) \qquad{(4.61)}$$

- 이를 *로짓(logit)* 이라고 부른다.
- 2-class 문제에서는 \\( \ln \frac{p(C\_1\|{\bf x})}{p(C\_2\|{\bf x})} \\) . 즉 각각의 확률에 대한 비율(ratio)에 로그(log)를 붙인 것과 같다.
    - 이를 로그 오즈(log odds) 라고 한다.
    - 좀 더 자세히 설명하자면, 성공 확률 \\( p \\) 와 실패 확률 \\( (1-p) \\) 에 대한 odds 는 \\( \frac{p}{(1-p)} \\) 이므로 여기에 로그를 붙인 것과 같다.



### <mark style='background-color: #dcffe4'> Softmax and Sigmoid </mark>

- 이 식을 가지고 좀 더 일반화하면 \\( K>2 \\) 인 경우에서도 식을 확장할 수 있다. 
    - 이를 일반 선형 모델(generalized linear model)이라고 한다.

$$p(C_k|{\bf x}) = \dfrac{p({\bf x}|C_k)p(C_k)}{\sum_j{p({\bf x}|C_j)p(C_j)}}=\dfrac{\exp(a_k)}{\sum_j{\exp(a_j)}} \qquad{(4.62)}$$

- 이를 *normalized exponential* 함수라고 부르며, 다중 클래스 분류에 사용되는 시그모이드 식이 된다.
    - 이 때 \\( a({\bf x}) \\) 는 \\( {\bf x} \\) 에 대한 선형 함수로 처리 가능하다.
    - 사실 맨 처음에 설명했던 2-class 모델도 위의 식으로 전개하면 동일한 식을 얻어낼 수 있다. 
        - 위 식을 \\( \exp(a\_1)/(\exp(a\_1)+\exp(a\_2)) \\) 로 놓고 전개하면 2-class 시그모이드가 나온다.
- 어쨌거나 여기서 \\( a\_k \\) 는 다음과 같이 정의된다.
        
$$a_k = \ln(p({\bf x}|C_k)p(C_k)) \qquad{(4.63)}$$

- *normalized exponential* 함수를 소프트 맥스 (*softmax function*) 함수라고 부른다.
- 이는 *max* 함수에 대한 평활화(smoothed) 버전이기 때문이다.



### <mark style='background-color: #dcffe4'> Continuous Inputs vs Discrete Inputs </mark>

#### Continuous Inputs

- 일단 클래스-조건부 밀도(class-conditional density)가 가우시안 형태라고 가정하자. 
- 또한 가장 간단한 구조를 고려하여 모든 클래스 사이의 공분산(covariance) 값은 모두 동일하다고 가정한다. (중요한 제약)
- 그러면 어떤 클래스가 주어졌을 때 해당 데이터가 나올 확률은 다음과 같다.

$$p({\bf x}|C_k) = \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp\left\{-\dfrac{1}{2}({\bf x}-{\bf \mu}_k)^T\Sigma^{-1}({\bf x}-{\bf \mu}_k)\right\} \qquad{(4.64)}$$

- **2-class** 문제로 이를 고려해보자.
- 최초 조건부 확률 식에 판별식을 넣는다.

$$p(C_1|{\bf x})=\sigma({\bf w}^T{\bf x}+w_0) \qquad{(4.65)}$$

- 이 식은 위에서  \\( \sigma(a) \\) 로 정의되어 있었다. 마찬가지로 \\( a=\ln\frac{p({\bf x}\|C\_1)p(C\_1)}{p({\bf x}\|C\_2)p(C\_2)} \\) 였다.
- 가우시안 분포 식을 위에 넣고 대입한다.

$${\bf w}^T{\bf x}+w_0 = \ln\frac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_2)p(C_2)}$$

- 이를 전개하면 다음의 식이 얻어진다.

$${\bf w} = \Sigma^{-1}({\bf \mu_1}-{\bf \mu_2}) \qquad{(4.66)}$$

$$w_0 = -\frac{1}{2}{\bf \mu_1}^T\Sigma^{-1}{\bf \mu_1} + \frac{1}{2}{\bf \mu_2}^T\Sigma^{-1}{\bf \mu_2} + \ln{\frac{p(C_1)}{p(C_2)}} \qquad{(4.67)}$$

- \\( x \\) 에 대한 2차 텀이 모두 사라지면서 직선 식이 얻어진다. 
    - 이는 두 클래스 사이의 공분산이 동일하기 때문에 이차항의 계수가 부호만 다르고 크기가 일치하여 약분되기 때문이다.
    - 자세한 전개 방식은 생략한다.




#### Discrite Inputs

- 이제 입력 값이 연속적인 값이 아니라 이산적인 값이라고 생각해보자. ( \\( x_i \\))
- 문제를 단순화하기 위해 \\( x_i \\) 가 가질 수 있는 값은 \\( x_i \in \{0, 1\} \\) 뿐이다.
- 입력 데이터가 \\( D \\) 차원이라면 각 클래스별로 얻을 수 있는 확률 분포의 실제 \\( x \\) 의 이산적인 범위는 \\( 2^D \\) 개이다.
   - 이 중 독립 변수는 \\( 2^D-1 \\) 이며 확률의 총 합이 1이기 때문에 변수 하나가 줄어들었다. (summation constraint)
- 여기서는 \\( x \\) 의 각 속성(feature)이 독립적이라고 가정하여 계산의 범위를 축소하도록 한다.
- 각 속성(feature)들이 모두 독립적이라는 가정을 *Naive Bayes* 가정이라고도 한다.
- 이제 샘플 하나의 클래스-조건부 확률 모델을 다음과 같이 기술할 수 있다.

$$p({\bf x}|C_k) = \prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i} \qquad{(4.81)}$$

- 이 식을 K-class 문제에서의 \\( a\_k \\) 함수에 대입하면

$$a_k({\bf x})=\ln(p({\bf x}|C_k)p(C_k))$$

$$a_k({\bf x})=\sum_{i=1}^{D}\left\{x_i\ln \mu_{ki}+(1-x_i)\ln(1-\mu_{ki})\right\}+\ln p(C_k) \qquad{(4.82)}$$

- 여기서도 \\( a\_k \\) 함수는 \\( x\_k \\) 에 대해 선형 함수이다. (feature가 독립적이라 가정했으므로)
- 2-class에서는 시그모이드를 도임하면 동일한 식을 얻을 수 있다.
- 여기는 \\( x\_i \\) 의 값이 이진 값인 경우만 고려했다. 하지만 \\( M>2 \\) 이 상태를 가지는 \\( x\_i \\) 에 대해서도 유사한 결과를 얻을 수 있다. 


### <mark style='background-color: #dcffe4'> Exponential Family </mark>

생략


## <mark style='background-color: #fff5b1'> 4.3 </mark>


- 2-class 문제에서는 클래스 \\( C\_1 \\) 로 분류되는 사후 확률 \\( p(C\_1\|{\bf x}) \\) 가 시그모이드(sigmoid) 함수로 제공되는 것과 K-class 문제에서는 클래스 \\( C\_k \\) 로 분류되는 사후 확률 \\( p(C\_k\|{\bf x}) \\) 가 소프트맥스(softmax) 함수로 제공되는 것을 확인하였다.
- 이를 해결하기 위해 실제 계산은 클래스-조건부 확률(class-conditional density) 값을 이용하였음을 확인하였다.

- 여기서는 다른 접근 방식을 취한다. 
    - MLE를 활용하여 **직접적으로** 파라미터를 결정하는 방법을 알아보자.
    - 이를 위한 iterative reweighted least squares(IRLS) 알고리즘을 살펴볼 것이다.

- 앞절에서는 간접적인 파라미터 결정 방법으로 클래스-조건부 확률(class-conditional density)를 알아보았는데, 이는 Generative 모델 방식이다.
    - 클래스-조건부 밀도와 사전 확률를 이용하여 사후 확률을 결정한다.
    
- 이번 절에서는 *discriminative* 학습의 형태를 제안하고 이를 통해 \\( p(C\_k\|{\bf x}) \\) 를 정의하여 MLE를 사용한다.
    - 이런 방식의 장점은 구해야 할 모수(paramter)의 개수가 Generative 모델에 비해 적다
    - 또한 Generative 모델에서 클래스-조건(class-conditional) 확률 분포를 잘못 선택하거나 실제 데이터의 분포를 잘 근사하지 못하는 경우 이를 사용하면 더 좋은 결과를 얻을 수 있다.
    



### <mark style='background-color: #dcffe4'> Fixed basis functions </mark>


- \\( \phi({\bf x}) \\) 공간에서 선형으로 분리되는 클래스들이 원래 공간 상에서도 선형 분류가 되어야 할 필요는 없다.
- 그리고 3장에서 살펴보았듯이 \\( \phi\_0({\bf x})=1 \\) 을 추가로 정의하여 식에 넣는다.
- 이에 상응되는 \\( w\_0 \\) 파라미터는 선형 회귀에서와 동일하게 Bias 역할을 수행하게 된다.

- 현실적으로 클래스-조건부 밀도(class-conditional density) \\( p({\bf x}\|C\_k) \\) 는 각각의 클래스 사이에서 중첩되기 쉽다. 
    - 따라서 각각의 값이 0 또는 1 과 같이 명확한 값으로 떨어지기보다는 이 사이의 어떤 값을 가지게 되는 경우가 많다.
    - 1장에서 살펴보았듯 표준화된 결정 이론을 적용하여 사후 확률을 구하게 되는 경우, 이 값은 최적의 솔루션이 된다.
        - 수식에 의해 에러를 최소화하게 된다.
- 그렇다고 \\( \phi({\bf x}) \\) 와 같은 기저 함수를 도입한다고 해서 이러한 중첩 현상이 해소되는 것도 아니다.
    - 오히려 중첩의 정도를 증가시키거나, 원래 관찰 공간에서는 없었던 중첩들을 만들어내기도 한다.
- 그럼 왜 이런 기저 함수를 사용하는가?
    - 기저 함수를 적절하게 선택하는 경우에 오히려 사후 확률 값을 더 쉽게 모델링 할 수 있기 때문.

- 마지막으로 고정된 형태의 기저 함수는 중요한 제약 사항을 가지고 있지만 이런 제약사항들에도 불구하고 실제 응용 단계에서는 많이 활용되고 있으니 일단은 좀 살펴보자.


### <mark style='background-color: #dcffe4'> Logistic Regression </mark>

- 이제 2-class 문제에서의 일반화된 선형 모델(generalized linear model)에 대해 알아보도록 하자.
- 4.2 절에서 분류 \\( C\_1 \\) 에 대한 사후 확률 값은 로지스틱 회귀로 기술되는 것을 확인하였다. 

$$p(C_1|\phi)=y(\phi)=\sigma({\bf w}^T\phi) \qquad{(4.87)}$$

- 기존에는 \\( x \\) 데이터를 바로 사용하였으나 여기서는 \\( \phi \\) 함수를 통해 변환 후 사용하게 된다.
- 변환된 입력 공간에서는 로지스틱 시그모이드 방식과 동일하게 처리된다.
- 2-class 문제이므로 \\( p(C\_2\|\phi)=1-p(C\_1\|\phi) \\) 이다.

- 기저 함수 \\( \phi \\) 가 M 차원을 공간을 가지게 된다면, 이 모델은 \\( M \\) 차원의 조정 가능한 모수를 가지게 된다.
    - 이와 대조적으로 가우시안 클래스-조건부 밀도에서는 일반적으로 더 많은 모수를 요구하게 된다.
        - 여기서는 만약 \\( x \\) 가 \\( M \\) 차원이라면 필요한 평균의 개수는 \\( 2M \\) 이 되고 공분산을 위해 \\( M(M+1)/2 \\) 개의 모수가 추가로 필요하다.
            - 지금 다루고 있는 문제는 2-클래스 문제이다.
        - 만약 사전 분포 \\( p(C\_1) \\) 까지 고려하게 되면 총 \\( M(M+5)/2+1 \\) 개의 파라미터가 필요하게 된다.
        - 즉, \\( M \\) 에 대해 이차 형식으로 추정해야 할 파라미터 개수가 증가한다.
        
- 우리는 로지스틱 회귀 모델의 모수 값을 결정하기 위해 MLE를 사용할 것이다.


### <mark style='background-color: #dcffe4'> Differentiation of Logistic function  </mark>

$$\sigma(a)=\dfrac{1}{1+\exp(-a)}$$

$$\dfrac{d\sigma}{da}=\dfrac{\exp(-a)}{(1+\exp(-a))^2} = \sigma(a)\left\{\dfrac{\exp(-a)}{1+exp(-a)}\right\}=\sigma(a)\left\{1-\dfrac{1}{1+\exp(-a)}\right\} = \sigma(1-\sigma)$$

$$\dfrac{d\sigma}{da} = \sigma(1-\sigma) \qquad{(4.88)}$$



### <mark style='background-color: #dcffe4'> MLE </mark>

- 현재 2-class 문제를 다루고 있으므로 기저 함수를 포함한 가능도 함수(likelihood) 함수를 정의해보자.
    - data set : \\( \{\phi\_n, t\_n\} \\) 
    - \\( t\_n \in \{0, 1\} \\)
    - \\( \phi\_n = \phi({\bf x}\_n) \\)
    - \\( y\_n = y(\phi\_n) = \sigma({\bf w}^T\phi\_n) \\)

$$p({\bf t}\;|{\bf w}) = \prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n} \qquad{(4.89)}$$

- 모수를 추정하기 위해 이번에는 음수항의 로그 가능도 함수를 사용한다. (negative logarithm of likelihood)
- 이를 에러 함수로 정의한다.

$$E({\bf w})= -\ln{p({\bf t}|{\bf w})} = - \sum_{n=1}^{N}\left\{t_n\ln{y_n}+(1-t_n)\ln(1-y_n)\right\} \qquad{(4.90)}$$

- 이런 형태의 에러 함수는 *cross-entropy error function* 으로 알려져 있다.
    - 특별한 것은 없고 가능도 함수에 음의 로그 값을 붙인 결과를 의미한다.

- 이제 \\( {\bf w} \\) 에 대해 미분하면,

$$\triangledown E({\bf w})=\sum_{n=1}^{N}(y_n-t_n)\phi_n \qquad{(4.91)}$$

- 이에 대한 증명은 연습문제 4.13을 참고할 것. (앞서 설명된 \\( \frac{d\sigma}{da} \\) 가 사용된다.)
    - 간단히만 정리해둔다.

$$\frac{\partial E}{\partial y_n} = \frac{1-t_n}{1-y_n} - \frac{t_n}{y_n} = \frac{y_n-t_n}{y_n(1-y_n)}$$

$$\frac{\partial y_n}{\partial a_n} = \frac{\partial \sigma(a_n)}{\partial a_n} = \sigma(a_n)(1-\sigma(a_n)) = y_n(1-y_n)$$

$$\nabla a_n = \sigma_n$$

$$\nabla E({\bf w})=\sum_{n=1}^{N}(y_n-t_n)\phi_n$$


- 식(4.91)을 보면 로지스틱 시그모이드의 미분과 관련된 요소들은 식에 더 이상 남아있지 않고,
    - 로그 가능도 함수의 기울기(gradient)를 위한 간단한 형태의 식이 된다.
- 사실 위의 식은 3장에 언급된 선형 회귀 모델의 \\( \triangledown E(w) \\) 와도 거의 같은 형태를 취하고 있다.
- 원한다면 한 번에 하나의 데이터를 추가하여 값을 업데이트하는 순차적인 모델을 만들어 낼 수 있다.
    - \\( n^{th} \\) 에 대한 입력을 \\( \triangledown E_n \\) 로 놓으면 된다.
    - 업데이트를 위한 식은 3장에서 언급한 것처럼 (식 3.22)를 사용하면 된다.

$${\bf w}^{(\tau+1)} = {\bf w}^{(\tau)}-\eta\triangledown E_n$$

- MLE를 사용하는 방식이 오버 피팅을 만들어낼 수 있다는 것은 이미 알고 있을 것이다.
    - 여기서도 마찬가지인데, 데이터 집합이 선형 분류가 가능한 경우 최대한 이를 분류하기 위해 과도한 피팅을 시도하기도 한다.
    - so prior + likelihood => MAP -> Regularized Logistic Regression




### <mark style='background-color: #dcffe4'> Iterative reweighted least squares </mark>

- 이제 3장에서 소개한 선형 회귀 모델에서의 MLE를 잠시 떠올려보자.
- 우리는 가우시안 노이즈 모델을 선정하여 식을 모델링했고, 이 식은 닫힌 형태(closed-form)이므로 최적의 해를 구할 수 있었다.
    - 이 식은 모수 벡터 \\( {\bf w} \\) 에 대해 2차식의 형태로 제공된다. 그래서 최소값이 1개이다.
- 로지스틱 회귀에서는 안타깝게도 더 이상 닫힌 형태의 식이 아니다.
    - 시그모이드 식에 의해서 비 선형 모델이 된다..
    - 하지만 2차식이 그리 중요한 요소는 아님
    - 좀 더 정확하게 이야기하자면, 에러 함수는 `Convex` 함수 이므로 *Newton-Raphson* 기법을 이용하여 최적화 가능하다.
    - 이를 통해 로그 가능도 함수에 대한 2차 근사식으로 사용 가능하다.
    - Newton-Raphson 갱신 방식은 아래와 같이 주어진다.

$${\bf w}^{(new)}={\bf w}^{(old)}-{\bf H}^{-1}\nabla E({\bf w}) \qquad{(4.92)}$$

- 여기서 \\( {\bf H} \\) 는 헤시안 행렬로 \\( E({\bf w}) \\) 함수에 대한 2차 미분값으로 정해진다.
- 이제 식(3.3) 과 식(3.12)를 이용하여 위의 식을 적용해 보도록 한다.
- 참고로 예전 식들은 다음과 같다.

$$y({\bf x}, {\bf w}) = \sum_{j=0}^{M-1} w_j \phi_j({\bf x}) = {\bf w}^T \phi({\bf x}) \qquad{(3.3)}$$

$$E_D({\bf w}) = \frac{1}{2}\sum_{n=1}^{N} \{ t_n - {\bf w}^T\phi({\bf x}_n) \}^2 \qquad{(3.12)}$$

- 따라서 이 함수의 그라디언트와 헤시안은 다음과 같다.

$$\nabla E({\bf w}) = \sum_{n=1}^{N} ({\bf w}^T \phi_n - t_n)\phi_n = \Phi^T\Phi{\bf w} - \Phi^T{\bf t} \qquad{(4.93)}$$

$${\bf H} = \nabla\nabla E({\bf w}) = \sum_{n=1}^{N} \phi_n \phi_n^T = \Phi^T \Phi \qquad{(4.94)}$$

- 여기서 \\( \Phi \\) 는 \\( N \times M \\) 크기의 행렬이다. (이 때 \\( n^{th} \\) 열은 \\( \phi\_n^T \\) 에 의해 주어진다.)
- *Newton-Raphson* 업데이트 식은 다음과 같이 된다.

$${\bf w}^{(new)} = {\bf w}^{(old)} - (\Phi^T\Phi)^{-1} \left\{ \Phi^T\Phi {\bf w}^{(old)}-\Phi^T {\bf t} \right\} = (\Phi^T\Phi)^{-1}\Phi^T{\bf t} \qquad{(4.95)}$$

- 이 식은 이미 앞서 보았던 최소 제곱법의 식이다.
    - 이 경우에는 에러 함수가 이차형식( `quadratic` ) 꼴이므로 *Newton-Raphson* 식이 한번의 반복으로 값을 얻을 수 있도록 전개된다.
    
-----

- 이제 *Newton-Raphson* 업데이트를 로지스틱 회귀에 적용해보기 위해 `cross-entropy` 오차함수에 대입을 해보도록 하자. (식 4.90)

$$\nabla E({\bf w}) = \sum_{n=1}^{N} (y_n - t_n)\phi_n = \Phi^T({\bf y}-{\bf t}) \qquad{(4.96)}$$

$${\bf H} = \nabla\nabla E({\bf w}) = \sum_{n=1}^{N} y_n(1-y_n) \phi_n \phi_n^T = \Phi^T {\bf R} \Phi \qquad{(4.97)}$$

- 여기서 \\( R \\) 은 \\( N \times N \\) 인 대각 행렬이다.

$$R_{nn} = y_n(1-y_n) \qquad{(4.98)}$$

- 이 식에 의해 Hessian 행렬은 더이상 상수(constant)가 아니다.
    - \\( R \\) 로 인해 \\( {\bf w} \\) 값에 의존하게 되고, 에러 함수는 더 이상 이차형식이 아니게 된다.
    
- \\( y\_n \\) 이 \\( (0 \le y\_n \le 1) \\) 이라는 사실로 부터 (이는 시그모이드 함수의 출력 범위이다.) \\( H \\) 의 속성을 이해할 수 있다.
    - 임의의 벡터 \\( {\bf u} \\) 에 대해 \\( {\bf u}^T{\bf H}{\bf u}>0 \\) 을 만족한다.
    - 따라서 헤시안 행렬 \\( {\bf H} \\) 는 양의 정부호 행렬(positive definition)을 만족하게 되어 에러 함수는 \\( {\bf w} \\) 에 대해 이차형식의 함수가 된다.
    - 최종적으로 하나의 최소값을 찾아 낼 수 있게 된다.
    
- 로지스틱 회귀 모델에서 *Newton-Raphson*  을 활용한 업데이트 모델은 다음과 같다.

$${\bf w}^{(new)} = {\bf w}^{(old)} - (\Phi^T{\bf R}\Phi)^{-1}\Phi^T({\bf y}-{\bf t})\\
= (\Phi^T{\bf R}\Phi)^{-1}\left\{\Phi^T{\bf R}\Phi{\bf w}^{(old)}-\Phi^T({\bf y}-{\bf t})\right\}\\
= (\Phi^T{\bf R}\Phi)^{-1}\Phi^T{\bf R}{\bf z} \qquad{(4.99)}$$

- \\( {\bf z} \\) 는 \\( N \\) 차원의 벡터이다.

$${\bf z} = \Phi{\bf w}^{(old)} - {\bf R}^{-1}({\bf y}-{\bf t}) \qquad{(4.100)}$$

- 식 (4.99)를 자세히 보면 앞서 살펴보았던 최소 제곱법의 일반식(normal equation)과 같은 형태임을 알 수 있다.
    - 여기서 norman equation 은 \\( GD \\) 방식이 아닌 미분을 통한 파라미터 찾기 형태의 식을 말한다.
- 하지만 \\( R \\) 이 상수가 아니라 파라미터 \\( {\bf w} \\) 에 영향을 받는 요소이므로 이를 반복 업데이트 방식으로 풀어야 한다.
    - 이러한 연유로 이러한 식을 IRLS ( *iterative reweighted least squares* )라고 부른다.
- 이렇게 가중치가 부여된 최소 제곱 문제는 가중치 대각 행렬 \\( {\bf R} \\) 을 분산(variance) 값으로 생각할 수도 있는데,
     - 왜냐하면 로지스틱 회귀 문제에서 \\( t \\) 의 평균과 분산 값이 다음으로 주어져있기 때문이다.
  
$$E[t] = \sigma({\bf x}) = y \qquad{(4.101)}$$

$$var[t] = E[t^2] - E[t]^2 = \sigma({\bf x}) - \sigma({\bf x})^2 = y(1-y) \qquad{(4.102)}$$


- 여기서는 \\( t^2=t \\) 를 사용하였다. (단, 이때 \\( t\in\{0,1\} \\) 이다.)
- 사실은 IRLS 는 변수 \\( a={\bf w}^T\phi \\) 에 대해 선형 문제를 가진다.
    - 따라서 로지스틱 시그모이드 함수 지역 근사 기법을 사용한다.

$$a_n({\bf w}) \simeq a_n({\bf w}^{(old)}) + \left. \frac{da_n}{dy_n}\right|_{ {\bf w}^{(old)} }(t_n-y_n)\\
= \phi_n^T {\bf w}^{(old)} - \frac{y_n-t_n}{y_n(1-y_n)}=z_n \qquad{(4.103)}$$




### <mark style='background-color: #dcffe4'> Multiclass logistic regression </mark>

- 다중 클래스 분류 문제를 다루는 Generative 모델에서는 선형 함수인 소프트맥스(softmax)가 사용된다고 이야기했다.

$$p(C_k|\phi) = y_k(\phi) = \frac{\exp(a_k)}{\sum_j \exp(a_j)} \qquad{(4.104)}$$

- 여기서 활성자(activations) \\( a_k \\) 는 다음과 같이 정의된다.

$$a_k = {\bf w}_k^T \phi \qquad{(4.105)}$$

- 클래스 조건부 분포와 사전 분포를 분리하기 위해 MLE 를 사용하고 베이즈 이론을 이용하여 사후 분포를 얻는다.
    - 이를 통해 암묵적으로 파라미터 \\( \{\bf w\_k\} \\) 를 구하게 된다.
    - 이를 위해 \\( y\_k \\) 에 대한 미분 식이 필요하다.
    
$$\frac{\partial y_k}{\partial a_j} = y_k(I_{kj}-y_i) \qquad{(4.106)}$$

- 여기서 \\( I\_{kj} \\) 는 단위 행렬(identity matrix) 이다.
- 이제 MLE 를 구한다. 가장 쉬운 방법은 타겟 벡터 \\( {\bf t}\_n \\) 에 대해 \\( 1-of-K \\) 코딩 스킴을 적용한다.
    - 따라서 클래스 \\( C\_k \\) 는 이진 벡터가 되어 \\( k \\) 에 해당되는 값은 1로 설정되고 나머지는 0으로 설정된다.
- 이 때의 가능도 함수는 다음과 같다.

$$p({\bf T}|{\bf w}_1,...{\bf w}_K) = \prod_{n=1}^{N}\prod_{k=1}^{K} p(C_k|\phi_n)^{t_{nk}} = \prod_{n=1}^{N}\prod_{k=1}^{K}y_{nk}^{t_{nk}} \quad{(4.107)}$$

- 이 때 \\( y\_{nk} = y\_k(\phi\_n) \\) 이고 \\( {\bf T} \\) 는 \\( N \times K \\) 인 행렬이다.
- 여기에 음의 로그 값을 붙이면 다음과 같아진다.

$$E({\bf w}_1, ..., {\bf w}_K) = -\ln p({\bf T}|{\bf w}_1, ...,{\bf w}_K) = - \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk}\ln(y_{nk}) \qquad{(4.108)}$$

- 이는 *cross-entroy* 라고 알려져있다.
- 정의된 에러함수 \\( E \\) 에 대한 미분값을 알아보도록 하자.
    - 이 때 식 (4.106)을 사용하여 구하면 된다.
    
$$\nabla_{ {\bf w}_j }E({\bf w}_1, ...,{\bf w}_K) = \sum_{n=1}^{N} (y_{nj}-t_{nj})\phi_n \qquad{(4.109)}$$

- 이 식을 구하기 위해 \\( \sum\_k t\_{nk}=1 \\) 을 사용하였다. (소프트맥스의 특징이다.)
- 이 식은 선형 모델과 로지스틱 회귀 모델에서 보았던 에러 함수와 동일한 기울기와 형태를 취하고 있는 것을 알 수 있다.

- 한번에 하나의 패턴에 적용하는 순차 알고리즘 공식을 사용할 수도 있다.
    - 앞서 선형 회귀 모델에서 하나의 데이터 \\( n \\) 에 대해 가능도 함수를 벡터 \\( {\bf w} \\) 로 미분한 식 \\( (y\_n-t\_n) \\) 과 \\( \Phi\_n \\) 를 곱한 식을 얻었었다.
    - 유사하게 에러 함수 식(4.108)과 소프트맥스 활성화 함수의 결합을 같은 형태로 얻을 수 있다.

- 배치(batch) 형태의 알고리즘을 얻기 위해서는 앞서 살펴본 IRLS 알고리즘에 대응되는 형태의 *Newton-Raphson* 업데이트를 사용 가능하다.
    - 이를 위해서는 \\( j \\) , \\( k \\) 내에 있는 \\( M \times M \\) 크기의 헤시안 행렬의 값을 계산해야 한다.

$$\nabla_{w_k}\nabla_{w_j}E({\bf w}_1, ...,{\bf w}_K) = \sum_{n=1}^{N}y_{nk}(I_{kj}-y_{nj})\phi_n\phi_n^T \qquad{(4.110)}$$





### <mark style='background-color: #dcffe4'> Probit regression </mark>

- 지금가지 우리는 클래스-조건부 분포가 지수족 분포를 따를 때의 사후 분포를 로지스틱 회귀를 이용하여 구하는 것을 살펴보았다.
- 이 때 선형 함수에 로지스틱 혹은 소프트맥스 변환을 통해 사후 분포를 예측하였다.
- 그러나 클래스-조건부 분포로 선택할 수 있는 분포가 모두 간단한 사후 확률을 가지는 것은 아니다.
    - 예를 들어 가우시안 혼합 분포를 사후 분포로 사용하는 경우 이런 모양을 얻을 수 없다.
- 따라서 여기서는 discriminative 확률 모델의 또 다른 형태를 살펴보도록 한다.
    - 2개의 분류 분제를 가진 일반화된 선형 모델(generalized linear modle) 프레임워크를 고려한다.
    
$$p(t=1|a) = f(a) \qquad{(4.111)}$$

- 이 때 \\( a={\bf w}^T\phi \\) 이고, 함수 \\( f(\cdot) \\) 는 활성 함수(activation function)가 된다.
- 여기서 링크(활성) 함수를 *noisy threshold model* 로 고려해볼 수 있다.
    - 이 함수는 입력 \\( \phi\_n \\) 을 \\( a\_n={\bf w}^T\phi\_n \\) 으로 평가한 다음 이를 다음처럼 해석할 수 있다.

$$\left\{\begin{array}{lr}t_n=1 & if\;a_n\ge \theta \\t_n=0 & otherwise\end{array}\right. \qquad{(4.112)}$$

- 여기서 \\( \theta \\) 는 고정된 값이 아니라 랜덤 변수로 취급한다.
- \\( \theta \\) 가 확률 밀도 \\( p(\theta) \\) 를 가진다면 활성 함수는 다음과 같이 기술할 수 있다.

$$f(a) = \int_{-\infty}^{a}p(\theta)d\theta \qquad{(4.113)}$$

- 이는 조건식에 의해 \\( P(a\ge \theta) \\) 를 나타내는 식이 되기 때문이다.

![figure4.13]({{ site.baseurl }}/images/Figure4.13.png){:class="center-block" height="200px"}

- 여기서 파란색 라인은 \\( p(\theta) \\) 로 두 개의 가우시안 함수의 혼합으로 이루어진 것을 알 수 있다.
- 붉은색 라인은 CDF 인 \\( f(a) \\) 로 대충 시그모이드 함수와 비슷하게 생겼다.
- \\( x \\) 축은 \\( \theta \\) 의 값으로 연두색 영역의 직선 위치가 바로 \\( a \\) 라고 생각하면 된다.
- 이제 \\( p(\theta) \\) 를 표준 정규 분포 (평균이 \\( 0 \\) 이고 분산이 단위 분산 \\( 1 \\) 인 가우시안 확률 분포)로 생각한다면 식을 다음과 같이 기술할 수 있다.

$$\Phi(a) = \int_{-\infty}^{a}N(\theta|0, 1)d\theta \qquad{(4.114)}$$

- 이 함수를 역 프로빗 함수(inverse probit function)라고 부른다.
    - 시그모이드와 거의 유사한 형태의 값을 가진다. 그림 4.9를 참고하면 된다.
- 실제 구현체에서는 다음과 같은 식을 주로 사용한다.

$$erf(a) = \frac{2}{\sqrt{\pi}} \int_{0}^{a} \exp(-\theta^2) d\theta \qquad{(4.115)}$$

- 이 함수는 보통 \\( erf \\) 함수 또는 에러 함수라고 부른다.
    - 이 때 "에러 함수"의 의미가 기계 학습에서의 에러 함수를 의미하는 것이 아니니 유의할 것
- \\( erf \\) 함수를 이용하여 역 프로빗 함수를 전개하면 다음 식을 얻을 수 있다.

$$\Phi(a) = \frac{1}{2} \left\{1+erf\left(\frac{a}{\sqrt{2}}\right)\right\} \qquad{(4.116)}$$

- 이 식을 프로빗 회귀 (probit regression)식이라고 부른다.

---

- MLE를 이용하여 모델의 파라미터 값을 결정할 수 있다.
    - 이 때 프로빗 회귀는 로지스틱 회귀와 유사한 결과를 얻게 된다.
    - 실제 4.5 절에서 프로빗 모델의 사용법을 확인할 것이다.
- 실제 문제에서는 고려해야 할 사항으로 outliers 가 있다.
    - 입력 벡터 \\( s \\) 를 측정할 때 에러가 발생하거나 타겟 값 \\( t \\) 가 잘못 부여된 경우에 발생할 수 있다.
    - 이런 데이터가 존재하는 경우 분류 결과를 심각하게 왜곡할 수 있음
    - 로지스틱 모델과 프로빗 모델은 이러한 상황에서 다르게 동작하게 된다.
        - 로지스틱 시그모이드의 경우 \\( x\rightarrow\infty \\) 이면 활성화 함수의 값이 \\( \exp(-x) \\) 와 같이 급격하게 줄어든다.
        - 반면 프로빗 활성화 함수는 \\( \exp(-x^2) \\) 과 같이 줄어들기 때문에 outlier에 훨씬 민감함
- 하지만 로지스틱과 프로빗 모델 모두 데이터는 모두 정확하게 라벨링되어 있다고 가정하는 모델이다.
    - 물론 잘못된 라벨의 영향도를 타겟 값 \\( t \\) 가 잘못된 값으로 할당될 확률 \\( \epsilon \\) 로 표현하여 모델 요소로 포함할 수 있다.
    - 이 경우 타겟 값에 대한 확률 분포는 다음과 같이 기술할 수 있다.
    
$$p(t|{\bf x}) = (1-\epsilon)\sigma({\bf x}) + \epsilon(1-\sigma({\bf x})) = \epsilon + (1-2\epsilon)\sigma({\bf x}) \qquad{(4.117)}$$





### <mark style='background-color: #dcffe4'> Canonical link functions </mark>

- 가우시안 노이즈 분포를 사용한 선형 회귀 모델에서 에러 함수는 음 로그 가능도 함수(negative log likelihood)를 사용한다. 

$$E_D({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}\{t_n-{\bf w}^T\phi(x_n)\}^2 \qquad{(3.12)}$$

- 위의 식에서 \\( y\_n = {\bf w}^T\Phi\_n \\) 이며 오차함수를 파라미터 \\( {\bf w} \\) 로 미분한 결과로 부터 식을 유도한다.
    - 로지스틱 시그모이드 활성 함수 또는 softmax 활성 함수를  cross-entropy 오차 함수를 결합하는 식에서도 비슷한 것을 확인함
- 이제 이러한 식들을 *exponential family* 식으로 전개를 시켜보도록 하자.
    - 이 때의 활성 함수를 정준 연결 함수 (canonical link function) 이라고 한다.
    - 한국말로 번역한 것이 좀 이상하므로 그냥 영어로 표현해서 사용하는 것이 더 좋을듯 하다.
    - 일단 타겟 변수의 확률 식을 표현해보자.
    
$$p(t|\eta, s) = \frac{1}{s}h\left(\frac{t}{s}\right)g(\eta)\exp\left(\frac{\eta t}{s}\right) \qquad{(4.118)}$$

- 4.2.4절에서는 입력 데이터 \\( {\bf x} \\) 에 대해 *exponential family* 분포를 가정했지만 여기서는 타겟 값 \\( t \\) 에 대해 가정한다.
- 식 (2.226) 을 참고하도록 하자.

$$y \equiv E[t|\eta] = =s\frac{d}{d\eta}\ln g(\eta) \qquad{(4.119)}$$

- 일반화된 선형 모델 ( *generalized linear model* , *Nelder & Wedderburn (1972)* ) 식은 다음과 같다.

$$y = f({\bf w}^T \phi) \qquad{(4.120)}$$

- 여기서 함수 \\( f( \dot ) \\) 는 활성 함수 ( *activation function* )로 알려져있다.
    - 그리고 이 때 \\( f^{-1}( \dot ) \\) 이 바로 연결 함수( *link function* )이다.
- 이제 식 (4.118) 의 *log likelihood* 함수를 \\( \eta \\) 의 함수로 표현해 본다.

$$\ln p({\bf t}|\eta, s) = \sum_{n=1}^N \ln p(t_n|\eta, s) = \sum_{n=1}^N \left\{ \ln g(\eta_n) + \frac{\eta_n t_n}{s} \right\} + const \qquad{(4.121)}$$

- 모든 관찰값이 동일한 스케일(scale) 파라미터를 공유한다고 가정한다.
    - 따라서 \\( s \\) 는 \\( \eta \\) 에 대해 독립적이다.
- 모델 파라미터 \\( {\bf w} \\) 에 대해 미분하면, 

$$\nabla_{\bf w} \ln p({\bf t}|\eta, s) = \sum_{n=1}^N \left\{ \frac{d}{d\eta_n}\ln g(\eta_n) + \frac{t_n}{s} \right\} \frac{d\eta_n}{dy_n} \frac{dy_n}{da_n}\nabla a_n = \sum_{n=1}^N \frac{1}{s} (t_n-y_n)\psi'(y_n)f'(a_n)\phi_n \qquad{(4.122)}$$

- 여기서 \\( a\_n = {\bf w}^T \phi\_n \\) 이고, \\( y\_n = f(a\_n) \\) 이다.

$$f^{-1}(y) = \psi(y) \qquad{(4.123)}$$

- \\( f(\psi(y)) = y \\) 이고 따라서 \\( f'(\psi)\psi'(y) = 1 \\) 이다. 
- 또한 \\( a=f^{-1}(y) \\) 이므로 \\( a=\phi \\) 이고 결국 \\( f'(a)\psi'(y) = 1 \\) 이 된다.

$$\nabla E({\bf w}) = \frac{1}{s} \sum_{n=1}^{N} \{y_n-t_n\}\phi_n \qquad{(4.124)}$$

