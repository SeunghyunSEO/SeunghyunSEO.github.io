---
title: 4.2 - 4.3 Probablistic Generative and Discriminative Models for Classification
categories: Brief_Review_for_PRML
tag: [PRML,MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---

---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


## <mark style='background-color: #fff5b1'> 4.2 </mark>



### <mark style='background-color: #dcffe4'> Notation </mark>

- 사후 확률 (posterior) : \\( p(C\_k\|{\bf x}) \\) 
    - 임의의 데이터 \\( x \\) 가 주어졌을 때 이 데이터가 특정 클래스 \\( C\_k \\) 에 속할 확률
    - \\( x \\) 는 샘플 하나를 의미하며 벡터로 표기된 이유는 여러 개의 feature를 가지기 때문임
- 클래스-조건부 밀도 (class-conditional density) : \\( p({\bf x}\|C\_k) \\)
    - 가능도 함수(likelihood function)가 아니냐고 묻는 분이 계시는데, 아니다.
    - 특정 클래스에서 입력된 하나의 데이터 \\( x \\) 가 발현될 확률을 의미하며, 각각의 클래스별로 계산되기 때문에
        - 데이터를 클래스 단위로 나누어 놓고 나면 각 클래스에 대한 \\( p({\bf x}) \\) 를 의미하는 것과 마찬가지가 된다.
        - 보통 가능도 함수(*likelihood*) 등을 통해 얻어진 모수 값을 이용하여 분포의 모양을 선택한 뒤 \\( x \\) 의 확률 값을 구하게 된다.
           - \\(p({\bf x}\|\theta\_{ml}) \\)
    - 가능도 함수 (*likelihood*) : \\( p({\bf X}\|C\_k) = \prod p({\bf x}\_i\|C\_k) \\)
        - 주어진 샘플 데이터가 실제 발현될 확률값을 주로 사용하며, 로그를 붙이는게 일반적이다.
        - 샘플 데이터는 i.i.d 를 가정하므로 보통은 확률 곱으로 표현 가능하다.
        - 특정 분포(distribution)를 사용하는 경우 주로 모수 추정에 사용된다.
        - 모수 추정이 완료되면 클래스-조건부 밀도 등의 식에서 이를 모수 값으로 사용한다.


- 클래스-조건부 밀도 \\( p({\bf x}\|C\_k) \\) 를 이용한 접근 방식에 대해 자세히 알아보도록 하자.
    - Generative 모델은 사후 확률 \\( p(C\_k\|{\bf x}) \\) 를 직접 구하는 것이 아니라 간접적 사후 확률을 예측하는 모델이다.
    - 따라서 사후 확률 대신 클래스-조건부 밀도와 사전 확률 등으로 사후 확률을 예측한다.
    - 즉, 임의의 \\( x \\) 가 특정 클래스에 속할 확률 값을 확인하고 이 중 가장 큰 확률 값을 가지는 클래스로 \\( x \\) 가 속할 클래스를 결정할 수 있다.
    - 가장 먼저 **2-class** 문제를 살펴보자. 다음은 \\( x \\) 가 클래스 \\( C\_1 \\) 에 속할 확률을 모델링하는 식이다. (베이즈 룰)
    
$$p(C_1|{\bf x}) = \dfrac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_1)p(C_1)+p({\bf x}|C_2)p(C_2)}=\dfrac{1}{1+\exp(-a)} = \sigma\;(a) \qquad{(4.57)}$$
    
$$a=\ln{\dfrac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_2)p(C_2)}} \qquad{(4.58)}$$



### <mark style='background-color: #dcffe4'> Sigmoid Function </mark>

- \\( \sigma\;(a) \\) 는 로지스틱 시그모이드(logistic sigmoid) 함수이며, 다음과 같이 정의된다.

$$\sigma\;(a)=\dfrac{1}{1+\exp(-a)} \qquad{(4.59)}$$

- 수식이 왜 이렇게 전개되는지 궁금할수도 있는데, 생각보다 쉽다.

$$\alpha = p({\bf x}|C_1)p(C_1)$$

$$\beta = p({\bf x}|C_2)p(C_2)$$

$$\dfrac{\alpha}{\alpha+\beta}=\dfrac{1}{\frac{\alpha+\beta}{\alpha}} = \dfrac{1}{1+\frac{\beta}{\alpha}}=\dfrac{1}{1+\exp({-\ln(\frac{\alpha}{\beta})})}$$



- 그림에서 보듯 *sigmoid* 라는 용어는 함수 식이 \\( S \\) 자 형태를 취하기 때문에 붙여진 이름이다.
    - 이런 함수들을 가끔 *squashing function* 이라고도 부르는데,
    - \\( x \\) 축 영역의 모든 값에 대응되는 함수 출력 값이 특정 범위에만 존재하기 때문이다. (여기서는 0 ~ 1 사이)

- 시그모이드가 도입된 이유는,
    - 시그모이드 자체가 특정 값으로 수렴되는 성질이 있으며 (0~1 사이의 값) 
    - 따라서 이 값을 확률 값으로 고려를 해도 되기 때문이다.
    - 게다가 모든 점에서 연속이며 미분 가능하므로 수학적 전개에도 매우 편리하다.



### <mark style='background-color: #dcffe4'> Logit </mark>

- 다음으로 로지스틱 시그모이드의 역(inverse)은 다음과 같다.

$$a=\ln\left(\dfrac{\sigma}{1-\sigma}\right) \qquad{(4.61)}$$

- 이를 *로짓(logit)* 이라고 부른다.
- 2-class 문제에서는 \\( \ln \frac{p(C\_1\|{\bf x})}{p(C\_2\|{\bf x})} \\) . 즉 각각의 확률에 대한 비율(ratio)에 로그(log)를 붙인 것과 같다.
    - 이를 로그 오즈(log odds) 라고 한다.
    - 좀 더 자세히 설명하자면, 성공 확률 \\( p \\) 와 실패 확률 \\( (1-p) \\) 에 대한 odds 는 \\( \frac{p}{(1-p)} \\) 이므로 여기에 로그를 붙인 것과 같다.



### <mark style='background-color: #dcffe4'> Softmax and Sigmoid </mark>

- 이 식을 가지고 좀 더 일반화하면 \\( K>2 \\) 인 경우에서도 식을 확장할 수 있다. 
    - 이를 일반 선형 모델(generalized linear model)이라고 한다.

$$p(C_k|{\bf x}) = \dfrac{p({\bf x}|C_k)p(C_k)}{\sum_j{p({\bf x}|C_j)p(C_j)}}=\dfrac{\exp(a_k)}{\sum_j{\exp(a_j)}} \qquad{(4.62)}$$

- 이를 *normalized exponential* 함수라고 부르며, 다중 클래스 분류에 사용되는 시그모이드 식이 된다.
    - 이 때 \\( a({\bf x}) \\) 는 \\( {\bf x} \\) 에 대한 선형 함수로 처리 가능하다.
    - 사실 맨 처음에 설명했던 2-class 모델도 위의 식으로 전개하면 동일한 식을 얻어낼 수 있다. 
        - 위 식을 \\( \exp(a\_1)/(\exp(a\_1)+\exp(a\_2)) \\) 로 놓고 전개하면 2-class 시그모이드가 나온다.
- 어쨌거나 여기서 \\( a\_k \\) 는 다음과 같이 정의된다.
        
$$a_k = \ln(p({\bf x}|C_k)p(C_k)) \qquad{(4.63)}$$

- *normalized exponential* 함수를 소프트 맥스 (*softmax function*) 함수라고 부른다.
- 이는 *max* 함수에 대한 평활화(smoothed) 버전이기 때문이다.



### <mark style='background-color: #dcffe4'> Continuous Inputs vs Discrete Inputs </mark>

#### Continuous Inputs

- 일단 클래스-조건부 밀도(class-conditional density)가 가우시안 형태라고 가정하자. 
- 또한 가장 간단한 구조를 고려하여 모든 클래스 사이의 공분산(covariance) 값은 모두 동일하다고 가정한다. (중요한 제약)
- 그러면 어떤 클래스가 주어졌을 때 해당 데이터가 나올 확률은 다음과 같다.

$$p({\bf x}|C_k) = \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp\left\{-\dfrac{1}{2}({\bf x}-{\bf \mu}_k)^T\Sigma^{-1}({\bf x}-{\bf \mu}_k)\right\} \qquad{(4.64)}$$

- **2-class** 문제로 이를 고려해보자.
- 최초 조건부 확률 식에 판별식을 넣는다.

$$p(C_1|{\bf x})=\sigma({\bf w}^T{\bf x}+w_0) \qquad{(4.65)}$$

- 이 식은 위에서  \\( \sigma(a) \\) 로 정의되어 있었다. 마찬가지로 \\( a=\ln\frac{p({\bf x}\|C\_1)p(C\_1)}{p({\bf x}\|C\_2)p(C\_2)} \\) 였다.
- 가우시안 분포 식을 위에 넣고 대입한다.

$${\bf w}^T{\bf x}+w_0 = \ln\frac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_2)p(C_2)}$$

- 이를 전개하면 다음의 식이 얻어진다.

$${\bf w} = \Sigma^{-1}({\bf \mu_1}-{\bf \mu_2}) \qquad{(4.66)}$$

$$w_0 = -\frac{1}{2}{\bf \mu_1}^T\Sigma^{-1}{\bf \mu_1} + \frac{1}{2}{\bf \mu_2}^T\Sigma^{-1}{\bf \mu_2} + \ln{\frac{p(C_1)}{p(C_2)}} \qquad{(4.67)}$$

- \\( x \\) 에 대한 2차 텀이 모두 사라지면서 직선 식이 얻어진다. 
    - 이는 두 클래스 사이의 공분산이 동일하기 때문에 이차항의 계수가 부호만 다르고 크기가 일치하여 약분되기 때문이다.
    - 자세한 전개 방식은 생략한다.




#### Discrite Inputs

- 이제 입력 값이 연속적인 값이 아니라 이산적인 값이라고 생각해보자. ( \\( x_i \\))
- 문제를 단순화하기 위해 \\( x_i \\) 가 가질 수 있는 값은 \\( x_i \in \{0, 1\} \\) 뿐이다.
- 입력 데이터가 \\( D \\) 차원이라면 각 클래스별로 얻을 수 있는 확률 분포의 실제 \\( x \\) 의 이산적인 범위는 \\( 2^D \\) 개이다.
   - 이 중 독립 변수는 \\( 2^D-1 \\) 이며 확률의 총 합이 1이기 때문에 변수 하나가 줄어들었다. (summation constraint)
- 여기서는 \\( x \\) 의 각 속성(feature)이 독립적이라고 가정하여 계산의 범위를 축소하도록 한다.
- 각 속성(feature)들이 모두 독립적이라는 가정을 *Naive Bayes* 가정이라고도 한다.
- 이제 샘플 하나의 클래스-조건부 확률 모델을 다음과 같이 기술할 수 있다.

$$p({\bf x}|C_k) = \prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i} \qquad{(4.81)}$$

- 이 식을 K-class 문제에서의 \\( a\_k \\) 함수에 대입하면

$$a_k({\bf x})=\ln(p({\bf x}|C_k)p(C_k))$$

$$a_k({\bf x})=\sum_{i=1}^{D}\left\{x_i\ln \mu_{ki}+(1-x_i)\ln(1-\mu_{ki})\right\}+\ln p(C_k) \qquad{(4.82)}$$

- 여기서도 \\( a\_k \\) 함수는 \\( x\_k \\) 에 대해 선형 함수이다. (feature가 독립적이라 가정했으므로)
- 2-class에서는 시그모이드를 도임하면 동일한 식을 얻을 수 있다.
- 여기는 \\( x\_i \\) 의 값이 이진 값인 경우만 고려했다. 하지만 \\( M>2 \\) 이 상태를 가지는 \\( x\_i \\) 에 대해서도 유사한 결과를 얻을 수 있다. 


### <mark style='background-color: #dcffe4'> Exponential Family </mark>

생략


## <mark style='background-color: #fff5b1'> 4.3 </mark>
