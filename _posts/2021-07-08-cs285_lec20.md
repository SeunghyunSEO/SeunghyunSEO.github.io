---
title: (yet) Lecture 20 - Inverse Reinforcement Learning

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)


Lecture 15의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=EcxpbhDeuZw&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=88)
- [Lecture Slide Link 1](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-20.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

이번 챕터의 주제는 `Inverse Reinforcement Learning (IRL)` 입니다.

![slide1](/assets/images/CS285/lec-20/slide1.png)
*Slide. 1.*


## <mark style='background-color: #fff5b1'> Why Inverse Reinforcement Learning?  </mark>

IRL 이란 뭘까요?

![slide2](/assets/images/CS285/lec-20/slide2.png)
*Slide. 2.*

지금까지 우리는 잘 디자인 된 `reward function`을 기준으로 (어떻게하면 1점 어떻게하면 -5점 ...) 학습을 해왔습니다. 
근데 이 reward function을 잘 정의하는 것 자체가 사람의 손을 많이 타는 어려운 작업이기 때문에 

![slide3](/assets/images/CS285/lec-20/slide3.png)
*Slide. 3.*

$$
\begin{aligned}
&\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}=\arg \max _{\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}} \sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \\

&\mathbf{s}_{t+1}=f\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \\

&\pi=\arg \max _{\pi} E_{\mathbf{s}_{t+1} \sim p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right), \mathbf{a}_{t} \sim \pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \\

&\mathbf{a}_{t} \sim \pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)
\end{aligned}
$$



![slide4](/assets/images/CS285/lec-20/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-20/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-20/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-20/slide7.png)
*Slide. 7.*


given:
states $\mathbf{s} \in \mathcal{S}$, actions $\mathbf{a} \in \mathcal{A}$
(sometimes) transitions $p\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a}\right)$
reward function $r(\mathbf{s}, \mathbf{a})$
$\operatorname{learn} \pi^{\star}(\mathbf{a} \mid \mathbf{s})$


given:
states $\mathbf{s} \in \mathcal{S}$, actions $\mathbf{a} \in \mathcal{A}$
(sometimes) transitions $p\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a}\right)$
samples $\left\{\tau_{i}\right\}$ sampled from $\pi^{\star}(\tau)$
$\operatorname{learn} r_{\psi}(\mathbf{s}, \mathbf{a})$
reward parameters
...and then use it to learn $\pi^{\star}(\mathbf{a} \mid \mathbf{s})$

$$
\begin{aligned}
&\text { linear reward function: }\\
&r_{\psi}(\mathbf{s}, \mathbf{a})=\sum_{i} \psi_{i} f_{i}(\mathbf{s}, \mathbf{a})=\psi^{T} \mathbf{f}(\mathbf{s}, \mathbf{a})
\end{aligned}
$$

![slide8](/assets/images/CS285/lec-20/slide8.png)
*Slide. 8.*

$$
\begin{aligned}
&\text { linear reward function: }\\
&r_{\psi}(\mathbf{s}, \mathbf{a})=\sum_{i} \psi_{i} f_{i}(\mathbf{s}, \mathbf{a})=\psi^{T} \mathbf{f}(\mathbf{s}, \mathbf{a})
\end{aligned}
$$



![slide9](/assets/images/CS285/lec-20/slide9.png)
*Slide. 9.*

$$
\max _{\psi, m} m \quad \text { such that } \psi^{T} E_{\pi^{\star}}[\mathbf{f}(\mathbf{s}, \mathbf{a})] \geq \max _{\pi \in \Pi} \psi^{T} E_{\pi}[\mathbf{f}(\mathbf{s}, \mathbf{a})]+m
$$

$$
\min _{\psi} \frac{1}{2}\|\psi\|^{2} \quad \text { such that } \psi^{T} E_{\pi^{\star}}[\mathbf{f}(\mathbf{s}, \mathbf{a})] \geq \max _{\pi \in \Pi} \psi^{T} E_{\pi}[\mathbf{f}(\mathbf{s}, \mathbf{a})]+D\left(\pi, \pi^{\star}\right)
$$

![slide10](/assets/images/CS285/lec-20/slide10.png)
*Slide. 10.*



![slide11](/assets/images/CS285/lec-20/slide11.png)
*Slide. 11.*

$$
\begin{aligned}
&p(\underbrace{\mathbf{s}_{1: T}, \mathbf{a}_{1: T}}_{\tau})=? ? \quad \text { no assumption of optimal behavior! }\\
&\begin{aligned}
p\left(\tau \mid \mathcal{O}_{1: T}\right) & p\left(\mathcal{O}_{t} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)=\exp \left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \\
p\left(\tau \mid \mathcal{O}_{1: T}\right) &=\frac{p\left(\tau, \mathcal{O}_{1: T}\right)}{p\left(\mathcal{O}_{1: T}\right)} \\
& \propto p(\tau) \prod_{t} \exp \left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)=p(\tau) \exp \left(\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)
\end{aligned}
\end{aligned}
$$



## <mark style='background-color: #fff5b1'> Learning the Reward Function  </mark>

![slide13](/assets/images/CS285/lec-20/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-20/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-20/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-20/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-20/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-20/slide18.png)
*Slide. 18.*



## <mark style='background-color: #fff5b1'> Approximations in High Dimensions  </mark>

![slide20](/assets/images/CS285/lec-20/slide20.png)
*Slide. 20.*

![slide21](/assets/images/CS285/lec-20/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-20/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-20/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-20/slide24.png)
*Slide. 24.*






## <mark style='background-color: #fff5b1'> IRL and GANs  </mark>

![slide26](/assets/images/CS285/lec-20/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-20/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-20/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-20/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-20/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-20/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-20/slide32.png)
*Slide. 32.*




### <mark style='background-color: #dcffe4'> Suggested Reading Papers on IRL </mark>

![slide33](/assets/images/CS285/lec-20/slide33.png)
*Slide. 33.*




## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)































