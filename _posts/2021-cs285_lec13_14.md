---
title: (yet) Lecture 13 and 14 - Exprolation

categories: CS285
tag: [RL]

toc: true
toc_sticky: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)


Lecture 13, 14의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=RTLeJrp5Yp4&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=56)
- [Lecture Slide Link 1](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-13.pdf), [Lecture Slide Link 2](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-14.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

![slide1](/assets/images/CS285/lec-13/slide1.png)
*Slide. 1.*

## <mark style='background-color: #fff5b1'> What’s the problem?  </mark>

![slide2](/assets/images/CS285/lec-13/slide2.png)
*Slide. 2.*

![slide3](/assets/images/CS285/lec-13/slide3.png)
*Slide. 3.*

![slide4](/assets/images/CS285/lec-13/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-13/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-13/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-13/slide7.png)
*Slide. 7.*

![slide8](/assets/images/CS285/lec-13/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-13/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-13/slide10.png)
*Slide. 10.*

![slide11](/assets/images/CS285/lec-13/slide11.png)
*Slide. 11.*



## <mark style='background-color: #fff5b1'> Three Classes of Exploration Methods  </mark>

![slide13](/assets/images/CS285/lec-13/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-13/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-13/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-13/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-13/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-13/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-13/slide19.png)
*Slide. 19.*




## <mark style='background-color: #fff5b1'> Exploration in Deep RL  </mark>

![slide21](/assets/images/CS285/lec-13/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-13/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-13/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-13/slide24.png)
*Slide. 24.*

![slide25](/assets/images/CS285/lec-13/slide25.png)
*Slide. 25.*

![slide26](/assets/images/CS285/lec-13/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-13/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-13/slide28.png)
*Slide. 28.*





## <mark style='background-color: #fff5b1'> More Novelty-Seeking Exploration </mark>

![slide30](/assets/images/CS285/lec-13/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-13/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-13/slide32.png)
*Slide. 32.*

![slide33](/assets/images/CS285/lec-13/slide33.png)
*Slide. 33.*

![slide34](/assets/images/CS285/lec-13/slide34.png)
*Slide. 34.*



## <mark style='background-color: #fff5b1'> Posterior Sampling in Deep RL </mark>

![slide36](/assets/images/CS285/lec-13/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-13/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-13/slide38.png)
*Slide. 38.*



## <mark style='background-color: #fff5b1'> Information Gain in Deep RL </mark>

![slide40](/assets/images/CS285/lec-13/slide40.png)
*Slide. 40.*

![slide41](/assets/images/CS285/lec-13/slide41.png)
*Slide. 41.*

![slide42](/assets/images/CS285/lec-13/slide42.png)
*Slide. 42.*

![slide43](/assets/images/CS285/lec-13/slide43.png)
*Slide. 43.*

![slide44](/assets/images/CS285/lec-13/slide44.png)
*Slide. 44.*

![slide45](/assets/images/CS285/lec-13/slide45.png)
*Slide. 45.*

![slide46](/assets/images/CS285/lec-13/slide46.png)
*Slide. 46.*






## <mark style='background-color: #fff5b1'> Definitions & concepts from information theory </mark>


![slide7](/assets/images/CS285/lec-14/slide7.png)
*Slide. 7.*

![slide8](/assets/images/CS285/lec-14/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-14/slide9.png)
*Slide. 9.*



***

## <mark style='background-color: #fff5b1'> Recap </mark>

![slide2](/assets/images/CS285/lec-14/slide2.png)
*Slide. 2.*

![slide3](/assets/images/CS285/lec-14/slide3.png)
*Slide. 3.*

![slide4](/assets/images/CS285/lec-14/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-14/slide5.png)
*Slide. 5.*





## <mark style='background-color: #fff5b1'> Learning without a reward function by reaching goals </mark>

![slide11](/assets/images/CS285/lec-14/slide11.png)
*Slide. 11.*

![slide12](/assets/images/CS285/lec-14/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-14/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-14/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-14/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-14/slide16.png)
*Slide. 16.*

![slide17](/assets/images/CS285/lec-14/slide17.png)
*Slide. 17.*

![slide18](/assets/images/CS285/lec-14/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-14/slide19.png)
*Slide. 19.*




## <mark style='background-color: #fff5b1'> A state distribution-matching formulation of reinforcement learning </mark>

![slide21](/assets/images/CS285/lec-14/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-14/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-14/slide23.png)
*Slide. 23.*



## <mark style='background-color: #fff5b1'> Is coverage of valid states a good exploration objective? </mark>

![slide25](/assets/images/CS285/lec-14/slide25.png)
*Slide. 25.*




## <mark style='background-color: #fff5b1'> Beyond state covering: covering the space of skills </mark>

![slide27](/assets/images/CS285/lec-14/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-14/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-14/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-14/slide30.png)
*Slide. 30.*




### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)































