---
title: Neural Networks (NN)
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Neural Networks (NN) </mark>

뉴럴 네트워크(Neural Network, NN)는 대체 무엇일까요?
아마 대부분 딥러닝으로 기계학습을 접하신 분들은 전통적인 머신러닝(MLE, MAP, Bayesian, SVM, EM Alogrithm 등등) 보다 '뉴럴 네트워크'라는 말을 더 먼저 들어보셨을 것 같습니다.

(물론 저도 그랬습니다ㅎ)

뉴럴네트워크는 아래같이 생겼죠? 다들 아는 내용이실테니 오늘은 간단히 수식 전개를 하고 학습하는 방법(Loss function 정의, optimization 등)에 대한 얘기는 조금만 하고, 
뉴럴 네트워크의 역사? 등에 대해서는 생략하고 조금 딴길로 새보도록 하겠습니다.

- <mark style='background-color: #fff5b1'> Neural Networks and Logistic Regression for Classification </mark>

뉴럴 네트워크는 보통 아래처럼 생겼습니다.

<img width="642" alt="hugo1" src="https://user-images.githubusercontent.com/48202736/107136261-13c0e280-6945-11eb-8a97-429b8e770da3.png">
*Fig. 1. 일반적인 Neural Network의 그림 from Hugo Larochelle's lecture slide*

우리가 너무 잘 아는 내용이죠. (사실 뭐 그림 넣기도 민망한게, 2010년대 중반까지 대세였던 CNN, RNN 네트워크들, 그리고 최근에 이를 대체하는 듯한 퍼포먼스를 보여주는 Transformer나 ViT(Transformer for Image)같은 최신 기법들에 비하면 아주 빈약한 그림입니다...)

위의 그림에서는 2개의 히든레이어와 그것을 구성하는 파라메터들, 그리고 non-linearity를 위한 activation function이 전부군요.

조금 더 간단한 네트워크를 생각해보겠습니다.

![hugo_00](https://user-images.githubusercontent.com/48202736/107136579-05c09100-6948-11eb-95c2-cd13dc1e19b7.png)
*Fig. 2. 더욱 간단한 NN*

입력값과 파라메터를 곱한다음에 그냥 sigmoid 함수로 계산된 값들을 0~1사이값으로 매핑해주는게 전부이군요.

어디서 본거같지 않나요?

네 맞습니다. 그냥 Logistic Regression 입니다.

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} \lambda^{y_i}(1-\lambda)^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(1-\frac{1}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ L = \sum_{i=1}^{I} y_i log[\frac{1}{1+exp[-\phi^T x_i]}] + \sum_{i=1}^{I}(1-y_i)log[\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]}] $$</center>

위의 수식을 최적화 하면 우리는 가지고 있는 학습 데이터를 잘 분류(Classification) 해주는 Decision Boundary를 하나 구할 수 있다는걸 이제 초등학생들도 알거같습니다.

<img width="1003" alt="hugo2" src="https://user-images.githubusercontent.com/48202736/107136264-17ed0000-6945-11eb-8239-847bbaa1624a.png">
*Fig. 3. 2차원 데이터의 Logistic Regression의 결과로 얻어지는 Decision Boundary, 사실*

- <mark style='background-color: #fff5b1'> XOR Problem </mark>

이제 어떤 분류 문제 예시를 생각해볼까요?

<img width="818" alt="hugo6" src="https://user-images.githubusercontent.com/48202736/107136269-1a4f5a00-6945-11eb-8197-8362743fb5bf.png">
*Fig. 4. XOR 문제, 이는 단순히 결정경계선 하나로 풀 수 없는 문제라고 알려져있다.*

위의 문제는 어떻게 결정 경계선을 그리면 될까요 ??? 

그림에도 나와있듯이 이는 단순히 하나의 decision boundary로는 분류할 수 없는 문제가 됩니다. (단순 logistic regression 안됨)

<img width="1095" alt="xor1" src="https://user-images.githubusercontent.com/48202736/107137353-19bbc100-694f-11eb-8d08-4b7251062b5e.png">
*Fig. 5. AND gate, OR gate 문제,이미지 출처 : [link](http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf)*

(AND, OR문제는 쉽게 하나의 결정 경계선으로도 풀 수 있음이 그림에 잘 나타나 있습니다.)

사실 이 문제는 굉장히 잘 알려져있는 문제 (XOR 문제)로 딥러닝, 뉴럴네트워크에 대해 배우게 되면 맨 처음 배우는 문제라서 다들 아실겁니다.

```
'아 그렇구나...!, 이래서 NN 해야하는구나 ...'
```

다시 문제로 돌아가서 이를 복습해보자면, XOR 논리 talbe은 아래와 같습니다.

<img width="818" alt="hugo6" src="https://user-images.githubusercontent.com/48202736/107136269-1a4f5a00-6945-11eb-8197-8362743fb5bf.png">

|--|--|--|
|x1|x2|y|
|0|0|0|
|0|1|1|
|1|0|1|
|1|1|0|

이를 풀기위해서 우리는 2개의 hidden state를 만든다음 이를 다시 합쳐 출력하는 간단한 1층 NN을 만들면 됩니다.

![xor_mine1](https://user-images.githubusercontent.com/48202736/107137795-58ec1100-6953-11eb-858f-ee76589b1f46.png)
*Fig. 6. 간단한 1-layer Neural Network*

여기에 적당한 값을 아래처럼 주면 굉장히 쉽게 XOR문제를 풀 수 있습니다.

![xor_mine2](https://user-images.githubusercontent.com/48202736/107137797-5b4e6b00-6953-11eb-9d10-376e1998c185.png)
*Fig. 7. XOR을 풀기 위한 Nerual Network*

하지만 이 hidden layer 한층의 두 hidden neuron이 각 각 의미하는 바는 뭘까요?? 

네 그렇습니다. 바로 NAND gate와 OR gate의 결과값을 의미합니다. 이 두 gate는 위의 *Fig. 5* 에서 볼 수 있다 싶이 decision boundary 하나로 풀 수 있는 문제였죠.

|--|--|--|--|--|
|x1|x2|h1(NAND)|h2(OR)|y(XOR)|
|0|0|1|0|0|
|0|1|1|1|1|
|1|0|1|1|1|
|1|1|0|0|0|

즉 위의 그림처럼 한번 NAND, OR 게이트로 갔다가 이의 출력값을 이용해 XOR로 다시 한번 가는 방법으로 문제를 푼겁니다.

![xor_mine3](https://user-images.githubusercontent.com/48202736/107137798-5c7f9800-6953-11eb-95ae-aba373f51657.png)
*Fig. 8. 각가의 hidden layer neuron의 의미*

이를 조금 더 직관적으로 시각화 해서 보면 아래와 같습니다.

<img width="1004" alt="hugo3" src="https://user-images.githubusercontent.com/48202736/107136265-191e2d00-6945-11eb-8825-45c1ab75bb26.png">
*Fig. 9. 각가의 hidden layer neuron의 의미는 사실상 결정 경계면 두개이다. from Pascal Vincent's slide *

즉 사실상 logistic 회귀가 만들어내는 구부러진(비선형의) hyperplane 두개를 잘 합쳐서 신호처리에서 배우는 filter같은 형식을 만들어내고 이를 통해서 분류를 한것이 되는거죠. 

이러한 XOR같은 문제를 ```Non-Lineary-Seperable Problem``` 이라고 하며, AND,OR 같은 문제들을 일반적으로 ```Linearly-Seperable Problem```이라고 합니다.

- <mark style='background-color: #fff5b1'> More Non-Linearly Seperable Problem </mark>

조금 더 복잡한 문제를 풀어볼까요?

바로 위의 *Fig. 9* 에서 여기서 hidden neuron들을 더 추가해보겠습니다. 
그러면 우리는 아래와 같은 분류 곡선을 또한 만들어 낼 수 있습니다.

<img width="1091" alt="hugo4" src="https://user-images.githubusercontent.com/48202736/107136266-19b6c380-6945-11eb-8529-91583e70f1ed.png">
*Fig. 10. 5개의 hidden neuron이 합쳐져 만든 결정 경계*

이는 아래와 같은 도넛 모양의 Non-linearly seperable 데이터를 분류하는 둥그런 분류기가 되겠네요.

![non-linear](https://user-images.githubusercontent.com/48202736/107138033-1b888300-6955-11eb-817c-1ac953ad3c77.png)
*Fig. 11. 도넛 모양의 Non-linearly seperable 데이터들*

물론 여기서 더 복잡하게 뿌려져 있는 데이터들도 뉴런들을 추가해서(표현력을 늘림) 더 정교하게 분류를 할 수 있습니다. 

<img width="1180" alt="cmu10" src="https://user-images.githubusercontent.com/48202736/107136296-2fc48400-6945-11eb-8796-72e5fb7a348d.png">
*Fig. 12. 조금 더 복잡한 오리모양?의 Non-linearly seperable 데이터들*

이를 분류하기 위해서는 1층 가지고는 조금 부족해보이네요. 아래의 그림을 보면서 설명하겠습니다.

<img width="565" alt="cmu12" src="https://user-images.githubusercontent.com/48202736/107136298-305d1a80-6945-11eb-990b-7698adc0495c.png">
*Fig. 13. 2-layer Neural Network*

위의 그림을 보면 출력 단계의 엄청 복잡한 분류경계면을 만들어내기 위해서, 바로 직전의 두 뉴런이 정오각형과 늘어진 오각형의 분류경계를 만들어내죠.
이 5각형 분류경계들은 다시 생각해보면 또 그 밑의 5개의 decision boundary들을 잘 합친게 됩니다.
즉 이런식으로 다 표현할 수 있다는거죠.

- <mark style='background-color: #fff5b1'> Another Perspective </mark>

방금 까지는 Neural Network의 각각의 파라메터를 곱하는 행위와 hidden neuron이 의미하는것이 뭔지를 살펴봤는데, 
이번에는 같은 걸 조금 다른 관점으로 봐보도록 하겠습니다.

선형대수학에서 데이터(벡터죠)와 매트릭스(네트워크의 파라메터)를 곱하는 행위는 뭘까요? 

네 그렇습니다. 선형 변환하는거죠. 데이터를 다른 차원으로 mapping 해주는 겁니다.


MNIST라는 고이다못해 화석이 되어버린 이미지 분류계의 유명한 손글씨 데이터를 생각해보겠습니다.

<img width="1287" alt="mnist" src="https://user-images.githubusercontent.com/48202736/107138457-dfa2ed00-6957-11eb-8d8d-65f02862e689.png">
*Fig. 14. MNIST dataset, 출처 : [link](https://www.mdpi.com/2076-3417/9/15/3169)*

이미지와 다음의 네트워크의 파라메터를 곱하면 hidden neuron은 어떻게 될까요

<center>$$ X \in \mathbb{R}^{1 \times 784} $$</center>
<center>$$ W \in \mathbb{R}^{784 \times 2} $$</center>
<center>$$ X W^T \in \mathbb{R}^{1 \times 2} $$</center>

네 그렇습니다. 2차원의 데이터가 되어버립니다.

![mnist2](https://user-images.githubusercontent.com/48202736/107138699-38bf5080-6959-11eb-80b5-00b3364f65f5.png)

이게 무슨 의미를 가질까요? 

다시 XOR문제를 생각해보도록 하겠습니다.

<img width="1020" alt="ucl_deep2" src="https://user-images.githubusercontent.com/48202736/107136271-1d4a4a80-6945-11eb-97a5-fdbe7b066d52.png">

편의상 색을 바꿔서 표현해보겠습니다.
 
<img width="581" alt="ucl_deep3" src="https://user-images.githubusercontent.com/48202736/107136272-1de2e100-6945-11eb-8495-fe35088e7fb5.png">

아까의 네트워크는 2차원 데이터를 2차원으로 매핑해주는 경우였죠.

![xor_mine2](https://user-images.githubusercontent.com/48202736/107137797-5b4e6b00-6953-11eb-9d10-376e1998c185.png)

아까는 각각의 뉴런 하나가 결정경계 하나를 만들어내고 이 둘을 합치면 새로운 더 복잡한 결정경계가 만들어진다는 접근을 했는데

이번에는 매트릭스를 곱하는 행위 자체가 데이터를 선형변환해서 다른 차원으로 매핑해준다고 생각해보도록 하겠습니다.

<img width="574" alt="ucl_deep5" src="https://user-images.githubusercontent.com/48202736/107136275-1e7b7780-6945-11eb-9f18-8a1a6205fa66.png">
<img width="1185" alt="ucl_deep6" src="https://user-images.githubusercontent.com/48202736/107136276-1e7b7780-6945-11eb-9e3d-c3bbadc92c06.png">

- <mark style='background-color: #fff5b1'> References </mark>

1. [Prince, Simon JD. Computer vision: models, learning, and inference. Cambridge University Press, 2012.](http://www.computervisionmodels.com/)

2. [Colah's Blog : Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

3. [Hugo Larochelle's slides](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)

4. [Pascal Vincent's slides](http://www.iro.umontreal.ca/~vincentp/publications.html)


dcffe4
