---
title: Neural Networks (NN)
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Neural Networks (NN) </mark>

뉴럴 네트워크(Neural Network, NN)는 대체 무엇일까요?
아마 대부분 딥러닝으로 기계학습을 접하신 분들은 전통적인 머신러닝(MLE, MAP, Bayesian, SVM, EM Alogrithm 등등) 보다 '뉴럴 네트워크'라는 말을 더 먼저 들어보셨을 것 같습니다.

(물론 저도 그랬습니다ㅎ)

뉴럴네트워크는 아래같이 생겼죠? 다들 아는 내용이실테니 오늘은 간단히 수식 전개를 하고 학습하는 방법(Loss function 정의, optimization 등)에 대한 얘기는 조금만 하고, 
뉴럴 네트워크의 역사? 등에 대해서는 생략하고 조금 딴길로 새보도록 하겠습니다.

- <mark style='background-color: #dcffe4'> Neural Networks and Logistic Regression for Classification </mark>

뉴럴 네트워크는 보통 아래처럼 생겼습니다.

<img width="642" alt="hugo1" src="https://user-images.githubusercontent.com/48202736/107136261-13c0e280-6945-11eb-8a97-429b8e770da3.png">
{: style="width: 70%;" class="center"}
*Fig. 1. 일반적인 Neural Network의 그림 from Hugo Larochelle's lecture slide*

우리가 너무 잘 아는 내용이죠. (사실 뭐 그림 넣기도 민망한게, 2010년대 중반까지 대세였던 CNN, RNN 네트워크들, 그리고 최근에 이를 대체하는 듯한 퍼포먼스를 보여주는 Transformer나 ViT(Transformer for Image)같은 최신 기법들에 비하면 아주 빈약한 그림입니다...)

위의 그림에서는 2개의 히든레이어와 그것을 구성하는 파라메터들, 그리고 non-linearity를 위한 activation function이 전부군요.

조금 더 간단한 네트워크를 생각해보겠습니다.

![hugo_00](https://user-images.githubusercontent.com/48202736/107136579-05c09100-6948-11eb-95c2-cd13dc1e19b7.png)
{: style="width: 50%;" class="center"}
*Fig. 2. 더욱 간단한 NN*

입력값과 파라메터를 곱한다음에 그냥 sigmoid 함수로 계산된 값들을 0~1사이값으로 매핑해주는게 전부이군요.

어디서 본거같지 않나요?

네 맞습니다. 그냥 Logistic Regression 입니다.

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} \lambda^{y_i}(1-\lambda)^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(1-\frac{1}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ L = \sum_{i=1}^{I} y_i log[\frac{1}{1+exp[-\phi^T x_i]}] + \sum_{i=1}^{I}(1-y_i)log[\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]}] $$</center>

위의 수식을 최적화 하면 우리는 가지고 있는 학습 데이터를 잘 분류(Classification) 해주는 Decision Boundary를 하나 구할 수 있다는걸 이제 초등학생들도 알거같습니다.

<img width="1003" alt="hugo2" src="https://user-images.githubusercontent.com/48202736/107136264-17ed0000-6945-11eb-8239-847bbaa1624a.png">
{: style="width: 100%;" class="center"}
*Fig. 3. 2차원 데이터의 Logistic Regression의 결과로 얻어지는 Decision Boundary, 사실*

- <mark style='background-color: #dcffe4'> XOR Problem </mark>

이제 어떤 분류 문제 예시를 생각해볼까요?

<img width="818" alt="hugo6" src="https://user-images.githubusercontent.com/48202736/107136269-1a4f5a00-6945-11eb-8197-8362743fb5bf.png">
{: style="width: 60%;" class="center"}
*Fig. 4. XOR 문제, 이는 단순히 결정경계선 하나로 풀 수 없는 문제라고 알려져있다.*

위의 문제는 어떻게 결정 경계선을 그리면 될까요 ??? 

그림에도 나와있듯이 이는 단순히 하나의 decision boundary로는 분류할 수 없는 문제가 됩니다. (단순 logistic regression 안됨)

<img width="1095" alt="xor1" src="https://user-images.githubusercontent.com/48202736/107137353-19bbc100-694f-11eb-8d08-4b7251062b5e.png">
{: style="width: 100%;" class="center"}
*Fig. 5. AND gate, OR gate 문제,이미지 출처 : [link](http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf)*

(AND, OR문제는 쉽게 하나의 결정 경계선으로도 풀 수 있음이 그림에 잘 나타나 있습니다.)

사실 이 문제는 굉장히 잘 알려져있는 문제 (XOR 문제)로 딥러닝, 뉴럴네트워크에 대해 배우게 되면 맨 처음 배우는 문제라서 다들 아실겁니다.

```
'아 그렇구나...!, 이래서 NN 해야하는구나 ...' 라는 어느정도 감을 주는 문제죠.
```

다시 문제로 돌아가서 이를 복습해보자면, XOR 논리 talbe은 아래와 같습니다.

<img width="818" alt="hugo6" src="https://user-images.githubusercontent.com/48202736/107136269-1a4f5a00-6945-11eb-8197-8362743fb5bf.png">
{: style="width: 60%;" class="center"}

|--|--|--|
|x1|x2|y|
|0|0|0|
|0|1|1|
|1|0|1|
|1|1|0|

이를 풀기위해서 우리는 2개의 hidden state를 만든다음 이를 다시 합쳐 출력하는 간단한 1층 NN을 만들면 됩니다.

![xor_mine1](https://user-images.githubusercontent.com/48202736/107137795-58ec1100-6953-11eb-858f-ee76589b1f46.png)
{: style="width: 60%;" class="center"}
*Fig. 6. 간단한 1-layer Neural Network*

여기에 적당한 값을 아래처럼 주면 굉장히 쉽게 XOR문제를 풀 수 있습니다.

![xor_mine2](https://user-images.githubusercontent.com/48202736/107137797-5b4e6b00-6953-11eb-9d10-376e1998c185.png)
{: style="width: 60%;" class="center"}
*Fig. 7. XOR을 풀기 위한 Nerual Network*

하지만 이 hidden layer 한층의 두 hidden neuron이 각 각 의미하는 바는 뭘까요?? 

네 그렇습니다. 바로 NAND gate와 OR gate의 결과값을 의미합니다. 이 두 gate는 위의 *Fig. 5* 에서 볼 수 있다 싶이 decision boundary 하나로 풀 수 있는 문제였죠.

|--|--|--|--|--|
|x1|x2|h1(NAND)|h2(OR)|y(XOR)|
|0|0|1|0|0|
|0|1|1|1|1|
|1|0|1|1|1|
|1|1|0|0|0|

즉 위의 그림처럼 한번 NAND, OR 게이트로 갔다가 이의 출력값을 이용해 XOR로 다시 한번 가는 방법으로 문제를 푼겁니다.

![xor_mine3](https://user-images.githubusercontent.com/48202736/107137798-5c7f9800-6953-11eb-95ae-aba373f51657.png)
{: style="width: 90%;" class="center"}
*Fig. 8. 각각의 hidden layer neuron의 의미*

이를 조금 더 직관적으로 시각화 해서 보면 아래와 같습니다.

<img width="1004" alt="hugo3" src="https://user-images.githubusercontent.com/48202736/107136265-191e2d00-6945-11eb-8825-45c1ab75bb26.png">
{: style="width: 100%;" class="center"}
*Fig. 9. 각각의 hidden layer neuron의 의미는 사실상 결정 경계면 두개이다. from Pascal Vincent's slide*

즉 사실상 logistic 회귀가 만들어내는 구부러진(비선형의) hyperplane 두개를 잘 합쳐서 신호처리에서 배우는 filter같은 형식을 만들어내고 이를 통해서 분류를 한것이 되는거죠. 

이러한 XOR같은 문제를 ```Non-Lineary-Seperable Problem``` 이라고 하며, AND,OR 같은 문제들을 일반적으로 ```Linearly-Seperable Problem```이라고 합니다.

- <mark style='background-color: #dcffe4'> More Non-Linearly Seperable Problem </mark>

조금 더 복잡한 문제를 풀어볼까요?

바로 위의 *Fig. 9* 에서 여기서 hidden neuron들을 더 추가해보겠습니다. 
그러면 우리는 아래와 같은 분류 곡선을 또한 만들어 낼 수 있습니다.

<img width="1091" alt="hugo4" src="https://user-images.githubusercontent.com/48202736/107136266-19b6c380-6945-11eb-8529-91583e70f1ed.png">
{: style="width: 100%;" class="center"}
*Fig. 10. 5개의 hidden neuron이 합쳐져 만든 결정 경계*

이는 아래와 같은 도넛 모양의 Non-linearly seperable 데이터를 분류하는 둥그런 분류기가 되겠네요.

![non-linear](https://user-images.githubusercontent.com/48202736/107138033-1b888300-6955-11eb-817c-1ac953ad3c77.png)
{: style="width: 70%;" class="center"}
*Fig. 11. 도넛 모양의 Non-linearly seperable 데이터들*

물론 여기서 더 복잡하게 뿌려져 있는 데이터들도 뉴런들을 추가해서(표현력을 늘림) 더 정교하게 분류를 할 수 있습니다. 

<img width="1180" alt="cmu10" src="https://user-images.githubusercontent.com/48202736/107136296-2fc48400-6945-11eb-8796-72e5fb7a348d.png">
{: style="width: 100%;" class="center"}
*Fig. 12. 조금 더 복잡한 오리모양?의 Non-linearly seperable 데이터들*

이를 분류하기 위해서는 1층 가지고는 조금 부족해보이네요. 아래의 그림을 보면서 설명하겠습니다.

<img width="565" alt="cmu12" src="https://user-images.githubusercontent.com/48202736/107136298-305d1a80-6945-11eb-990b-7698adc0495c.png">
{: style="width: 100%;" class="center"}
*Fig. 13. 2-layer Neural Network*

위의 그림을 보면 출력 단계의 엄청 복잡한 분류경계면을 만들어내기 위해서, 바로 직전의 두 뉴런이 정오각형과 늘어진 오각형의 분류경계를 만들어내죠.
이 5각형 분류경계들은 다시 생각해보면 또 그 밑의 5개의 decision boundary들을 잘 합친게 됩니다.
즉 이런식으로 다 표현할 수 있다는거죠.

- <mark style='background-color: #fff5b1'> Another Perspective : Linear Transformation </mark>

방금 까지 Neural Network의 각각의 파라메터를 곱하는 행위와 hidden neuron이 의미하는것이 뭔지를 살펴봤는데, 
이번에는 같은 걸 조금 다른 관점으로 봐보도록 하겠습니다.

선형대수학에서 데이터(벡터죠)와 매트릭스(네트워크의 파라메터)를 곱하는 행위는 뭘까요? 

네 그렇습니다. 선형 변환하는거죠. 데이터를 다른 차원으로 mapping 해주는 겁니다.


MNIST라는 고이다못해 화석이 되어버린 이미지 분류계의 유명한 손글씨 데이터를 생각해보겠습니다.

<img width="1287" alt="mnist" src="https://user-images.githubusercontent.com/48202736/107138457-dfa2ed00-6957-11eb-8d8d-65f02862e689.png">
{: style="width: 100%;" class="center"}
*Fig. 14. MNIST dataset, 출처 : [link](https://www.mdpi.com/2076-3417/9/15/3169)*

이미지와 다음의 네트워크의 파라메터를 곱하면 hidden neuron은 어떻게 될까요

<center>$$ X \in \mathbb{R}^{1 \times 784} $$</center>
<center>$$ W \in \mathbb{R}^{784 \times 2} $$</center>

네 그렇습니다. 2차원의 데이터가 되어버립니다.

<center>$$ X W^T \in \mathbb{R}^{1 \times 2} $$</center>

![mnist2](https://user-images.githubusercontent.com/48202736/107138699-38bf5080-6959-11eb-80b5-00b3364f65f5.png)
{: style="width: 100%;" class="center"}
*Fig. 15. 2차원 좌표계의 점이 되어버린 고차원의 이미지 데이터*

이게 무슨 의미를 가질까요? 

다시 XOR문제를 생각해보도록 하겠습니다. 

<img width="581" alt="ucl_deep3" src="https://user-images.githubusercontent.com/48202736/107136272-1de2e100-6945-11eb-8495-fe35088e7fb5.png">
{: style="width: 50%;" class="center"}
*Fig. 16. Recap. XOR problem, 이미지 출처 : [UCLxDeepmind lecutre](https://deepmind.com/learning-resources/deep-learning-lecture-series-2020)*

편의상 색을 바꿔서 표현해보겠습니다.
 
<img width="574" alt="ucl_deep4" src="https://user-images.githubusercontent.com/48202736/107136274-1de2e100-6945-11eb-8b5e-b403b71a96c1.png">
{: style="width: 50%;" class="center"}
*Fig. 17. Recap. XOR problem*

아까의 네트워크는 2차원 데이터를 2차원으로 매핑해주는 경우였죠.

![xor_mine2](https://user-images.githubusercontent.com/48202736/107137797-5b4e6b00-6953-11eb-9d10-376e1998c185.png)
{: style="width: 70%;" class="center"}
*Fig. 18. Recap. Neural Network for XOR problem*

아까는 각각의 뉴런 하나가 결정경계 하나를 만들어내고 이 둘을 합치면 새로운 더 복잡한 결정경계가 만들어진다는 접근을 했는데

이번에는 매트릭스를 곱하는 행위 자체가 데이터를 선형변환해서 다른 차원으로 매핑해준다고 생각해보도록 하겠습니다.

그 결과는 아래와 같습니다.

<img width="1185" alt="ucl_deep6" src="https://user-images.githubusercontent.com/48202736/107136276-1e7b7780-6945-11eb-9e3d-c3bbadc92c06.png">
{: style="width: 100%;" class="center"}

<img width="1197" alt="ucl_deep7" src="https://user-images.githubusercontent.com/48202736/107136277-1f140e00-6945-11eb-91f5-dbb6544d54a1.png">
{: style="width: 100%;" class="center"}
*Fig. 19,20. 새롭게 2차원상에 (2차원 ->2차원) 뿌려진 데이터*

우리는 이렇게 새로 매핑된 데이터에 대해서 아래처럼 결정 경계 하나만 그려주면 분류를 할 수 있게 됩니다.

<img width="750" alt="ucl_deep8" src="https://user-images.githubusercontent.com/48202736/107136278-1faca480-6945-11eb-9368-8c88dba4fbc1.png">
{: style="width: 100%;" class="center"}

<img width="747" alt="ucl_deep9" src="https://user-images.githubusercontent.com/48202736/107136279-20453b00-6945-11eb-962a-d77a7d552dee.png">
{: style="width: 100%;" class="center"}
*Fig. 21,22. Neural Network Classifier for XOR problem*
(그림이 근데 조금 이상합니다... 선을 잘못그은거 같네요. 그림을 가져온 UCLxDeepmind 강의에서도 lecturer가 그림이 이상하다고 양해 바란다고 했으니 선은 무시해주시길 바랍니다.)

감이 오셨나요?

조금 더 복잡한 문제를 생각해보도록 하겠습니다.

아래의 데이터는 어떻게 분류해야할까요? 위에서 말했던 것처럼 여러개의 결정 경계를 그려서 분류하는 것을 생각해도 되지만 
이번에는 마찬가지로 데이터를 다른 차원으로(공간으로) transformation 하는 경우에 대해 생각해봅시다.

![harvard2](https://user-images.githubusercontent.com/48202736/107136316-41a62700-6945-11eb-80ad-d6c5a5b45767.png)
{: style="width: 100%;" class="center"}
*Fig. 23. 조금 더 복잡한 나선 형태의 데이터들, 이미지 출처 : [Harvard-IACS](https://github.com/Harvard-IACS/2020-ComputeFest)*

우리는 마찬가지로 주어진 데이터를 행렬 곱을 통해 적당히 2차원에 매핑할 수 있습니다. 

![harvard3](https://user-images.githubusercontent.com/48202736/107136317-436fea80-6945-11eb-8d2d-1a5a06ca4005.png)
{: style="width: 100%;" class="center"}
*Fig. 24. transformation된 데이터들*

이제 여기서 결정 경계 하나를 그려 분류하기란 정말 쉽겠죠?

![harvard4](https://user-images.githubusercontent.com/48202736/107136318-436fea80-6945-11eb-868c-ef62ee3dedc2.png)
{: style="width: 100%;" class="center"}
*Fig. 25. Neural Network Classifier for more complex data points*

하나 더 예시를 생각해보겠습니다.

아까의 원형 모양의 데이터가 기억나시나요? 이것도 마찬가지입니다. 3차원으로 매핑시켜보면 분류하기란 매우 쉬워집니다.

![non-linear](https://user-images.githubusercontent.com/48202736/107138033-1b888300-6955-11eb-817c-1ac953ad3c77.png)
{: style="width: 70%;" class="center"}
*Fig. 26. Recap. 도넛 모양의 Non-linearly seperable 데이터들*

![non-linear2](https://user-images.githubusercontent.com/48202736/107138037-1f1c0a00-6955-11eb-860a-aef22f99c37b.png)
{: style="width: 80%;" class="center"}
*Fig. 27. 3차원에 매핑된 도넛 모양의 데이터들*

마지막으로 예시를 들어보겠습니다. 

아까의 784차원 손글씨 이미지들을 이용해 이진 분류가 아닌 다중 분류를 해볼까요?
한마디로 숫자 0,1,2,...,9를 분류하는 분류기를 만드는겁니다.

<img width="1287" alt="mnist" src="https://user-images.githubusercontent.com/48202736/107138457-dfa2ed00-6957-11eb-8d8d-65f02862e689.png">
*Fig. 28. Recap. MNIST dataset*

![3b1b_mnist](https://user-images.githubusercontent.com/48202736/107139919-00704000-6962-11eb-8913-ab16f9297f4c.gif)
*Fig. 29. Nerual Network Classifier for MNIST*

우리가 이해한 바 대로 뉴럴 네트워크는 분류하고자 하는 데이터들을 다른 차원으로 매핑하고(학습을 통해 점점 더 잘 매핑하겠죠) 마지막에 이를 분류하는 경계선 하나만 그어주면 됐죠?
물론 그건 이진분류의 얘기고 다중분류니 경계선을 여러개 그어줘야 하는 차이가 있긴 합니다.

이제 우리의 직관대로 분류 하기 직전의 레이어의 데이터가 결정경계선 10개 그어주면 될 정도로 잘 매핑됐는지 볼까요?

<img width="525" alt="embedding_projector" src="https://user-images.githubusercontent.com/48202736/107136320-47037180-6945-11eb-9d94-dfa4c8ed97e2.png">
{: style="width: 80%;" class="center"}
*Fig. 30. MNIST 데이터의 embedding space, 사실 더욱 고차원인 데이터를 3차원에 매핑해서 보여준 것 뿐이고 원래는 더 고차원임, 출처 : [link](http://projector.tensorflow.org/)*

데이터가 잘 매핑된걸 볼 수 있습니다.

우리는 이처럼 데이터가 새롭게 뿌려진 공간을 ```embedding space``` 혹은 ```latent space```(latent는 조금 아닐 수도 있을 것 같네요)라고 하고,
이렇게 풀고자 하는 문제를 위해 데이터를 잘 매핑하는 것을 학습 것을 ```Representation Learning``` 이라고 합니다.

- <mark style='background-color: #dcffe4'> Matrix Multiplication = Correlation </mark>

마지막으로 우리가 데이터 벡터와 파라메터 간 매트릭스 곱이 어떤 알려지지 않은 embeddig space로 데이터를 데려다 주는 건 알겠으나, 이게 무슨 의미인지가 궁금합니다.

<img width="1226" alt="cmu8" src="https://user-images.githubusercontent.com/48202736/107136292-2e935700-6945-11eb-9867-fd8a74fb56a2.png">
*Fig. 31. 매트릭스 곱은 곧 입력값과 해당 뉴런과의 상관관계를 나타내는 것이다.*

사실 매트릭스 곱을 한다는건 위와 같이 입력값과의 상관 관계(correlation)를 나타내는 것으로, 유사할수록 높은 값을 return 하게 됩니다.

우리가 숫자를 분류하는 경우를 생각해보면 각각의 hidden neuron이 의미하는 것들은 잘 학습이 됐다면 결국 그 하나하나가 숫자 0,1,2, ...,9와의 correlation을 따지는 것이 되는거죠

예를들어 우리가 네트워크를 통해 10차원으로 데이터를 매핑해버렸다고 하면(마지막에 분류기에 들어가기 전) 각각의 축의 값들이 의미하는바가 곧 
1과 가까운 정도, 2와 가까운 정도, 3과 가까운 정도... 가 되는 거라는 겁니다.  

![3b1b_mnist2](https://user-images.githubusercontent.com/48202736/107139923-06662100-6962-11eb-8643-533ebf57ee50.gif)
*Fig. 32. 숫자 9를 구성하는 요소와의 상관관계를 파악하는 neuron들*

<br>

이해를 돕기 위해 예시를 하나 더 들어보겠습니다.
만약 우리가 다중분류가 아니라 단순히 숫자냐 숫자가 아니냐로 분류하는 경우를 생각해보면 마지막 레이어의 뉴런들이 각각 의미하는 바는 아래와 같을 겁니다. 

<img width="1018" alt="cmu7" src="https://user-images.githubusercontent.com/48202736/107136291-2e935700-6945-11eb-9b7c-7879510682bd.png">
*Fig. 33. 숫자를 나타내는 요소들이 있는지 검사하는 의미를 지니고 있는 hidden neuron들*


- <mark style='background-color: #fff5b1'> Convolutional Neural Network (CNN) </mark>

우리가 잘 아는 CNN은 일반적인 뉴럴 네트워크와는 조금 다른, 그러니까 Image Processing에서 사용되던 특징을 추출하는 filter를 학습가능하게(learnable) 만들어 
데이터를 통해 필터의 파라메터를 업데이트 하는 방식으로 학습해, 이미지 분류 성능을 비약적으로 발전시킨 네트워크입니다.

![cnn0](https://user-images.githubusercontent.com/48202736/107139926-07974e00-6962-11eb-804c-c69e91ceba39.gif)
{: style="width: 70%;" class="center"}
*Fig. 34. 이 필터 연산도 사실상 correlation 계산을 하는것과 같다. 신호처리의 convolution과는 거리가 있는데 이름이 왜 이렇게 지어졌는지, 들었던 것 같은데 기억이 잘...*

여기서 각각의 필터들이 무엇을 배우는지는 아래와 같이 잘 알려져있습니다.

![cnn1](https://user-images.githubusercontent.com/48202736/107139927-08c87b00-6962-11eb-96e3-501f49efeffd.jpeg)
*Fig. 35. What Kernel of CNN learn, 출처 : [cs231n](http://cs231n.stanford.edu/)*

이번에 CNN을 다루지는 않을거지만, 이것을 언급한 이유는 CNN도 결국 이 글의 내용인 NN의 각 neruon들이 의미하는 바와 크게 다르지 않은 것이라는 겁니다. 

- <mark style='background-color: #fff5b1'> References </mark>

1. [Colah's Blog : Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

2. [Hugo Larochelle's slides](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)

3. [Pascal Vincent's slides](http://www.iro.umontreal.ca/~vincentp/publications.html)

4. [CMU Deeplearnng lecture slides](http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/)
