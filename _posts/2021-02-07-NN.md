---
title: Neural Networks (NN)
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Neural Networks (NN) </mark>

뉴럴 네트워크(Neural Network, NN)는 대체 무엇일까요?
아마 대부분 딥러닝으로 기계학습을 접하신 분들은 전통적인 머신러닝(MLE, MAP, Bayesian, SVM, EM Alogrithm 등등) 보다 '뉴럴 네트워크'라는 말을 더 먼저 들어보셨을 것 같습니다.

(물론 저도 그랬습니다ㅎ)

뉴럴네트워크는 아래같이 생겼죠? 다들 아는 내용이실테니 오늘은 간단히 수식 전개를 하고 학습하는 방법(Loss function 정의, optimization 등)에 대한 얘기는 조금만 하고, 
뉴럴 네트워크의 역사? 등에 대해서는 생략하고 조금 딴길로 새보도록 하겠습니다.

- <mark style='background-color: #fff5b1'> Neural Networks and Logistic Regression for Classification </mark>

뉴럴 네트워크는 보통 아래처럼 생겼습니다.

<img width="642" alt="hugo1" src="https://user-images.githubusercontent.com/48202736/107136261-13c0e280-6945-11eb-8a97-429b8e770da3.png">
*Fig. 1. 일반적인 Neural Network의 그림 from Hugo Larochelle's lecture slide*

우리가 너무 잘 아는 내용이죠. (사실 뭐 그림 넣기도 민망한게, 2010년대 중반까지 대세였던 CNN, RNN 네트워크들, 그리고 최근에 이를 대체하는 듯한 퍼포먼스를 보여주는 Transformer나 ViT(Transformer for Image)같은 최신 기법들에 비하면 아주 빈약한 그림입니다...)

위의 그림에서는 2개의 히든레이어와 그것을 구성하는 파라메터들, 그리고 non-linearity를 위한 activation function이 전부군요.

조금 더 간단한 네트워크를 생각해보겠습니다.

![hugo_00](https://user-images.githubusercontent.com/48202736/107136579-05c09100-6948-11eb-95c2-cd13dc1e19b7.png)
*Fig. 2. 더욱 간단한 NN*

입력값과 파라메터를 곱한다음에 그냥 sigmoid 함수로 계산된 값들을 0~1사이값으로 매핑해주는게 전부이군요.

어디서 본거같지 않나요?

네 맞습니다. 그냥 Logistic Regression 입니다.

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} \lambda^{y_i}(1-\lambda)^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(1-\frac{1}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ L = \sum_{i=1}^{I} y_i log[\frac{1}{1+exp[-\phi^T x_i]}] + \sum_{i=1}^{I}(1-y_i)log[\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]}] $$</center>

위의 수식을 최적화 하면 우리는 가지고 있는 학습 데이터를 잘 분류(Classification) 해주는 Decision Boundary를 하나 구할 수 있다는걸 이제 초등학생들도 알거같습니다.

<img width="1003" alt="hugo2" src="https://user-images.githubusercontent.com/48202736/107136264-17ed0000-6945-11eb-8239-847bbaa1624a.png">
*Fig. 3. 2차원 데이터의 Logistic Regression의 결과로 얻어지는 Decision Boundary, 사실*

- <mark style='background-color: #fff5b1'> XOR Problem </mark>

이제 어떤 분류 문제 예시를 생각해볼까요?

<img width="818" alt="hugo6" src="https://user-images.githubusercontent.com/48202736/107136269-1a4f5a00-6945-11eb-8197-8362743fb5bf.png">
*Fig. 4. XOR 문제, 이는 단순히 결정경계선 하나로 풀 수 없는 문제라고 알려져있다.*

위의 문제는 어떻게 결정 경계선을 그리면 될까요 ??? 

그림에도 나와있듯이 이는 단순히 하나의 decision boundary로는 분류할 수 없는 문제가 됩니다. (단순 logistic regression 안됨)

<img width="1095" alt="xor1" src="https://user-images.githubusercontent.com/48202736/107137353-19bbc100-694f-11eb-8d08-4b7251062b5e.png">
*Fig. 5. AND gate, OR gate 문제,이미지 출처 : [link](http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf)*

(AND, OR문제는 쉽게 하나의 결정 경계선으로도 풀 수 있음이 그림에 잘 나타나 있습니다.)

사실 이 문제는 굉장히 잘 알려져있는 문제 (XOR 문제)로 딥러닝, 뉴럴네트워크에 대해 배우게 되면 맨 처음 배우는 문제라서 다들 아실겁니다.

```
'아 그렇구나...!, 이래서 NN 해야하는구나 ...'
```

다시 문제로 돌아가서 이를 복습해보자면, XOR 논리 talbe은 아래와 같습니다.

<img width="818" alt="hugo6" src="https://user-images.githubusercontent.com/48202736/107136269-1a4f5a00-6945-11eb-8197-8362743fb5bf.png">

|--|--|--|
|x1|x2|y|
|0|0|0|
|1|0|1|
|0|1|1|
|1|1|0|



<img width="1004" alt="hugo3" src="https://user-images.githubusercontent.com/48202736/107136265-191e2d00-6945-11eb-8825-45c1ab75bb26.png">
<img width="1091" alt="hugo4" src="https://user-images.githubusercontent.com/48202736/107136266-19b6c380-6945-11eb-8529-91583e70f1ed.png">
<img width="1093" alt="hugo5" src="https://user-images.githubusercontent.com/48202736/107136267-19b6c380-6945-11eb-8683-1388cd1fed25.png">


![xor2](https://user-images.githubusercontent.com/48202736/107137355-1cb6b180-694f-11eb-9ac7-ba25e9a7e43f.png)

- <mark style='background-color: #fff5b1'> Non-Linearity </mark>

- <mark style='background-color: #fff5b1'> Logistic Regression VS Neural Networks </mark>

- <mark style='background-color: #fff5b1'> What is Representation? </mark>

1. [Prince, Simon JD. Computer vision: models, learning, and inference. Cambridge University Press, 2012.](http://www.computervisionmodels.com/)

2. [Colah's Blog : Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

3. [Hugo Larochelle's slides](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)

4. [Pascal Vincent's slides](http://www.iro.umontreal.ca/~vincentp/publications.html)


dcffe4
