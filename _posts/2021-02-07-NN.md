---
title: Neural Networks (NN)
categories: MachineLearning
tag: [MachineLearning,ML]

toc: true
toc_sticky: true
---

- <mark style='background-color: #fff5b1'> Neural Networks (NN) </mark>

뉴럴 네트워크(Neural Network, NN)는 대체 무엇일까요?
아마 대부분 딥러닝으로 기계학습을 접하신 분들은 전통적인 머신러닝(MLE, MAP, Bayesian, SVM, EM Alogrithm 등등) 보다 '뉴럴 네트워크'라는 말을 더 먼저 들어보셨을 것 같습니다.

(물론 저도 그랬습니다ㅎ)

뉴럴네트워크는 아래같이 생겼죠? 다들 아는 내용이실테니 오늘은 간단히 수식 전개를 하고 학습하는 방법(Loss function 정의, optimization 등)에 대한 얘기는 조금만 하고, 
뉴럴 네트워크의 역사? 등에 대해서는 생략하고 조금 딴길로 새보도록 하겠습니다.


뉴럴 네트워크는 보통 아래처럼 생겼습니다.

<img width="642" alt="hugo1" src="https://user-images.githubusercontent.com/48202736/107136261-13c0e280-6945-11eb-8a97-429b8e770da3.png">
*Fig. 1. 일반적인 Neural Network의 그림 from Hugo Larochelle's lecture slide*

우리가 너무 잘 아는 내용이죠. (사실 뭐 그림 넣기도 민망한게, 2010년대 중반까지 대세였던 CNN, RNN 네트워크들, 그리고 최근에 이를 대체하는 듯한 퍼포먼스를 보여주는 Transformer나 ViT(Transformer for Image)같은 최신 기법들에 비하면 아주 빈약한 그림입니다...)

위의 그림에서는 2개의 히든레이어와 그것을 구성하는 파라메터들, 그리고 non-linearity를 위한 activation function이 전부군요.

조금 더 간단한 네트워크를 생각해보겠습니다.

![hugo_00](https://user-images.githubusercontent.com/48202736/107136579-05c09100-6948-11eb-95c2-cd13dc1e19b7.png)
*Fig. 2. 더욱 간단한 NN*

입력값과 파라메터를 곱한다음에 그냥 sigmoid 함수로 계산된 값들을 0~1사이값으로 매핑해주는게 전부이군요.

어디서 본거같지 않나요?

네 맞습니다. 그냥 Logistic Regression 입니다.

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} \lambda^{y_i}(1-\lambda)^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(1-\frac{1}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ Pr(y|X,\phi) = \prod_{i=1}^{I} (\frac{1}{1+exp[-\phi^T x_i]})^{y_i}(\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]})^{1-y_i} $$</center>

<center>$$ L = \sum_{i=1}^{I} y_i log[\frac{1}{1+exp[-\phi^T x_i]}] + \sum_{i=1}^{I}(1-y_i)log[\frac{exp[-\phi^T x_i]}{1+exp[-\phi^T x_i]}] $$</center>

위의 수식을 최적화 하면 우리는 가지고 있는 학습 데이터를 잘 분류(Classification) 해주는 Decision Boundary를 하나 구할 수 있다는걸 이제 초등학생들도 알거같습니다.

<img width="1003" alt="hugo2" src="https://user-images.githubusercontent.com/48202736/107136264-17ed0000-6945-11eb-8239-847bbaa1624a.png">


<img width="1004" alt="hugo3" src="https://user-images.githubusercontent.com/48202736/107136265-191e2d00-6945-11eb-8825-45c1ab75bb26.png">
<img width="1091" alt="hugo4" src="https://user-images.githubusercontent.com/48202736/107136266-19b6c380-6945-11eb-8529-91583e70f1ed.png">
<img width="1093" alt="hugo5" src="https://user-images.githubusercontent.com/48202736/107136267-19b6c380-6945-11eb-8683-1388cd1fed25.png">
<img width="818" alt="hugo6" src="https://user-images.githubusercontent.com/48202736/107136269-1a4f5a00-6945-11eb-8197-8362743fb5bf.png">

- <mark style='background-color: #fff5b1'> Non-Linearity </mark>

- <mark style='background-color: #fff5b1'> Logistic Regression VS Neural Networks </mark>

- <mark style='background-color: #fff5b1'> What is Representation? </mark>

1. [Prince, Simon JD. Computer vision: models, learning, and inference. Cambridge University Press, 2012.](http://www.computervisionmodels.com/)

2. [Colah's Blog : Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

3. [Hugo Larochelle's slides](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)

4. [Pascal Vincent's slides](http://www.iro.umontreal.ca/~vincentp/publications.html)


dcffe4
