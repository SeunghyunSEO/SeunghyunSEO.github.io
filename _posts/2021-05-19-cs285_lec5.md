---
title: (미완) Lecture 5 - Policy Gradients

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 5의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=17)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


이번 챕터에서 배울 내용은 "정책 경사 알고리즘 (Policy Gradient Algorithm)" 입니다. 정책 경사 알고리즘은 수 많은 강화학습 알고리즘들 중에서도 가장 단순한 방법론이고, 강화 학습의 Objective Function 자체를, 정책의 파라메터 $$\theta$$에 대해서 직접 미분해 업데이트하는 방법론 입니다. 

![slide1](/assets/images/CS285/lec-5/slide1.png)
*Slide. 1.*





## <mark style='background-color: #fff5b1'> Recap </mark>

Recap이기 때문에 이전부터 강의를 잘 따라오신 분들은 가볍게 듣고 넘기셔도 될 것 같습니다.

![slide2](/assets/images/CS285/lec-5/slide2.png)
*Slide. 2.*

Policy는 입출력을 서로 연결해주는 Mapping Function이기 때문에, 네트워크의 파라메터가 곧 정책의 파라메터가 됩니다.

일반적인 딥러닝과 다를 바 없이 입력을 given으로 출력을 만들어 내는 것 처럼, 강화학습에서는 상태 $$s$$를 givne, 행동 $$a$$의 분포를 출력합니다. 
분포 또한 딥러닝과 같이 연속적 (일반적으로 ML에선 회귀) 이거나 이산적 (분류) 일 수 있죠.


그리고 그림에서 알 수 있듯, Mapping Function을 통해 어떤 행동 $$a$$를 산출해 내고, $$s,a$$를 given으로 다음 상태를 예측하게 되는데,
이 상태가 어떻게 되는지를 내포하고 있는 Transition Operator(Probability Matrix), $$T$$는 우리가 처음부터 알고 있을 수도 아닐 수도 있습니다.
이 때 이 Operator를 다른 Network를 만들어 예측하면 Model을 만드는 것이 됩니다. (Model-based RL)


궤적 (Trajectory)이란 한 에피소드 내의 일련의 상태,행동들 $$s_1,a_1,\cdots,s_T,a_T$$ 의 모음이며, Trajectory Distribution은 Chain Rule에 따라서  *Slide. 2.*의 중간에 있는 수식 처럼 나타낼 수 있습니다 (Initial State Distribution와 Transition Operator 그리고 Policy의 곱). Trajecctory의 수식에는 $$\pi_{\theta}$$라는 항이 있는데, 이 의미는 "현재 내가 가지고 있는 정책을 따라서, $$s_1,a_1,\cdots,s_T,a_T$$ 을 샘플링 한 것이 Trajectory다" 라는 것 입니다.


한편, Model-based RL과 다르게 Transition Probability를 모르는 상태에서 샘플링을 통해 학습을 진행하는 것을 `Model-free RL`라고 하며,


마지막으로, 우리가 강화학습에서 원하는 것은 어떠한 Trajectory Distribution를 따르는 Trajectory하에서의 보상 값들의 합의 기대값 (즉 Trajectory Distribution에서 샘플링한 여러 Trajectory들에 대해서 각각의 $$1~T$$까지의 보상의 합을 더한 것이라고 할 수 있음)을 Objective Function이라고 정의하고
이 보상에 대한 기대값을 최대로 하는 `정책의 파라메터를 찾는 것 (최적화 하는 것)` 이 강화학습의 목표라고 할 수 있었습니다.

$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$


![slide3](/assets/images/CS285/lec-5/slide3.png)
*Slide. 3.*


*Slide. 3.*는 강화학습의 Obejective Function을 State-Action Marginal을 사용해서 표현하면, 에피소드의 끝이 없는 Infinite Horizon Case와 끝이 정해진 Fintie Horizon Case 둘 다에 대해서 Objective Function을 정의할 수 있음을 보여주는 내용을 Recap한 것입니다.






## <mark style='background-color: #fff5b1'> Direct Policy Differentiation </mark>

이제 어떻게 Obejective Function을 최적화 (optimize)했는지에 대해서 조금 디테일 하게 알아보도록 하겠습니다.

![slide4](/assets/images/CS285/lec-5/slide4.png)
*Slide. 4.*

Objective Function은 다시 아래와 같은데,


$$
\theta^{\ast} = argmax_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ]
$$

만약 우리가 $$p(s_1)$$, $$p(s_{t+1} \vert s_t)$$같은 정보를 모른다고 하면 어떻게 $$J(\theta)$$를 추정해야 할까요?

우리는 real-world에 대해서 현재 가지고 있는 Policy를 직접 돌려서 이들을 샘플링 (sampling) 할 수 있을 겁니다.

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_t r(s_t,a_t) ] \approx \frac{1}{N} \sum_i \sum_t r(s_{i,t},a_{i,t})
$$

$$N$$번 Policy를 돌려서 (roll-out) $$N$$개의 Trajectory들을 얻게 된거죠.

이렇게 얻어진 Trajectory들 각각에 대해서 보상의 합을 평균 내어 $$J(\theta)$$로 쓰게 되면서 우리는 Unbiased Estimate를 할 수 있게 됩니다. 
(N이 커질수록 더 정확해집니다.)





![slide5](/assets/images/CS285/lec-5/slide5.png)
*Slide. 5.*

그리고 우리는 단순히 현재 Policy가 좋은지 나쁜지, 그러니까 이 현재의 Policy를 가지고 샘플링한 Trajectory들이 과연 좋은지? (좋은 점수를 내는 것들인지?)를 단순히 평가할 뿐만 아니라, 이를 바탕으로 `Policy를 개선` 시키고 싶은게 목적이기 때문에 미분값을 계산해야 합니다.


위의 Objective Fucntion, 즉 기대값은 아래와 같이 적분식으로도 표현이 가능하고,

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \sum_{t=1}^T r(s_t,a_t) ]
$$

$$
r(\tau) = \sum_{t=1}^T r(s_t,a_t)
$$

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ r(\tau) ]
$$

$$
J(\theta) = \int p_{\theta}(\tau) r(\tau) d\tau
$$

우리는 이를 통해 정책을 업데이트하고 싶기 때문에, 정책의 파라메터 $$\theta$$에 대해서 Objective를 미분합니다.
미분 연산자, $$\bigtriangledown$$는 선형적이기 때문에 이를 적분식 안으로 넣으면 아래와 같이 표현할 수 있는데요,

$$
\bigtriangledown_{\theta} J(\theta) = \int \bigtriangledown_{\theta} p_{\theta} (\tau) r(\tau) d\tau
$$

여기서 우리는

$$
p_{\theta}(\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) = p_{\theta}(\tau) \frac{ \bigtriangledown_{\theta} p_{\theta} (\tau) }{ p_{\theta} (\tau) }
$$

$$
p_{\theta}(\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) = \cancel{p_{\theta}(\tau)} \frac{ \bigtriangledown_{\theta} p_{\theta} (\tau) }{ \cancel{p_{\theta} (\tau)} }
$$

와 같은 항등식을 이용하면, (기억이 안나시는 분은 log의 미분에 대해서 찾아보시면 됩니다.)


$$
\bigtriangledown_{\theta} J(\theta) = \int \bigtriangledown_{\theta} p_{\theta} (\tau) r(\tau) d\tau = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau
$$

가 되고, 마지막으로 기대값의 정의를 이용해서 이를 다시 기대값의 형태로 바꾸면

$$
\bigtriangledown_{\theta} J(\theta) = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

최종적으로 Objective를 Policy의 파라메터 $$\theta$$에 대해 미분한 수식을 얻을 수 있습니다.
(모든 Trajectory Sample들에 대해서 그 Trajectory가 나타날 확률에 "로그"를 취한 값을 미분한 값과, 그 Trajectory를 따랐을 때의 보상값이 곱해져있죠)



하지만 아직 끝난 것이 아닙니다, 바로 $$\bigtriangledown_{\theta} log p_{\theta} (\tau)$$ 텀 때문인데요, 좀 더 수식을 진행해보도록 하겠습니다.

![slide6](/assets/images/CS285/lec-5/slide6.png)
*Slide. 6.*

우리가 여태까지 유도한 수식은 아래와 같고,

$$
J(\theta) = \int p_{\theta}(\tau) r(\tau) d\tau
$$

$$
\theta^{\ast} = argmax_{\theta} J(\theta)
$$

$$
\bigtriangledown_{\theta} J(\theta) = \int p_{\theta} (\tau) \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) d\tau = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

위의 수식에서 우변에 존재하는 $$log p_{\theta}(\tau)$$ 텀을 다루기 위해서,
여기에 우리가 Transition Probability는 모른다고 했으나 이를 이용해서 Trajectory Distribution을 아래처럼 표현해 보도록 하겠습니다.

$$ 
p_{\theta} (\tau) = p_{\theta}(s_1,a_1,\cdots,s_T,a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t \vert s_t) p(s_{t+1} \vert s_t, a_t ) 
$$

여기서 양변에 $$log$$를 취하면 아래와 같은 식이 되는데요,

$$ 
log p_{\theta} (\tau) = log p(s_1) \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + log p(s_{t+1} \vert s_t, a_t ) 
$$


우리가 원하는 것은 $$\bigtriangledown_{\theta} log p_{\theta}(\tau)$$이기 때문에 양변에 미분연산자를 붙혀주면 아래의 식처럼 나타낼 수 있습니다.

$$
\bigtriangledown_{\theta} log p_{\theta}(\tau) = \bigtriangledown_{\theta} [log p(s_1) + \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + log p(s_{t+1} \vert s_t, a_t )]
$$

여기서 우리가 $$\theta$$에 대해서 미분을 했기 때문에, 이와 관련없는 텀들은 전부 없앨 수 있는데요, 

$$
\bigtriangledown_{\theta} log p_{\theta}(\tau) = \bigtriangledown_{\theta} [\cancel{log p(s_1)} + \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) + \cancel{log p(s_{t+1} \vert s_t, a_t )}]
$$

결국 최종적으로 우리는 아래의 $$\bigtriangledown_{\theta} log p_{\theta}(\tau)$$를 대체해서

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ]
$$

아래와 같은 미분 식을 얻어낼 수 있습니다.

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ ( \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) ) r(\tau) ]
$$

$$
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ ( \sum_{t=1}^T log \pi_{\theta}(a_t \vert s_t) ) (\sum_{t=1}^T r(s_t,a_t)) ]
$$


이렇게 함으로써 우리가 얻을 수 있는 의미는 뭘까요? 

우선 Expectation, $$\mathbb{E}$$속의 모든 term들이 알려져 있다는 겁니다. 
당연히 현재 Policy로 접근 (access)할 수 있고, 우리의 샘플들에 대해서 보상 (reward)이 얼마인지도 평가 (evaluate) 가능하니까요.

하지만 여기서 더 중요한 부분이 있는데, 바로 알려지지 않았던 term들인 Initial State Probability $$p(s_1)$$과 Transition Probability, $$p(s_{t+1} \vert s_t,a_t)$$들이 미분을 하는 과정에서 사라졌다는 겁니다. 
즉, 우리가 Transition Probability (혹은 Model) 을 모르고 출발하기도 했지만, 이를 따로 정의해주지 않아도, 그러니까 Model이라는 것을 따로 추정하면서 학습하지 않아도 된다는 것이죠.



Objective는 다시 아래의 *Slide. 7.*에서와 같이 $$\mathbb{E}$$을 $$\sum$$로 대체할 수 있고 (sampling), 이렇게 얻어낸 Objective의 미분체를 통해 경사 하강법 (Gradient Descent) 기법을 통해 정책을 개선시킬 수 있습니다. 

![slide7](/assets/images/CS285/lec-5/slide7.png)
*Slide. 7.*


*Slide. 7.*의 중간 부분에서는 여태까지 전개한 수식을 이전에 살펴봤던 RL 알고리즘의 Anatomy와 각각 대응시켜 표현한 것입니다.
이러한 기본적인 Policy Gradient Algorithm은 `REINFORCE`라고 부르기도 합니다. 
[REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)는 1990년대에 [Williams](https://en.wikipedia.org/wiki/Ronald_J._Williams)가 제안한 첫 번째 정책 경사 알고리즘이며, 
*Slide. 7.*의 하단에 나와있는 세가지 스텝으로 이루어져 있는 기본적인 정책 경사 알고리즘입니다.




하지만 안타깝게도 지금까지 살펴본 기본적인 방법론은 현실에서 잘 작용하지 않는다고 하는데요, 
앞으로 남은 파트에서 정책 경사 알고리즘 실제로 어떤 일을 하는것인지에 대한 직관적인 이해와 더불어 실제로 잘 작동하는 정책 경사 알고리즘들에 대해서 알아본다고 합니다.













## <mark style='background-color: #fff5b1'> Understanding Policy Gradients </mark>

"직관적으로 정책 경사 알고리즘을 하면 어떻게 정책이 학습되는 걸까?"

![slide9](/assets/images/CS285/lec-5/slide9.png)
*Slide. 9.*

우리가 여태까지 수학적으로 정책 경사 알고리즘을 유도해 봤고, 그 결과 아래와 같은 근사 식을 얻어냈습니다.

$$
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ) ( \sum_{t=1}^T r(s_{i,t},a_{i,t}) ) 
$$

여기서 Lecturer Sergey는 $$ \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) $$ 가 하는 일이 뭔지에 대해서 한번 생각해 보자고 하는데요,

그렇게 하기 위해 우선 우리가 가진 정책 $$\pi_{\theta}$$가 이미지를 입력으로 이산적인 행동 (Discrete Action)을 출력한다고 생각해 보도록 하겠습니다.
(당연히 정책은 뉴럴 네트워크 (Neural Network, NN)로 모델링 되어있으니, NN이 가지는 파라메터가 곧 $$\theta$$입니다.





![slide10](/assets/images/CS285/lec-5/slide10.png)
*Slide. 10.*



이미 눈치를 채신 분들도 계시겠지만 수식에서

$$
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ) ( \sum_{t=1}^T r(s_{i,t},a_{i,t}) ) 
$$

reward 부분만 빼면 이는 Log-Likelihood만 남게 되는데요, 

$$
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) )) 
$$

이렇게 되면 Objective는 딥러닝의 지도 학습 (Supervised Learning)에서 일반적으로 사용하는 최대 우도 측정 (Maximum Likelihood Estimation, MLE)나 다름없게 됩니다. 

즉 정답에 해당하는 log probability를 최대화 하는 방향으로 학습하는거죠.


하지만 우리는 Imitation Learning이 아니기 때문에 정답 레이블이 따로 없으며, 뒤에 $$( \sum_{t=1}^T r(s_{i,t},a_{i,t}) )$$가 붙어있죠, 
이 말의 의미는 보상 값 (Reward Value)에 따라서 좋았던 Trajectory의 log probability는 증가시키고, 나빴던 Trajectory에 대해서는 log probability를 줄이는 방향으로 학습을 하겠다는 겁니다.
즉 MLE의 Weighted Sum 버전이라고 생각을 할 수 있겠습니다.


***

```
조금 더 설명을 보태보겠습니다. 
```

우리가 지금 가정한 문제는 이산 확률 분포를 나타내는 (즉 이진 분류 문제를 푸는 것) 경우이므로, 크로스 엔트로피 (Cross-Entropy, CE) Loss를 줄이는 것이며, 수식을 CE와 똑같은 폼으로 나타내기 위해서 *Slide. 10.*의 수식에는 없지만 $$t_{i,t}$$를 추가해줘야 할 것 같습니다.


$$t_{i,t}$$는 당연히 정답을 의미하는 원핫 벡터이며, 예를들어 현재 자동차가 "좌,우" 두 가지 선택지만 고를 수 있다면, 
어떤 상태에서 "좌" 가 정답일때는 $$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$이 되고 "우"가 정답일 때는 $$\begin{bmatrix} 0 \\ 1 \end{bmatrix}$$이 되는 값이 곱해져, 정답에 대해서 잘 맞추지 못한것에 대해서만 패널티를 부과하는게 CE Objective Function에 대한 해석이겠죠. 
(수식에 직접 확률을 대입해 보시면 아시겠지만, $$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$이 곱해져 정답이라는 것이 Activation 됐을 때 그 정답을 출력으로 낼 확률이 적으면 손실함수의 값이 크기 때문에, 이를 줄이는 방향으로 학습하면, 정답을 출력으로 뱉을 확률이 높아지는게 수식이 가지는 의미입니다.)

$$
\bigtriangledown_{\theta}J_{ML}(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} t_{i,t} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) )) 
$$

즉 이를 최적화 한다는 것은 각각의 상태마다 정답이라고 레이블링 된 행동들이 나올 확률을 1에 가깝게 계속해서 높히는 것이 됩니다. 


하지만 `우리는 Imitation Learning을 하는 것이 아니기 때문에 어떤 행동을 해야 한다는 정답 레이블`이 없습니다.
게다가 우리가 원래 가지고 있는 수식은 $$t_{i,t}$$가 없죠.
대신에 어떤 행동을 했을 때 그 행동에 대한 댓가를 나타내는 $$ ( \sum_{t=1}^T r(s_{i,t},a_{i,t})$$를 수식에 곱했죠.


즉 우리는 정답이라고 알려주는 텀 $$t_{i,t}$$이 $$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$ 되어서 정답 행동과 관련 없는것들은 모두 죽이는 것 대신에,
모든 가능한 행동에 대한 각각의 보상 값 $$ r(s_{i,t},a_{i,t})$$을 해당 행동을 할 확률과 곱해서 나타낸다는 겁니다 (i.e. $$r_{i,t} = \begin{bmatrix} 0.7 \\ -0.2 \end{bmatrix}$$). 
$$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$가 곱해지는 것이 아니기 때문에, 정답 행동이 아닌 (정답이랄것도 없죠) 모든 행동에 대해서 보상값들을 다 곱하는 상황에서 Objective가 최종적으로 가장 큰 보상값을 리턴하기 위해서는 가장 큰 보상을 산출하는 행동의 확률을 크게해야 각각의 행동과 보상을 곱해 더했을 때 최종적으로 가장 큰 값을 리턴하니, 가장 큰 보상을 산출하는 행동의 확률을 크게하고, 적거나 혹은 음수의 보상을 리턴하는 행동은 확률을 줄여야 합니다.


결국 강화학습의 Objective를 최대화 하는 것은 곧 현재 정책을 가지고 `여러 행동을 해 보면서 좋았던 행동은 더 자주 나오게 (높은 확률로), 아닌 행동은 덜 나오게 (낮은 확률로)끔 확률을 조정하는 것` 라는 의미를 갖게 되는겁니다.

***




![slide11](/assets/images/CS285/lec-5/slide11.png)
*Slide. 11.*

Policy Gradient Algorithm의 예시로 휴머노이드가 관절을 컨트롤하면서 걸어가게끔 하는 경우에 대해서 얘기를 하는데요,
여기서는 Action Space가 연속적 (continuous) 합니다.


다들 아시다시피 머신러닝에서 연속적인 출력 분포를 가정하는 경우 `가우시안 분포`를 많이 사용하게 되는데요,
출력 분포를 가우시안 분포로 가정하는 경우 MLE를 한다는 것이 결국 `오차 제곱 항 (Mean Squared Error, MSE)` Loss 를 줄이는 것과 동치가 되기 때문에, 
아까 CE Loss에 reward term을 weighted sum한 것 처럼 MSE Loss에 reward term을 weighted sum하는걸로 간단하게 Objective Function을 구성할 수 있을 것입니다.




![slide12](/assets/images/CS285/lec-5/slide12.png)
*Slide. 12.*

*Slide. 12.*는 "What is the policy gradient actually doing?"에 대해서 얘기하는 슬라이드인데요, 앞서 했던 얘기와 별 다를 게 없습니다.
즉 좋았던 행동 (Trajectory라고 할 수 도 있음)은 더 지향하게 하고, 나빴던 행동은 지양하게 하는거죠.  






![slide13](/assets/images/CS285/lec-5/slide13.png)
*Slide. 13.*

*Slide. 13.*는 state가 아니라 observation, $$o$$를 given으로 했을 때의 gradient를 나타내는데요,
state와 observation의 차이는 `Markov Property`를 만족하느냐 아니냐의 차이였죠? 
하지만 gradient를 계산하는 수식을 보시면 Markov Property를 고려하는 수식이 전혀 없기 때문에 $$o,s$$ 어떤 것을 given으로 학습을 하던지 수식이 변하지 않습니다.


(결론은 그냥 똑같단 소립니다.)






![slide14](/assets/images/CS285/lec-5/slide14.png)
*Slide. 14.*

여태까지 알아본 정책 경사 알고리즘이 굉장히 `straightforward` 하지만 몇 가지 문제점이 있는데요,
위의 그림에서 x축은 `trajectory`이며, y축은 `reward`이자 trajectory를 샘플링할 확률입니다.
물론 trajectory가 단순히 1차원일리 없겠지만 우선 편의를 위해 그렇게 표현했다고 합니다.


우리가 3개의 trajectory를 먼저 샘플링 했더니 녹색을 얻었다고 생각해봅시다.

![high_variance1](/assets/images/CS285/lec-5/high_variance1.png){: width="70%"}

정책 경사 알고리즘의 수식과 직관에 따라서 우리는 `negative reward`를 리턴하는 trajectory는 덜 일어나게끔, 그러니까 log-prob을 줄이고, 조금이라도 `positive reward`를 리턴하는 trajectory는 장려하게 됩니다.

![high_variance2](/assets/images/CS285/lec-5/high_variance2.png){: width="70%"}

그 결과 위와 같이 policy가 업데이트 되어 trajectory distibution이 변화했습니다.
하지만 위와 같은 샘플을 얻은 상황에서 MDP의 특성을 이용해 전혀 다른 결과를 내보도록 하겠습니다.

MDP에서는 상수를 더하거나 해도 수식적으로 문제가 되지 않기때문에 위의 수식에서 reward에 적당한 상수를 더하면 아래와 같은 그림을 얻을 수 있는데요,

![high_variance3](/assets/images/CS285/lec-5/high_variance3.png){: width="70%"}

이러한 경우 모든 trajectory가 좋게 평가되어 아래와 같은 결과를 초래합니다.

![high_variance4](/assets/images/CS285/lec-5/high_variance4.png){: width="70%"}

정책 경사 알고리즘은 이렇듯 샘플에 따라서 `high variance` 문제를 가지고 있습니다.
(이러한 variance는 sample 수가 무한대에 가까우면 문제가 안된다고 합니다 (?))

```
이는 예를들어 이렇게 생각해 볼 수도 있는데요,
수학 문제를 푸는 로봇이 점수를 잘 받았냐 못 받았냐를 평가해서 알고리즘을 업데이트 하려고 했을 때, 
10점이나 100점이나 양수이기 때문에 결국 좋은 점수라고 인식해서 문제가 생기는 거죠.

하지만 그 수학 문제의 평균이 50점이라면 10점은 50점 대비 엄청 좋지 않은 점수겠죠..?
```

![slide15](/assets/images/CS285/lec-5/slide15.png)
*Slide. 15.*

*Slide. 15.*는 여태까지의 리뷰이니 넘어가도록 하겠습니다.










## <mark style='background-color: #fff5b1'> Reducing Variance </mark>

그렇다면 `High Variance` 상황에서 Variance를 줄이는 방법은 무엇이 있을까요?

결론부터 얘기하자면 아래와 같습니다.

> 1. Causuality 적용 
> 2. Baseline 적용

이 둘이 무엇인지에 대해서 지금부터 알아보고 이 두가지를 적용해서 실제로 사용할 수 있는 `practical algorithm` 수식을 정의하도록 하겠습니다.

![slide17](/assets/images/CS285/lec-5/slide17.png)
*Slide. 17.*

`Causuality`는 간단합니다. 이는 우리가 존재하는 universe에서 언제나 참인 명제로, 우리가 어떤 시점 $$t$$에서 한 행동이 미래($$t < t'  $$)에 영향을 줄 지언정 과거($$t' < t $$)에는 영향을 줄 수 없다는 겁니다. 
(중요한 점은 이것이 MDP랑 다르다는 겁니다. MDP는 우리가 다루는 프로세스에 따라서 참일수도 거짓일수도 있지만, Causuality는 언제나 참입니다.)

이를 적용하면 수식을 아래와 같이 수정할 수 있습니다.


$$
\begin{aligned}
&
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ) ( \sum_{t=1}^T r(s_{i,t},a_{i,t}) ) 
& \\

&
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ) ( \sum_{t'=1}^T r(s_{i,t'},a_{i,t'}) ) 
& \\

&
\bigtriangledown_{\theta}J(\theta) \approx \sum_{i=1}^N ( \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ) ( \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) ) 
& \\
\end{aligned}
$$

`Causality`가 작용하는 이유는 굉장히 단순한데요, `reward의 총합`이 줄어들었기 때문에 trajectory 곱해지는 값이 줄어들기 때문입니다. 그리고 `작은 값에 대해 기대값을 취하는 것`은 `작은 분산`을 가져오기 때문이죠.


+여기서 앞으로 우리는 $$( \sum_{t'=t}^T r(s_{i,t'},a_{i,t'})$$라는 양을 $$\hat{Q_{i,t}}$$로 따로 정의하고 이를 `reward to go`라고 부를 겁니다. 이는 lecture 4에서 봤던 `큐 함수 (Q-Function)`과 같습니다. 이를 의도적으로 대체한 이유에 대해서 다음에 `Actor-Critic`알고리즘에 대해서 배우며 다시 얘기하겠다고 lecturer는 말합니다. 


그 다음으로 variance를 줄이는 방법은 바로 `Baseline`을 도입하는건데요,

![slide18](/assets/images/CS285/lec-5/slide18.png)
*Slide. 18.*

아래와 같이 간단하게 모든 trajectory가 가지는 보상에 대한 평균인 ($$b = \frac{1}{N} \sum_{i=1}^N r(\gamma)$$)를 기준으로 그보다 낮으면 음수, 높으면 양수를 리턴하게 하면 됩니다. 

$$
\begin{aligned}

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau)
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \bigtriangledown_{\theta} log p_{\theta} (\tau) [ r(\tau) - b ]
& \\
 
&
\text{where } b = \frac{1}{N} \sum_{i=1}^N r(\tau)
& \\
\end{aligned}
$$
 
 
이는 매우 직관적이긴 하나 이렇게 해도 되는가 싶은 의문이 들 수 있습니다.
그래서 Sergey 교수님은 이것이 Variance에만 영향을 주고 여전히 전체 수식에 영향을 주지 않는다는 것을 간단하게 증명합니다 (강의 참조). 


그리고 물론 보상들에 대한 평균값을 baseline으로 쓰는 것이 베스트가 아닐 수도 있으나 실제로 잘 작용한다고 합니다. 
강의에서는 추가적으로 강화학습 알고리즘을 실제로 구현하는 데 있어 잘 사용되지는 않지만 optimal baseline이 무엇인지 알아보는 것이  분산을 이해하는데 좋기 때문에 여기서 더 전개를 합니다.


![slide19](/assets/images/CS285/lec-5/slide19.png)
*Slide. 19.*

분산은 다들 아시다시피 아래와 같습니다.

$$ 
Var[x] = E[x^2] - E[x]^2 
$$

그리고 아까 정의한 gradient 수식은 아래와 같았죠.


$$
\begin{aligned}

&
\bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[ \bigtriangledown_{\theta} log p_{\theta} (\tau) ( r(\tau) - b ) ]
& \\
 
&
\text{where } b = \frac{1}{N} \sum_{i=1}^N r(\tau)
& \\

\end{aligned}
$$

위의 두가지 수식을 이용해서 Variance에 대입하면 아래와 같은데요,

$$
Var = \bigtriangledown_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[ (\bigtriangledown_{\theta} log p_{\theta} (\tau) ( r(\tau) - b ))^2 ] \\ - \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) ( r(\tau) - b ) ]^2
$$

여기서 우변의 두번째 항인 $$ \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) ( r(\tau) - b ) ] $$ 가 원래의 정책 경사 알고리즘 수식 $$ \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [ \bigtriangledown_{\theta} log p_{\theta} (\tau) r(\tau) ] $$ 와 다르지 않다는 걸 알 수 있습니다. 

이는 기대값 내에 상수가 있는건 수식에 영향을 주지 않는다는 것이고, 즉 baseline이 기대값 내에서 `unbaised`하다는 뜻입니다. (baselines are unbaiased in expectation)


이제 여기서 

$$
\bigtriangledown_{\theta} log p_{\theta} (\tau) = g(\tau)
$$

로 바꾸고 Variance 수식을 다시 바꿔쓴 뒤 미분을 취하면


$$
\frac{dVar}{db} = \frac{d}{db} \mathbb{E}[g(\tau)^2 (r(\tau) - b)^2]
$$

가 됩니다.
우리가 원하는 것은 `최적의 b는 무엇인가?`이기 때문에, b에 대해서 미분을 한 뒤 0인 지점을 찾을 것이고, 앞서 첫번째 수식의 우변의 두번째 항을 b에 상관없는 텀으로 바꿨기 때문에 이는 b에 무관한 텀이라 미분했을 때 0이 된겁니다.
어쨌든 이 미분식을 좀 더 전개하면

$$
\begin{aligned}

&
\frac{dVar}{db} = \frac{d}{db} \mathbb{E}[g(\tau)^2 (r(\tau) - b)^2] = \frac{d}{db}( \mathbb{E}[g(\tau)^2 r(\tau)^2] - 2\mathbb{E}[g(\tau)^2 r(\tau) b] + b^2 \mathbb{E}[g(\tau)^2] )
& \\

&
= - 2\mathbb{E}[g(\tau)^2 r(\tau) + 2b \mathbb{E}[g(\tau)^2] = 0
& \\

\end{aligned}
$$

를 얻을 수 있습니다.


즉 이렇게 미분이 0일 때의 솔루션인 `optimal value of b`는 아래와 같습니다.

$$
b = \frac{\mathbb{E}[g(\tau)^2 r(\tau)}{\mathbb{E}[g(\tau)^2]}
$$

여기서 알 수 있는 중요한 사실 중 하나는, optimal value b를 구하는 수식에 gradient 값, $$\bigtriangledown_{\theta} log p_{\theta} (\tau) $$가 들어가 있다는 점입니다. 즉 policy를 구성하느 네트워크의 수많은 파라메터들에 대해서 어떤 파라메터를 최적화 하느냐에 따라서 매 번 baseline이 다르다는 겁니다.


(하지만 실제로는 그냥 평균값을 많이 쓴다고 합니다)


![slide20](/assets/images/CS285/lec-5/slide20.png)
*Slide. 20.*

*Slide. 20.*는 서브섹션에  리뷰페이지 이므로 곱씹어 보시고 넘어가시면 될 것 같습니다.








## <mark style='background-color: #fff5b1'> Off-Policy Policy Gradients </mark>

이 다음으로 알아볼 것은 `Off-Policy` 세팅에서 작동하는 정책 경사 알고리즘인데요, 앞서 우리가 살펴봤던 것은 `On-Policy` 세팅의 알고리즘으로 이 둘의 차이를 이해하고 어떤 개선점이 있었는가를 이해하는 것이 이번 장의 목표입니다.


첫째로 `정책 경사 알고리즘 (Policy Gradient Algorithm)`은 on-policy 알고리즘인 이유는 무엇이고 무슨 문제가 있는걸까요?

![slide22](/assets/images/CS285/lec-5/slide22.png)
*Slide. 22.*

정책 경사 알고리즘은 우리가 현재 가지고 있는 정책 (randomly initialized neural network 겠죠?) 부터 매 iteration을 통해서 정책을 업데이트할 때 마다 `새로운 샘플들`을 뽑아야 합니다. 
이러한 이유 때문에 정책 경사 알고리즘을 `On-policy Algorithm` 이라고 하며, 여기서 매 번 trajectory들을 샘플링 하는 것이 굉장히 골칫덩이가 됩니다.


왜 골칫덩이냐 하면, 뉴럴 네트워크로 모델링한 심층 강화 학습 (Deep RL) 방법론들은 대게 비선형성이 강하기 때문에 경사 상승이나 경사 하강법 등을 통해서 최적화를 진행할 때 정책을 크게 업데이트 할 수 없어서 파라메터 값이 조금씩만 약간씩만 변하는데, 이렇게 별로 변하지도 않은 정책 때문에 매번 샘플링을 다시 해야 한다는 것 자체가 굉장히 알고리즘을 비효율적으로 만들기 때문입니다. (어떻게든 연산량을 줄여야죠)


물론 이렇게 샘플을 만들어내는 것의 연산량이 저렴 (cheap)하면 정책 경사 알고리즘은 최선책이 될 수도 있습니다.
하지만 대부분의 경우 이렇지 않기 때문에 우리는 이 샘플링 하는 부분을 효율저기게 손볼것인데요, 이 때 `Importance Sampling`라는 것을 이용할 것입니다.


(통계학과의 베이지안 통계학 강의를 들을 적에 Importance Sampling를 머신러닝에서는 어떻게 쓰나... 하고 넘겼었는데 여기서 쓰는걸 다 보게되네요...)


`Importance Sampling`은 서로 다른 분포로부터 얻은 샘플들을 어떤 동일한 하나의 분포하에서의 Expectation을 평가하는 일반적인 테크닉입니다. 이는 아래와 같이 수식을 유도할 수 있습니다.

![importance_sampling](/assets/images/CS285/lec-5/importance_sampling.png){: width="70%"}

이 수식에서 approximation이 들어간 것은 하나도 없기 때문에 Important Sampling은 unbiased 수식입니다.


`Importance Sampling`을 적용한 정책 경사 알고리즘의 Key idea는 한마디로 다음과 같으며,

- 매번 업데이트 되는 $$p_{\theta}(\tau)$$에서 샘플링하지 말고 $$\bar{p}(\tau)$$라는 분포에서 샘플링하면 어떨까? (여기서 $$\bar{p}(\tau)$$는 파라메터화 되어있지 않습니다.)

이를 적용한 정책 경사 알고리즘의 수식을 전개하는 것은 아래와 같습니다.

![slide23](/assets/images/CS285/lec-5/slide23.png)
*Slide. 23.*

(수식을 또 따로 적지는 않겠습니다.)


위의 슬라이드에서 최종적으로 얻은 수식에서 알 수 있듯 우리는 $$p(s_1),p(s_{t+1} \vert s_t, a_t)$$를 알 필요가 없습니다.
계속 전개를 진행하겠습니다.



![slide24](/assets/images/CS285/lec-5/slide24.png)
*Slide. 24.*

우리가 원하는 것은 아래의 수식을 계산해서

$$
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[r(\tau)]
$$

이를 최대화 하는 정책을 찾는거죠.

$$
\theta^{\ast} = arg max_{\theta}J(\theta)
$$

여기서 Importance Sampling을 적용한 정책 경사 알고리즘은 다음과 같았습니다.

$$
J(\theta) = \mathbb{E}_{\tau \sim \bar{p_{\theta}}(\tau)}[\frac{ p_{\theta}(\tau) }{ \bat{p_{\theta}}(\tau)} r(\tau)]
$$

여기서 우리가 새로운 파라메터 $$\theta'$$들에 대한 값을 추정할 수 있을까요? 
우선 이 때의 `기대 보상값의 총합 (Objective)` 을 아래와 같이 정의할 수 있습니다. 

$$
\begin{aligned}

& 
J(\theta') = \mathbb{E}_{\tau \sim p_{\theta'}(\tau)}[r(\tau)]
& \\

&
J(\theta') = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\frac{ p_{\theta'}(\tau) }{ p_{\theta}(\tau)} r(\tau)] 
& \\
\end{aligned}
$$

여기서 정말 중요한 점은 새로운 파라메터 $$\theta'$$와 관련있는 term은 우변의 기대값 속 분자 밖에 없다는 겁니다.
즉, 여기에 미분을 적용해도 분자에만 적용된다는 거죠.

$$
\bigtriangledown_{\theta'} J(\theta') = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\frac{ \bigtriangledown_{\theta'} p_{\theta'}(\tau) }{ p_{\theta}(\tau)} r(\tau)] 
$$

여기에 

$$
p_{\theta}(\tau) \bigtriangledown_{\theta} log p_{\theta}(\tau) = \bigtriangledown_{\theta} p_{\theta}(\tau)
$$

라는 간단한 테크닉을 적용하면

$$
\bigtriangledown_{\theta'} J(\theta') = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\frac{ p_{\theta'}(\tau) }{ p_{\theta}(\tau)} \bigtriangledown_{\theta'} log p_{\theta'}(\tau)  r(\tau)] 
$$

위와 같은 식을 얻을 수 있습니다.

만약에 여기서 $$\theta = \theta'$$라는 경우를 생각해보면 (back to the on-policy), 우리는 *Slide. 24.* 하단에 나와있듯 원래의 정책 경사 알고리즘과 같은 수식을 얻을 수 있다는 것을 쉽게 알 수 있습니다.


하지만 우리가 지금 알고싶어하는 `Off-Policy` 세팅에서는 $$\theta != \theta'$$ 이겠죠?

![slide25](/assets/images/CS285/lec-5/slide25.png)
*Slide. 25.*

다시, 미분한 Objective는 아래와 같고,

$$
\bigtriangledown_{\theta'} J(\theta') = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\frac{ p_{\theta'}(\tau) }{ p_{\theta}(\tau)} \bigtriangledown_{\theta'} log p_{\theta'}(\tau)  r(\tau)] 
$$

$$\theta != \theta'$$ 이며, 기대값에 있는 `Importance Ratio` $$\frac{ p_{\theta'}(\tau) }{ p_{\theta}(\tau)}$$ 를 앞서 유도한 것으로 바꾸면 아래의 식을 얻을 수 있습니다.

$$
\bigtriangledown_{\theta'} J(\theta') = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[ (\prod_{t=1}^T \frac{ \pi_{\theta'}(a_t \vert s_t) }{ \pi_{\theta}(a_t \vert s_t) }) (\bigtriangledown_{\theta'} log \pi_{\theta'}(a_t \vert s_t) )  ( \sum_{t=1}^T r(s_t,a_t) ) ] 
$$

즉 모든 time-step $$t$$에 대해서 정책 $$\pi_{\theta}$$와 $$\pi_{\theta'}$$의 비율만큼이 곱해지는 겁니다.





![slide26](/assets/images/CS285/lec-5/slide26.png)
*Slide. 26.*










## <mark style='background-color: #fff5b1'> Implementing Policy Gradients </mark>

![slide28](/assets/images/CS285/lec-5/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-5/slide29.png)
*Slide. 29.*

![slide30](/assets/images/CS285/lec-5/slide30.png)
*Slide. 30.*

![slide31](/assets/images/CS285/lec-5/slide31.png)
*Slide. 31.*

![slide32](/assets/images/CS285/lec-5/slide32.png)
*Slide. 32.*











## <mark style='background-color: #fff5b1'> Advanced Policy Gradients </mark>

![slide34](/assets/images/CS285/lec-5/slide34.png)
*Slide. 34.*

![slide35](/assets/images/CS285/lec-5/slide35.png)
*Slide. 35.*

![slide36](/assets/images/CS285/lec-5/slide36.png)
*Slide. 36.*

![slide37](/assets/images/CS285/lec-5/slide37.png)
*Slide. 37.*

![slide38](/assets/images/CS285/lec-5/slide38.png)
*Slide. 38.*

![slide39](/assets/images/CS285/lec-5/slide39.png)
*Slide. 39.*

![slide40](/assets/images/CS285/lec-5/slide40.png)
*Slide. 40.*












### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)








