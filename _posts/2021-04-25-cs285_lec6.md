---
title: () Lecture 6 - Actor-Critic Algorithms

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 6의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=wr00ef_TY6Q&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=23)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


![slide1](/assets/images/CS285/lec-6/slide1.png)
*Slide. 1.*

Lecture 5 에서는 정책 경사 알고리즘 (Policy Gradient Algorithm, PG)에 대해서 알아봤습니다.
이번에는 PG에 기반한 `Actor-Critic Algorithm`에 대해서 알아보도록 하겠습니다.

미리 살짝 말씀드리자면 Actor-Critic을 발전시킨 `Advantage Actor-Critic, A2C`, 하나의 에이전트가 아닌 여러 에이전트로 부터 학습하는 비동기식 알고리즘인 `Asynchronous Advantage Actor-Critic, A3C` 도 있지만 본 강의에서는 다루지 않는 것 같습니다.



## <mark style='background-color: #fff5b1'> Recap </mark>

강의에서는 간단하게 PG에 대해서 복습을 하고 넘어갑니다. (익숙하신 분은 다음 섹션으로 넘어가시면 됩니다.)

![slide2](/assets/images/CS285/lec-6/slide2.png)
*Slide. 2.*

PG는 `Trajectory`를 여러번 샘플링한 뒤 이를 통해 환경과 상호작용하면서 policy를 `directly 업데이트`하죠.
(수식을 곱씹어 보시면 좋을 것 같습니다. 각 Trajectory마다 매 time-step에서의 정책의 log-probability와 그 시점부터 끝날시점 T까지의 보상의 합을 다 더한 것을 weighted-sum 한 수식입니다.)


지난 강의에서 `보상 (reward to go)`을 의도적으로 $$Q^{\pi} (x_t,u_t)$$ 라고 정의했었던 것도 기억이 나실 겁니다.


그리고 PG의 직관적인 이해는 곧 `많은 보상을 초래할 것 같은 행동은 더욱 잘 일어나게끔, 그 반대는 덜 발생하게끔` 하는 것이었습니다.

![slide3](/assets/images/CS285/lec-6/slide3.png)
*Slide. 3.*

그러나 이러한 굉장히 straight forward한 알고리즘인 PG에도 당연히 문제점이 있었는데요,

그것은 바로 어떤 시점에서의 기대 보상값을 계산할 때, MDP에는 랜덤성 (randomness)가 크기 때문에, 굉장히 다양한 경우의 수가 존재한다는 겁니다. 그렇기 때문에 PG를 수행할 때 single sample만을 사용하는 경우 굉장히 알고리즘의 `분산이 크게 (high variance)` 됩니다.

그러므로 우리는 어떤 시점 $$t'$$부터 $$T$$까지 하나의 샘플 (경로)만 고려하지 않고 여러번 샘플링을 해서 알고리즘을 최적화 한다면 이 `분산을 줄일 수 있게 (lower variance)` 됐었죠. 

$$ 
& \hat{Q_i,t} \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) & \\
& \hat{Q_i,t} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{i,t'},a_{i,t'}) \vert s_t,a_t ] & \\
$$ 

![slide4](/assets/images/CS285/lec-6/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-6/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-6/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-6/slide7.png)
*Slide. 7.*

![slide8](/assets/images/CS285/lec-6/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-6/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-6/slide10.png)
*Slide. 10.*



## <mark style='background-color: #fff5b1'> From Evaluation to Actor Critic </mark>

![slide12](/assets/images/CS285/lec-6/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-6/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-6/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-6/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-6/slide16.png)
*Slide. 16.*



## <mark style='background-color: #fff5b1'> Actor-Critic Design Decisions </mark>

![slide18](/assets/images/CS285/lec-6/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-6/slide19.png)
*Slide. 19.*



## <mark style='background-color: #fff5b1'> Critics as Baselines </mark>

![slide21](/assets/images/CS285/lec-6/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-6/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-6/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-6/slide24.png)
*Slide. 24.*



## <mark style='background-color: #fff5b1'> Review, Examples, and Additional Readings </mark>

![slide26](/assets/images/CS285/lec-6/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-6/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-6/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-6/slide29.png)
*Slide. 29.*





### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [강화학습 알아보기(4) - Actor-Critic, A2C, A3C from 김환희님](https://greentec.github.io/reinforcement-learning-fourth/)













