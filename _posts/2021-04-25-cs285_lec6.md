---
title: () Lecture 6 - Actor-Critic Algorithms

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 6의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=wr00ef_TY6Q&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=23)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


![slide1](/assets/images/CS285/lec-6/slide1.png)
*Slide. 1.*

Lecture 5 에서는 정책 경사 알고리즘 (Policy Gradient Algorithm, PG)에 대해서 알아봤습니다.
이번에는 PG에 기반한 `Actor-Critic Algorithm`에 대해서 알아보도록 할 것입니다.


시작하기에 앞서 아래의 `Deep Reinforcement Learning Taxonomy`을 보시면 도움이 될 것 같습니다.

![rl_taxonomy_intellabs_for_lec5](/assets/images/CS285/lec-5/rl_taxonomy_intellabs_for_lec5.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec5](/assets/images/CS285/lec-5/rl_taxonomy_openai_for_lec5.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))


Taxonomy에 보시면 Actor-Critic을 발전시킨 `Advantage Actor-Critic, A2C`, 하나의 에이전트가 아닌 여러 에이전트로 부터 학습하는 비동기식 알고리즘인 `Asynchronous Advantage Actor-Critic, A3C` 등도 있지만 본 강의에서는 다루지 않는 것 같습니다.




## <mark style='background-color: #fff5b1'> Recap </mark>

지난 Lecture 5에서 우리는 `정책 경사 알고리즘 (Policy Gradient Algorithm, PG)`에 대해 배웠습니다.

![slide2](/assets/images/CS285/lec-6/slide2.png)
*Slide. 2.*

PG는 `Trajectory`를 여러번 샘플링한 뒤 이를 통해 환경과 상호작용하면서 policy를 `directly 업데이트`하죠.
(각 Trajectory마다 매 time-step에서의 정책의 log-probability와 그 시점부터 끝날시점 T까지의 보상의 합을 다 더한 것을 weighted-sum 한 수식이었죠.)


지난 강의에서 `보상 (reward to go)`을 의도적으로 $$Q^{\pi} (x_t,u_t)$$ 라고 정의했었던 것도 기억이 나실 겁니다.


그리고 PG의 직관적인 이해는 곧 `많은 보상을 초래할 것 같은 행동은 더욱 잘 일어나게끔, 그 반대는 덜 발생하게끔` 하는 것이었습니다.

![slide3](/assets/images/CS285/lec-6/slide3.png)
*Slide. 3.*

$$Q^{\pi} (x_t,u_t)$$에 대해서 조금 더 얘기해보자면 이는 `어떤 상태에서 어떤 행동을 했을 때 이 행동과 policy를 따라서 끝까지 가 봤을때 얻을 보상`이라고 할 수 있습니다.

이 보상함수를 평가할 수 있는 더 좋은 방법이 있을까요?

![additive1](/assets/images/CS285/lec-6/additive1.png){: width="70%"}

그것은 바로 어떤 시점 $$t'$$ 에서의 액션을 평가할 때 현재 가지고있는 정책을 가지고 여러번 끝까지 가보고 (샘플링) 이를 보상값으로 쓰는겁니다. 

![additive2](/assets/images/CS285/lec-6/additive2.png){: width="70%"}

이는 정책과 MDP에는 랜덤성 (randomness)를 가지고 있기 때문에 그렇습니다. 

$$ 
\begin{aligned}
& \hat{Q_{i,t}} = \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) & \\
& \hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ] & \\
\end{aligned}
$$ 

이러한 문제점은 앞서 Lecture 5에서 살펴봤던 것들처럼 정책 경사 알고리즘의 문제점인 `High Variance` 와도 직접적으로 관련이 있는데요, 여러번 샘플링한 것을 reward to go로 사용할수록 Variance는 줄어들고 한번 사용하면 Variance는 굉장히 큽니다. 
그리고 당연히 Full Expectation을 계산할 수 있으면 Variance는 엄청나게 줄어들겁니다.


그러니까 우리가 만약

$$
\hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$

같은 근사 Q-Function이 아니라 진짜 (`True Q-Function`)에 

$$
Q(s_t, a_t) = \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$

수 있다면 좋지 않을까요?

그렇다면 Objective는 아래처럼 다시 쓸 수 있게 됩니다.

$$
\begin{aligned}
& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \hat{Q_{i,t}} & \\

& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) Q(s_{i,t},a_{i,t}) & \\

\end{aligned}
$$

즉 더이상 우변의 reward to go가 샘플링을 통해서 얻은 값이 아닌 그 해당 $$s_t$$에서의 가능한 수를 모두 탐색해본 `Full Expectation`이 되는 겁니다.


그렇다면 지난 Lecture 5에서 배운 것 처럼 baseline을 `True Q-Function`에도 적용할 수 있을까요?



![slide4](/assets/images/CS285/lec-6/slide4.png)
*Slide. 4.*

답은 '당연하다' 입니다.


(6장은 오피셜 pdf가 굉장히 수식이 겹쳐서 안보이는 경우가 많네요, 최대한 latex으로 다시 쓰겠습니다.) 


5장에서 `baseline` $$b$$를 `average reward` 로 설정하는게 꽤 괜찮다고 했었으니 그대로 쓰겠습니다.
True Q-Function을 쓰면 아래와 같이 나타낼 수 있겠네요.

$$
b_i = \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t})
$$

그리고 이를 도입한 `Gradient of Objective`는 아래와 같습니다.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - b)
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t}))
& \\
\end{aligned}
$$

이렇게 도입된 baseline을 기준으로 이를 넘는 결과를 가져오는 행동은 독려하고 아닌 행동의 확률은 줄이는 것이 정책 경사 알고리즘의 Variance를 더욱 줄여주게 됩니다.
여기서 baseline은 사실 bias를 야기하는 action $$a_t$$이 아닌 state $$s_t$$에만 의존할 수 있는데요, 
바로 이 state에만 의존하는 (depend) 수식으로 최고의 선택은 `가치 함수 (Value Function)`가 될 수 있겠습니다. (optimal은 아니라고 하네요)

$$
V(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [Q(s_t,a_t)]
$$

가치함수는 어떤 상태 $$s_t$$에서 선택 가능한 행동들에 대해서 모두 해보고 그 보상들을 합치는 (정확히는 기대값) 입니다.
즉 가치함수를 baseline으로 고르는 것은 굉장히 현명한 선택인거죠.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t}))
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - V(s_{i,t}))
& \\
\end{aligned}
$$

이는 사실 굉장히 중요한 점을 시사하는데요, 위의 수식에서 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$ 텀이 log probability에 곱해짐으로써 `어떤 상태에서, 어떤 행동을 선택 하는 것이 모든 행동을 고려했을 때의 평균 값 보다 얼마나 좋은가?`를 나타내기 때문입니다.


이만큼 중요한 의미를 가지는 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$를 우리는 `special term`으로 나타내는데, 이것이 바로 `Advantage Function` 입니다.


자 이제 `State Value Function`과 `State-Action Value Function`에 대해서 조금 더 얘기해보도록 하겠습니다. (State-Action Value Function을 Q라고 할 수도 있으나 완전히 같은 것은 아니라고 하네요 (?))


![slide5](/assets/images/CS285/lec-6/slide5.png)
*Slide. 5.*

앞서 다 얘기한 것이지만 다시 정의하자면, State-Action Value Function은 우리가 $$s_t$$에서 행동 $$a_t$$를 취했을 때 이후 현재 가지고 있는 정책을 통해 에피소드가 끝날때까지 돌려보고 매 step마다의 보상값들을 전부 더한겁니다. 

$$
Q(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]
$$

큐 함수를 아래와 같이 윗첨자에 $$\pi$$를 추가하여 표시하곤 하는데요,

$$
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t}
$$

이는 큐 함수가 정책 $$\pi$$에 의존한다는걸 강조하기 위한 겁니다.
그러니까 가능한 모든 정책이 각각의 다른 큐 함수를 가지고 있다는 거죠.


$$
V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)] \scriptstyle{\text{ total reward from } s_t}
$$

가치 함수는 이러한 큐함수에 기대값을 취해서 어떤 상태 $$s_t$$에서 가능한 행동 옵션들을 취했을 때의 큐함수를 전부 더한겁니다 (정확히는 그 행동을 취할 확률까지 곱한 기대값). 


마지막으로 우리는 Advantage Function을 아래와 같이 정의할 수 있었습니다.

$$
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) \scriptstyle{\text{ how much better } a_t is}
$$


다시 셋을 정리하면 

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)
& \scriptstyle{\text{ how much better } a_t is} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t is} \\

\end{aligned}
$$


![slide6](/assets/images/CS285/lec-6/slide6.png)
*Slide. 6.*



![slide7](/assets/images/CS285/lec-6/slide7.png)
*Slide. 7.*



 
![slide8](/assets/images/CS285/lec-6/slide8.png)
*Slide. 8.*



![slide9](/assets/images/CS285/lec-6/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-6/slide10.png)
*Slide. 10.*






## <mark style='background-color: #fff5b1'> From Evaluation to Actor Critic </mark>

![slide12](/assets/images/CS285/lec-6/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-6/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-6/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-6/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-6/slide16.png)
*Slide. 16.*



## <mark style='background-color: #fff5b1'> Actor-Critic Design Decisions </mark>

![slide18](/assets/images/CS285/lec-6/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-6/slide19.png)
*Slide. 19.*



## <mark style='background-color: #fff5b1'> Critics as Baselines </mark>

![slide21](/assets/images/CS285/lec-6/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-6/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-6/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-6/slide24.png)
*Slide. 24.*



## <mark style='background-color: #fff5b1'> Review, Examples, and Additional Readings </mark>

![slide26](/assets/images/CS285/lec-6/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-6/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-6/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-6/slide29.png)
*Slide. 29.*





### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [강화학습 알아보기(4) - Actor-Critic, A2C, A3C from 김환희님](https://greentec.github.io/reinforcement-learning-fourth/)













