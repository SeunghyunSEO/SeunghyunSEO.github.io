---
title: () Lecture 6 - Actor-Critic Algorithms

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 6의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=wr00ef_TY6Q&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=23)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


![slide1](/assets/images/CS285/lec-6/slide1.png)
*Slide. 1.*

Lecture 5 에서는 정책 경사 알고리즘 (Policy Gradient Algorithm, PG)에 대해서 알아봤습니다.
이번에는 PG에 기반한 `Actor-Critic Algorithm`에 대해서 알아보도록 할 것입니다.


시작하기에 앞서 아래의 `Deep Reinforcement Learning Taxonomy`을 보시면 도움이 될 것 같습니다.

![rl_taxonomy_intellabs_for_lec5](/assets/images/CS285/lec-5/rl_taxonomy_intellabs_for_lec5.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec5](/assets/images/CS285/lec-5/rl_taxonomy_openai_for_lec5.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))


Taxonomy에 보시면 Actor-Critic을 발전시킨 `Advantage Actor-Critic, A2C`, 하나의 에이전트가 아닌 여러 에이전트로 부터 학습하는 비동기식 알고리즘인 `Asynchronous Advantage Actor-Critic, A3C` 등도 있지만 본 강의에서는 다루지 않는 것 같습니다.




## <mark style='background-color: #fff5b1'> Recap </mark>

지난 Lecture 5에서 우리는 `정책 경사 알고리즘 (Policy Gradient Algorithm, PG)`에 대해 배웠습니다.

![slide2](/assets/images/CS285/lec-6/slide2.png)
*Slide. 2.*

PG는 `Trajectory`를 여러번 샘플링한 뒤 이를 통해 환경과 상호작용하면서 policy를 `directly 업데이트`하죠.
(각 Trajectory마다 매 time-step에서의 정책의 log-probability와 그 시점부터 끝날시점 T까지의 보상의 합을 다 더한 것을 weighted-sum 한 수식이었죠.)


지난 강의에서 `보상 (reward to go)`을 의도적으로 $$Q^{\pi} (x_t,u_t)$$ 라고 정의했었던 것도 기억이 나실 겁니다.


그리고 PG의 직관적인 이해는 곧 `많은 보상을 초래할 것 같은 행동은 더욱 잘 일어나게끔, 그 반대는 덜 발생하게끔` 하는 것이었습니다.

![slide3](/assets/images/CS285/lec-6/slide3.png)
*Slide. 3.*

$$Q^{\pi} (x_t,u_t)$$에 대해서 조금 더 얘기해보자면 이는 `어떤 상태에서 어떤 행동을 했을 때 이 행동과 policy를 따라서 끝까지 가 봤을때 얻을 보상`이라고 할 수 있습니다.

이 보상함수를 평가할 수 있는 더 좋은 방법이 있을까요?

![additive1](/assets/images/CS285/lec-6/additive1.png){: width="60%"}

그것은 바로 어떤 시점 $$t'$$ 에서의 액션을 평가할 때 현재 가지고있는 정책을 가지고 여러번 끝까지 가보고 (샘플링) 이를 보상값으로 쓰는겁니다. 

![additive2](/assets/images/CS285/lec-6/additive2.png){: width="60%"}

이는 정책과 MDP에는 랜덤성 (randomness)를 가지고 있기 때문에 그렇습니다. 

$$ 
\begin{aligned}
& \hat{Q_{i,t}} \approx \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) & \\
& \hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ] & \\
\end{aligned}
$$ 

이러한 문제점은 앞서 Lecture 5에서 살펴봤던 것들처럼 정책 경사 알고리즘의 문제점인 `High Variance` 와도 직접적으로 관련이 있는데요, 여러번 샘플링한 것을 reward to go로 사용할수록 Variance는 줄어들고 한번 사용하면 Variance는 굉장히 큽니다. 
그리고 당연히 Full Expectation을 계산할 수 있으면 Variance는 엄청나게 줄어들겁니다.


그러니까 우리가 만약

$$
\hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$

같은 근사 Q-Function이 아니라 진짜 (`True Q-Function`)에 

$$
Q(s_t, a_t) = \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$

수 있다면 좋지 않을까요? ($$\approx$$ 가 아니라 $$=$$ 인 것에 주의)

그렇다면 Objective는 아래처럼 다시 쓸 수 있게 됩니다.

$$
\begin{aligned}
& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \hat{Q_{i,t}} & \\

& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) Q(s_{i,t},a_{i,t}) & \\

\end{aligned}
$$

즉 더이상 우변의 reward to go가 샘플링을 통해서 얻은 값이 아닌 그 해당 $$s_t$$에서의 가능한 수를 모두 탐색해본 `Full Expectation`이 되는 겁니다.


그렇다면 지난 Lecture 5에서 배운 것 처럼 baseline을 `True Q-Function`에도 적용할 수 있을까요?



![slide4](/assets/images/CS285/lec-6/slide4.png)
*Slide. 4.*

답은 '당연하다' 입니다.


(6장은 오피셜 pdf가 굉장히 수식이 겹쳐서 안보이는 경우가 많네요, 최대한 latex으로 다시 쓰겠습니다.) 


5장에서 `baseline` $$b$$를 `average reward` 로 설정하는게 꽤 괜찮다고 했었으니 그대로 쓰겠습니다.
True Q-Function을 쓰면 아래와 같이 나타낼 수 있겠네요.

$$
b_i = \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t})
$$

그리고 이를 도입한 `Gradient of Objective`는 아래와 같습니다.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - b)
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t}))
& \\
\end{aligned}
$$

이렇게 도입된 baseline을 기준으로 이를 넘는 결과를 가져오는 행동은 독려하고 아닌 행동의 확률은 줄이는 것이 정책 경사 알고리즘의 Variance를 더욱 줄여주게 됩니다.
여기서 baseline은 사실 bias를 야기하는 action $$a_t$$이 아닌 state $$s_t$$에만 의존할 수 있는데요, 
바로 이 state에만 의존하는 (depend) 수식으로 최고의 선택은 `가치 함수 (Value Function)`가 될 수 있겠습니다. (optimal은 아니라고 하네요)

$$
V(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [Q(s_t,a_t)]
$$

가치함수는 어떤 상태 $$s_t$$에서 선택 가능한 행동들에 대해서 모두 해보고 그 보상들을 합치는 (정확히는 기대값) 입니다.
즉 가치함수를 baseline으로 고르는 것은 굉장히 현명한 선택인거죠.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t}))
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - V(s_{i,t}))
& \\
\end{aligned}
$$

이는 사실 굉장히 중요한 점을 시사하는데요, 위의 수식에서 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$ 텀이 log probability에 곱해짐으로써 `어떤 상태에서, 어떤 행동을 선택 하는 것이 모든 행동을 고려했을 때의 평균 값 보다 얼마나 좋은가?`를 나타내기 때문입니다.


이만큼 중요한 의미를 가지는 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$를 우리는 `special term`으로 나타내는데, 이것이 바로 `Advantage Function` 입니다.


자 이제 `State Value Function`과 `State-Action Value Function`에 대해서 조금 더 얘기해보도록 하겠습니다. (State-Action Value Function을 Q라고 할 수도 있으나 완전히 같은 것은 아니라고 하네요 (?))


![slide5](/assets/images/CS285/lec-6/slide5.png)
*Slide. 5.*

앞서 다 얘기한 것이지만 다시 정의하자면, State-Action Value Function은 우리가 $$s_t$$에서 행동 $$a_t$$를 취했을 때 이후 현재 가지고 있는 정책을 통해 에피소드가 끝날때까지 돌려보고 매 step마다의 보상값들을 전부 더한겁니다. 

$$
Q(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]
$$

큐 함수를 아래와 같이 윗첨자에 $$\pi$$를 추가하여 표시하곤 하는데요,

$$
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t}
$$

이는 큐 함수가 정책 $$\pi$$에 의존한다는걸 강조하기 위한 겁니다.
그러니까 가능한 모든 정책이 각각의 다른 큐 함수를 가지고 있다는 거죠.


$$
V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)] \scriptstyle{\text{ total reward from } s_t}
$$

가치 함수는 이러한 큐함수에 기대값을 취해서 어떤 상태 $$s_t$$에서 가능한 행동 옵션들을 취했을 때의 큐함수를 전부 더한겁니다 (정확히는 그 행동을 취할 확률까지 곱한 기대값). 


마지막으로 우리는 Advantage Function을 아래와 같이 정의할 수 있었습니다.

$$
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) \scriptstyle{\text{ how much better } a_t is}
$$


다시 셋을 정리하면 아래와 같습니다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)
& \scriptstyle{\text{ how much better } a_t is} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t is} \\

\end{aligned}
$$

***

이 셋은 RL에서 굉장히 중요한 `Quantity` 이므로 직관적으로 감이 안오신다면 이쯤에서 잠시 끊고 깊게 생각해 본 뒤  넘어가셔도 좋을 것 같습니다.

***

하지만 당연히 실제 알고리즘이 동작할 때 우리는 Advantage Function의 `정확한 값 (correct value)`를 알 수 없습니다. 
그래서 우리는 이 함수를 근사해서 사용하게 될텐데요, 이 근사 함수가 원본에 가까울수록 variance가 낮아집니다.


이제 아래의 Anatomy를 다시 보시면,

![additive3](/assets/images/CS285/lec-6/additive3.png){: width="70%"}
*Fig. Anatomy of Deep RL *

녹색박스가 좀 더 디테일해졌음을 알 수 있습니다. 
이 녹색부분인 `Fitting Value Function` 부분을 좀 더 살펴보도록 하겠습니다.

![slide6](/assets/images/CS285/lec-6/slide6.png)
*Slide. 6.*

```
fit what to what?
```

$$Q,V,A$$ 각각을 어떤 타겟에 피팅시켜야할까요?


다시 세 가지를 적고요

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)
& \scriptstyle{\text{ how much better } a_t is} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t is} \\

\end{aligned}
$$


여기서 우선 Q는 $$s_t,a_t$$가 랜덤 변수가 아니기 때문에 아래와 같이 수식을 다시 쓸 수 있다고 하는데요, (같은 수식입니다, 한스텝 전개했을 뿐)

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

\end{aligned}
$$

여기서 우변의 두 번째 항은 아래와 바꿀 수 있습니다.. 


$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \underbrace{\sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]}{V^{\pi}(s_{t+1})}
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})
& \\

\end{aligned}
$$

즉 Q 함수를 `current reward` + `expected value of the reward of the value function of the next time step`로 나타낼 수 있는겁니다.

하지만 여기에는 Vanilla Policy Gradient 에는 필요 없었던 `Transition Probability`가 포함되어 있습니다.


여기서 우리가 원하는 진짜 값인 $$\mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})$$ 를 구해내기는 어려우니, 여기에 근사 (approximation) 가 한번 들어갑니다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + V^{\pi}(s_{t+1})
& \\

\end{aligned}
$$

위의 수식이 의미하는 바는 next-time step의 state에 대한 분포를 이전에 해왔던 것 처럼 `a single sample estimator`로 딱 한번 샘플링 하는것으로 근사하는겁니다. 
하지만 주의해야 할 점은 딱 그 한스텝만 한번 샘플링하는것이고 그 뒤로는 여전히 모든 값을 고려하는 Expectation을 계산한다는 겁니다.

물론 이런 근사를 한 결과값은 당연히 정확한 값과는 거리가 있겠고, variance도 조금 더 높습니다.


하지만 이렇게 하는데에는 이유가 있는데요,

바로 Q함수를 근사한 덕분에 `Advantage Function`을 아래와 같이 표현할 수 있기 때문입니다.


$$
\begin{aligned}

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + V^{\pi}(s_{t+1})
& \\

&
A^{\pi}(s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_{t})
& \\ 

&
A^{\pi}(s_t,a_t) = r(s_t,a_t) + V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})
& \\ 

\end{aligned}
$$

이렇게 얻은 수식이 가져다주는 이점은 바로 수식이 $$V^{\pi}$$에 전적으로 의존하게 된다는 건데요,
이렇게 V를 학습하는 것이 Q와 A를 근사하는 것보다 쉽다고 합니다. 
왜냐하면 Q와 A는 $$s_t,a_t$$ 둘 다 필요하지만, `V는 state만 있으면` 되기 때문입니다.


즉, 근사함수가 의존하는 factor가 하나 더 적기 때문에 더 적은 샘플로 다음 state를 예측하는 V함수를 찾는건 할만 하다는 겁니다.

(물론 이 강의를 좀 더 진행하고난 뒤에는 $$Q^{\pi}$$를 직접 근사하는 방법론에 대해서도 살펴볼 것이라고 합니다.)


한편, 이 분야에서 `Universal Function Approximator`로 잘 쓰는 것이 있는데요, 그것은 바로

![additive4](/assets/images/CS285/lec-6/additive4.png){: width="70%"}
*Fig. Neural Netowrk as Universal Function Approximator*

`뉴럴 네트워크 (Neural Network, NN)` 입니다.
그리고 이 네트워크는 $$\phi$$로 파라메터화 되어있습니다.
(주의 : 정책은 $$\pi$$로 파라메터화 되어있음)


이제 $$V^{\pi}(s)$$ 를 근사시켜봅시다.

![slide7](/assets/images/CS285/lec-6/slide7.png)
*Slide. 7.*

이 $$V^{\pi}(s)$$를 피팅하는 과정은 일반적으로 `Policy Evaluation`이라고도 하는데요,
그 이유는 $$V^{\pi}(s)$$가 매 state마다의 policy의 가치 (얼마나 좋은가?) 를 계산하기 때문입니다.


그리고 지난 강의에서 초기 상태인 $$s_1$$의 가치 함수의 기대값이 바로 Objective와 동일하고, 이를 최대로 하는 것이 곧 강화학습의 목표라고도 할 수 있다고 했었습니다.

$$
J(\theta) = \mathbb{E}_{s_1 \sim p(s_1)} [ V^{\pi} (s_1) ] 
$$

그렇다면 어떻게 Policy를 평가할 수 있을까요?


그 중 하나는 바로 `Monte Carlo Policy Evaluation` 입니다.
이는 정책 경사 알고리즘이 작용하는 방식과도 같은 것으로, 여러번 샘플링해서 이를 바탕으로 평가를 하는 방법이었습니다. (기대값은 모든 상황을 다 고려해야하지만 이는 사실상 불가능하기때문에)

$$
V^{\pi} (s_t) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t'=t}^T r(s_{t'},a_{t'})
$$
 
![slide8](/assets/images/CS285/lec-6/slide8.png)
*Slide. 8.*

몬테 카를로 방법을 사용해서 몇개의 trajectory를 샘플링 했을 때 연속적인 state space에서 똑같은 곳을 다시 방문할 수는 없지만 아주 근사한 상태를 방문할 수는 있을겁니다. 그리고 뉴럴 네트워크라는 근사함수는 이러한 비슷한것을 한데 묶어서 생각할 수 있는, 그러니까 `Generalization`을 잘 하는 함수죠 (오버피팅 하겠지만, 좋은 네트워크는 잘 해야 합니다).


그러므로 우리가 가치 함수를 학습시킬 데이터는 현재 가지고 있는 정책으로 `simulation`을 돌려보고 (`roll out`) 얻은 경로를 정답으로 하는겁니다. (이를 데이터 삼아 사용함)

$$
V^{\pi} (s_t) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t'=t}^T r(s_{t'},a_{t'})
$$

$$
\text{training data : } (s_{i,t}, \underbrace{ \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) }{ y_{i,t} } )
$$

그리고 이를 기반으로 `지도학습 (Supervised Learning)`을 하면,
Value Function은 출력이 (지금은 벡터가 아니라 `scalar`) 보상값이고 이 값은 연속적이기 때문에 가치 함수를 fitting하는 것은 출력 분포를 가우시안 분포로 모델링하여 얻은 아래의 `MSE loss`를 줄이는, 이른 바 `회귀 (Regression)` 문제를 푸는 것이 됩니다.

$$
L(\phi) = \frac{1}{2} \sum_i \parallel \hat{V}_{\phi}^{\pi}(s_i) - y_i \parallel^2
$$

여기서 이 `가치 함수 네트워크가 심하게 오버피팅을 하면` $$y_{i,t}$$를 직접적으로 가져다가 사용하는 Vanilla Policy Gradient와 비교해서 성능이 아주 안좋을 수 있지만, 일반화에 성공한다면 우리는 더 낮은 variance를 얻을 수 있습니다.



왜냐하면 직접적으로 $$y_{i_t}$$를 사용해 이를 평균내서 weighted-sum 하는 기존의 정책 경사 알고리즘은 비슷한 state에서의 비슷하지 않은 label들을 평균내서 사용하기 때문입니다. (?)








![slide9](/assets/images/CS285/lec-6/slide9.png)
*Slide. 9.*

하지만 아직도 부족합니다. 우리가 어떤 상태에서 직접 시뮬레이션을 끝까지 돌려서 얻은 보상값을 타겟으로 하지 말고 `True expecte value of rewards`를 사용할 순 없을까요?

$$
\begin{aligned}

& 
y_{i,t} = \sum_{t=t'}^T \mathbb{E}_{\pi_{\theta}}[r(s_{t'},a_{t'}) \vert s_{i,t}]
& \scriptstyle{ideal target} \\

&
y_{i,t} = \sum_{t=t'}^T r(s_{i,t'},a_{i,t'})
& \scriptstyle{Monte Carlo (MC) Traget (one sample)} \\

\end{aligned}
$$








![slide10](/assets/images/CS285/lec-6/slide10.png)
*Slide. 10.*











## <mark style='background-color: #fff5b1'> From Evaluation to Actor Critic </mark>

![slide12](/assets/images/CS285/lec-6/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-6/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-6/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-6/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-6/slide16.png)
*Slide. 16.*



## <mark style='background-color: #fff5b1'> Actor-Critic Design Decisions </mark>

![slide18](/assets/images/CS285/lec-6/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-6/slide19.png)
*Slide. 19.*



## <mark style='background-color: #fff5b1'> Critics as Baselines </mark>

![slide21](/assets/images/CS285/lec-6/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-6/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-6/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-6/slide24.png)
*Slide. 24.*



## <mark style='background-color: #fff5b1'> Review, Examples, and Additional Readings </mark>

![slide26](/assets/images/CS285/lec-6/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-6/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-6/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-6/slide29.png)
*Slide. 29.*





### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [강화학습 알아보기(4) - Actor-Critic, A2C, A3C from 김환희님](https://greentec.github.io/reinforcement-learning-fourth/)













