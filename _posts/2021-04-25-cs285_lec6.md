---
title: () Lecture 6 - Actor-Critic Algorithms

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 6의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=wr00ef_TY6Q&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=23)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


![slide1](/assets/images/CS285/lec-6/slide1.png)
*Slide. 1.*

Lecture 5 에서는 정책 경사 알고리즘 (Policy Gradient Algorithm, PG)에 대해서 알아봤습니다.
이번에는 PG에 기반한 `Actor-Critic Algorithm`에 대해서 알아보도록 할 것입니다.


시작하기에 앞서 아래의 `Deep Reinforcement Learning Taxonomy`을 보시면 도움이 될 것 같습니다.

![rl_taxonomy_intellabs_for_lec5](/assets/images/CS285/lec-5/rl_taxonomy_intellabs_for_lec5.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec5](/assets/images/CS285/lec-5/rl_taxonomy_openai_for_lec5.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))


Taxonomy에 보시면 Actor-Critic을 발전시킨 `Advantage Actor-Critic, A2C`, 하나의 에이전트가 아닌 여러 에이전트로 부터 학습하는 비동기식 알고리즘인 `Asynchronous Advantage Actor-Critic, A3C` 등도 있지만 본 강의에서는 다루지 않는 것 같습니다.




## <mark style='background-color: #fff5b1'> Recap </mark>

강의에서는 간단하게 PG에 대해서 복습을 하고 넘어갑니다. (익숙하신 분은 다음 섹션으로 넘어가시면 됩니다.)

![slide2](/assets/images/CS285/lec-6/slide2.png)
*Slide. 2.*

PG는 `Trajectory`를 여러번 샘플링한 뒤 이를 통해 환경과 상호작용하면서 policy를 `directly 업데이트`하죠.
(각 Trajectory마다 매 time-step에서의 정책의 log-probability와 그 시점부터 끝날시점 T까지의 보상의 합을 다 더한 것을 weighted-sum 한 수식이었죠.)


지난 강의에서 `보상 (reward to go)`을 의도적으로 $$Q^{\pi} (x_t,u_t)$$ 라고 정의했었던 것도 기억이 나실 겁니다.


그리고 PG의 직관적인 이해는 곧 `많은 보상을 초래할 것 같은 행동은 더욱 잘 일어나게끔, 그 반대는 덜 발생하게끔` 하는 것이었습니다.

![slide3](/assets/images/CS285/lec-6/slide3.png)
*Slide. 3.*

$$Q^{\pi} (x_t,u_t)$$에 대해서 조금 더 얘기해보자면 이는 `어떤 상태에서 어떤 행동을 했을 때 이 행동과 policy를 따라서 끝까지 가 봤을때 얻을 보상`이라고 할 수 있습니다.

이 보상함수를 평가할 수 있는 더 좋은 방법이 있을까요?

![additive1](/assets/images/CS285/lec-6/additive1.png){: width="70%"}

그것은 바로 어떤 시점 $$t'$$ 에서의 액션을 평가할 때 현재 가지고있는 정책을 가지고 여러번 끝까지 가보고 (샘플링) 이를 보상값으로 쓰는겁니다. 

![additive2](/assets/images/CS285/lec-6/additive2.png){: width="70%"}

이는 정책과 MDP에는 랜덤성 (randomness)를 가지고 있기 때문에 그렇습니다. 

$$ 
\begin{aligned}
& \hat{Q_{i,t}} \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) & \\
& \hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{i,t'},a_{i,t'}) \vert s_t,a_t ] & \\
\end{aligned}
$$ 

이러한 문제점은 앞서 Lecture 4에서 살펴봤던 것들처럼 정책 경사 알고리즘의 문제점인 `High Variance` 와도 직접적으로 관련이 있는데요, 여러번 샘플링한 것을 reward to go로 사용할수록 Variance는 줄어들고 한번 사용하면 Variance는 굉장히 큽니다. 
그리고 당연히 Full Expectation을 계산할 수 있으면 Variance는 엄청나게 줄어들겁니다.


그러니까 우리가 만약

$$
\hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{i,t'},a_{i,t'}) \vert s_t,a_t ]
$$

같은 근사 Q-Function이 아니라 진짜 (`True Q-Function`)에 

$$
Q_{i,t} = \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{i,t'},a_{i,t'}) \vert s_t,a_t ]
$$



![slide4](/assets/images/CS285/lec-6/slide4.png)
*Slide. 4.*

![slide5](/assets/images/CS285/lec-6/slide5.png)
*Slide. 5.*

![slide6](/assets/images/CS285/lec-6/slide6.png)
*Slide. 6.*

![slide7](/assets/images/CS285/lec-6/slide7.png)
*Slide. 7.*

![slide8](/assets/images/CS285/lec-6/slide8.png)
*Slide. 8.*

![slide9](/assets/images/CS285/lec-6/slide9.png)
*Slide. 9.*

![slide10](/assets/images/CS285/lec-6/slide10.png)
*Slide. 10.*



## <mark style='background-color: #fff5b1'> From Evaluation to Actor Critic </mark>

![slide12](/assets/images/CS285/lec-6/slide12.png)
*Slide. 12.*

![slide13](/assets/images/CS285/lec-6/slide13.png)
*Slide. 13.*

![slide14](/assets/images/CS285/lec-6/slide14.png)
*Slide. 14.*

![slide15](/assets/images/CS285/lec-6/slide15.png)
*Slide. 15.*

![slide16](/assets/images/CS285/lec-6/slide16.png)
*Slide. 16.*



## <mark style='background-color: #fff5b1'> Actor-Critic Design Decisions </mark>

![slide18](/assets/images/CS285/lec-6/slide18.png)
*Slide. 18.*

![slide19](/assets/images/CS285/lec-6/slide19.png)
*Slide. 19.*



## <mark style='background-color: #fff5b1'> Critics as Baselines </mark>

![slide21](/assets/images/CS285/lec-6/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-6/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-6/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-6/slide24.png)
*Slide. 24.*



## <mark style='background-color: #fff5b1'> Review, Examples, and Additional Readings </mark>

![slide26](/assets/images/CS285/lec-6/slide26.png)
*Slide. 26.*

![slide27](/assets/images/CS285/lec-6/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-6/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-6/slide29.png)
*Slide. 29.*





### <mark style='background-color: #dcffe4'> asd </mark>

## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [강화학습 알아보기(4) - Actor-Critic, A2C, A3C from 김환희님](https://greentec.github.io/reinforcement-learning-fourth/)













