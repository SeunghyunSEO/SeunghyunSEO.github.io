---
title: 5.7 Bayesian Neural Networks
categories: Brief_Review_for_PRML
tag: [PRML,MachineLearning,ML]

toc: true
toc_sticky: true

comments: true
---


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---

### <mark style='background-color: #dcffe4'> 5.7.1 사후 매개변수 분포 (Posterior parameter distribution) </mark>

$$
p(t \vert x,w,\beta) = N(t \vert y(x,w), \beta^{-1})
$$

$$
p(w \vert \alpha) = N(w \vert 0, \alpha^{-1} I)
$$

N개의 관측값 (Observations, Samples) $$x_1,x_2, \cdots, x_n$$과 이에 해당하는 표적값 (Target) $$D=\{ t_1,t_2,\cdots,x_n \}$$에 대해서 likelihood는 아래와 같습니다.

$$
p(D \vert w,\beta) = \prod_{n=1}^N N(t_n \vert y(x_n,w),\beta^{-1})
$$

posterior는 일반적으로 아래와 같은 관계식을 따르므로

$$
posterior \approx likelihood \times \prior
$$

최종적으로 다음 형태를 취하게 된다.

$$
p(w \vert D,\alpha,\beta) \approx p(w \vert \alpha) p(D \vert w,\beta)
$$

여기서 $$y(x,w)$$의 $$w$$에 대한 비선형성으로 인해서 위의 식은 비 가우시안 분포가 될 것입니다.

라플라스 근사법을 사용해서 posterior의 가우시안 근사치를 구할 수 있는데, 이를 위해서는 사후 분포의 (지역적) 최대값을 찾아야 하며, (= MAP solution을 의미함) 이는 activation function이 가지는 전체 함수의 비선형성 때문에 닫힌 해 (closed-form)를 구할 수 없으므로 반복적인 최적화를 통해 구해야 합니다. 


MAP 솔루션은 간단하게 posterior에 대해서 $$log$$를 취한 뒤 최적화를 통해 구할 수 있습니다.

$$
ln p(w \vert D) = -\frac{\alpha}{2} w^T w - \frac{\beta}{2} \sum_{n=1}^N { \{ y(x_n,w) - t_n \} }^2 + const
$$

앞서 배운 것 처럼 우리가 likelihood로 가우시안 분포를 가정했으므로 (=회귀 문제) MSE loss가 나오는 것은 당연하고, 여기에 파라메터에 대해서 또 한번 가우시안 prior를 가정했으므로 정규화 항이 딸려서 나오는 것은 당연하겠죠?.



머신러닝을 통해서 최종적으로 추론하려는 것은, 

$$
p(t \vert x,D)
$$

이고, 베이지안 방법론의 목적은 파라메터를 점 추정 하는것에 그치지 않고 (MLE나 MAP는 최대값인 파라메터 딱 하나만을 결과물로 취함), 모두 고려하는 것이기 때문에

$$
p(t \vert x,D) = \int p(t \vert x,w) p(w \vert D, \alpha, \beta) dw
$$

처럼 표현이 가능하고, 우리는 라플라스 근사식을 통해서 $$\int$$속 복잡한 posterior를 근사 분포로 만들어 최종적으로 다음과 같은 수식을 얻을 수 있었습니다.

$$
p(t \vert x,D) = \int p(t \vert x,w) q(w \vert D) dw
$$

이제 학습이 끝난 후 어떤 unseen 데이터 $$x$$가 들어오면 이를 위의 수식에 넣어 적분을 통해 $$t$$값을 추론해 내기만 하면 되는것입니다.
하지만 이 적분식을 해석적으로(?) 계산하기는 여전히 어려운데 왜냐하면 이는 $$w$$의 함수로 주어지는 네트워크 함수 $$y(x,w)$$가 비선형이기 때문이라고 합니다.




### <mark style='background-color: #dcffe4'> 5.7.2 초매개변수의 최적화 (Hyperparameter optimization) </mark>

여태까지는 likelihood와 prior의 분산을 의미하는 $$\alpha,\beta$$가 고정되어있는 경우를 가정하고 문제를 풀었는데요 (fixed variance problem),

### <mark style='background-color: #dcffe4'> 5.7.3 베이지안 뉴럴 네트워크를 통한 분류 (Bayesian neural networks for classification) </mark>

베이지안 뉴럴 네트워크를 통해서 문제를 풀어보도록 할건데요, 다중 분류 문제는 이진 분류 문제와 크게 다르지 않기 때문에 우선 간단한 이진 분류 문제를 가정해보도록 하겠습니다.

$$
ln p(D \vert w) = \sum_{n=1}^{N} \{ t_n ln y_n + (1-t_n) ln(1-y_n) \} 
$$

여기서 분산을 나타내는 $$\beta$$가 존재하지 않는 이유는, 데이터 포인트들이 올바르게 레이블링 되어있다고 가정하기 때문입니다. 
이제 prior를 가정할것인데, 앞선 문제들과 마찬가지로 등방 가우시안 분포를 가정하도록 할겁니다.

$$
p(w \vert \alpha) = N(w \vert 0, \alpha^{-1}I)
$$

이 모델에 라플라스 방법론을 적용하는 것은 아래와 같습니다.

- 1.첫 번째로 파라메터 $$\alpha$$를 초기화한다. 
- 2.로그 사후 분포를 최대화 함으로써 매개변수 $$w$$의 값을 찾는다. (마찬가지로 이는 정규화 항이 포함된 Binary Cross Entropy (BCE) 수식을 최소화 하는 것이 될겁니다.)

$$
E(w) = -ln p(D \vert w) + \frac{\alpha}{2} w^T w
$$

- 3.MAP의 해인, $$w_{MAP}$$를 구하고 나면 음의 로그 가능도 (negative log likelihood, nll)의 이차 미분값들로 이루어진 Hessian Matrix, $$H$$를 구한다.
- 4.이를 이용해 posterior의 근사 분포를 구한다. (라플라스 근사)
- 5.(optional) prior의 분산, $$\alpha$$마저 최적화한다.


![Fig5.22](/assets/images/PRML_5.7/Fig5.22.png)
*Fig. 5.22*



![Fig5.23](/assets/images/PRML_5.7/Fig5.23.png)
*Fig. 5.23*




### <mark style='background-color: #dcffe4'> References </mark>
