---
title: Lecture 6 - Actor-Critic Algorithms

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있기도 하고 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 따라가는게 좋을 것 같아 강의 슬라이드를 그대로 사용해서 글을 전개하려고 합니다. (그리고 이해를 돕기 위해 추가 자료를 중간 중간 사용할 예정입니다.)


Lecture 6의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=wr00ef_TY6Q&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=23)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


![slide1](/assets/images/CS285/lec-6/slide1.png)
*Slide. 1.*

Lecture 5 에서는 정책 경사 알고리즘 (Policy Gradient Algorithm, PG)에 대해서 알아봤습니다.
이번에는 PG에 기반한 `Actor-Critic Algorithm`에 대해서 알아보도록 할 것입니다.


시작하기에 앞서 아래의 `Deep Reinforcement Learning Taxonomy`을 보시면 도움이 될 것 같습니다.

![rl_taxonomy_intellabs_for_lec5](/assets/images/CS285/lec-6/rl_taxonomy_intellabs_for_lec5.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec5](/assets/images/CS285/lec-6/rl_taxonomy_openai_for_lec5.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))


Taxonomy에 보시면 Actor-Critic 알고리즘에는 이를 발전시킨 `Advantage Actor-Critic, A2C`, 하나의 에이전트가 아닌 여러 에이전트로 부터 학습하는 비동기식 알고리즘인 `Asynchronous Advantage Actor-Critic, A3C` 라는 알고리즘들도 있는데 이번 강의에서 이 개념들까지 다루고 있습니다.




## <mark style='background-color: #fff5b1'> Recap </mark>

지난 Lecture 5에서 우리는 `정책 경사 알고리즘 (Policy Gradient Algorithm, PG)`에 대해 배웠습니다.

![slide2](/assets/images/CS285/lec-6/slide2.png)
*Slide. 2.*

PG는 `Trajectory`를 여러번 샘플링한 뒤 이를 통해 환경과 상호작용하면서 policy를 `directly 업데이트`하죠.
(각 Trajectory마다 매 time-step에서의 정책의 log-probability와 그 시점부터 끝날시점 T까지의 보상의 합을 다 더한 것을 weighted-sum 한 수식이었죠.)


지난 강의에서 `보상 (reward to go)`을 의도적으로 $$Q^{\pi} (x_t,u_t)$$ 라고 정의했었던 것도 기억이 나실 겁니다.


그리고 PG의 직관적인 이해는 곧 `많은 보상을 초래할 것 같은 행동은 더욱 잘 일어나게끔, 그 반대는 덜 발생하게끔` 하는 것이었습니다.





## <mark style='background-color: #fff5b1'> Improving Policy Gradient (Lower Variance) </mark>


![slide3](/assets/images/CS285/lec-6/slide3.png)
*Slide. 3.*

$$Q^{\pi} (x_t,u_t)$$에 대해서 조금 더 얘기해보자면 이는 `어떤 상태에서 어떤 행동을 했을 때 이 행동과 policy를 따라서 끝까지 가 봤을때 얻을 보상`이라고 할 수 있습니다.

이 보상함수를 평가할 수 있는 더 좋은 방법이 있을까요?

![additive1](/assets/images/CS285/lec-6/additive1.png){: width="60%"}

그것은 바로 어떤 시점 $$t'$$ 에서의 액션을 평가할 때 현재 가지고있는 정책을 가지고 여러번 끝까지 가보고 (샘플링) 이를 보상값으로 쓰는겁니다. 

![additive2](/assets/images/CS285/lec-6/additive2.png){: width="60%"}

이는 정책과 MDP에는 랜덤성 (randomness)를 가지고 있기 때문에 그렇습니다. 

$$ 
\begin{aligned}
& \hat{Q_{i,t}} \approx \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) & \\
& \hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ] & \\
\end{aligned}
$$ 

이러한 문제점은 앞서 Lecture 5에서 살펴봤던 것들처럼 정책 경사 알고리즘의 문제점인 `High Variance` 와도 직접적으로 관련이 있는데요, 여러번 샘플링한 것을 reward to go로 사용할수록 Variance는 줄어들고 한번 사용하면 Variance는 굉장히 큽니다. 
그리고 당연히 Full Expectation을 계산할 수 있으면 Variance는 엄청나게 줄어들겁니다.


여기에 더해 아래의 i번째 trajectory의 t번째 스텝에서 여러번 샘플링 하는것을 넘어서

$$
\hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$

Full Expectation을 계산한다면, 그러니까 t번째 스텝에서의 진짜 (`True Q-Function`)에 대해서 계산할 수 있다면 좋지 않을까요? ($$\approx$$ 가 아니라 $$=$$ 인 것에 주의)

$$
Q(s_t, a_t) \color{red}{=} \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$


그렇다면 정책 경사 기반 Objective는 아래처럼 다시 쓸 수 있게 됩니다.

$$
\begin{aligned}
& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \hat{Q_{i,t}} & \\

& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \color{red}{Q(s_{i,t},a_{i,t})} & \\

\end{aligned}
$$


즉 더이상 우변의 reward to go가 샘플링을 통해서 얻은 값이 아닌 그 해당 $$s_t$$에서의 가능한 수를 모두 탐색해본 `Full Expectation`이 되는 겁니다.


그렇다면 지난 Lecture 5에서 배운 것 처럼 baseline을 `True Q-Function`에도 적용할 수 있을까요?



![slide4](/assets/images/CS285/lec-6/slide4.png)
*Slide. 4.*

답은 '당연하다' 입니다.


(6장은 오피셜 pdf가 굉장히 수식이 겹쳐서 안보이는 경우가 많네요, 최대한 latex으로 다시 쓰겠습니다.) 


5장에서 `baseline` $$b$$를 우리가 샘플링한 trajectory들에 대한 보상의 평균인, `average reward` 로 설정하는게 꽤 괜찮다고 했었으니 그대로 쓰겠습니다.
True Q-Function을 쓰면 아래와 같이 나타낼 수 있겠네요.

$$
b_i = \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t})
$$

그리고 이를 도입한 `Gradient of Objective`는 아래와 같습니다.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - b)
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \color{red}{\frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t})} )
& \\
\end{aligned}
$$

이렇게 도입된 baseline을 기준으로 이를 넘는 결과를 가져오는 행동은 독려하고 아닌 행동의 확률은 줄이는 것이 정책 경사 알고리즘의 Variance를 더욱 줄여주게 됩니다.
여기서 baseline은 사실 bias를 야기하는 action $$a_t$$이 아닌 state $$s_t$$에만 의존할 수 있는데요, 
바로 이 state에만 의존하는 (depend) 수식으로 최고의 선택은 `가치 함수 (Value Function)`가 될 수 있겠습니다. (optimal은 아니라고 하네요)

$$
V(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [Q(s_t,a_t)]
$$

가치함수는 어떤 상태 $$s_t$$에서 선택 가능한 행동들에 대해서 모두 해보고 그 보상들을 합치는 (정확히는 기대값) 입니다.
즉 가치함수를 baseline으로 고르는 것은 굉장히 현명한 선택인거죠.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \color{red}{\frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t})} )
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \color{blue}{V(s_{i,t})} )
& \\
\end{aligned}
$$

이는 사실 굉장히 중요한 점을 시사하는데요, 위의 수식에서 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$ 텀이 log probability에 곱해짐으로써 `어떤 상태에서, 어떤 행동을 선택 하는 것이 모든 행동을 고려했을 때의 평균 값 보다 얼마나 좋은가?`를 나타내기 때문입니다.


이만큼 중요한 의미를 가지는 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$를 우리는 `special term`으로 나타내는데, 이것이 바로 `Advantage Function`, $$A$$ 입니다.


자 이제 `State Value Function`과 `State-Action Value Function`에 대해서 조금 더 얘기해보도록 하겠습니다. (State-Action Value Function을 Q라고 할 수도 있으나 완전히 같은 것은 아니라고 하네요 (?))


![slide5](/assets/images/CS285/lec-6/slide5.png)
*Slide. 5.*

앞서 다 얘기한 것이지만 다시 정의하자면, State-Action Value Function은 우리가 $$s_t$$에서 행동 $$a_t$$를 취했을 때 이후 현재 가지고 있는 정책을 통해 에피소드가 끝날때까지 돌려보고 매 step마다의 보상값들을 전부 더한겁니다. 

$$
Q(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]
$$

큐 함수를 아래와 같이 윗첨자에 $$\pi$$를 추가하여 표시하곤 하는데요,

$$
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t}
$$

이는 큐 함수가 정책 $$\pi$$에 의존한다는걸 강조하기 위한 겁니다.
그러니까 가능한 모든 정책이 각각의 다른 큐 함수를 가지고 있다는 거죠.


$$
V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)] \scriptstyle{\text{ total reward from } s_t}
$$

가치 함수는 이러한 큐함수에 기대값을 취해서 어떤 상태 $$s_t$$에서 가능한 행동 옵션들을 취했을 때의 큐함수를 전부 더한겁니다 (정확히는 그 행동을 취할 확률까지 곱한 기대값). 


마지막으로 우리는 앞서 말한 것 처럼 Q와 V의 차이를 아래처럼 Advantage Function로 정의해 표시할 수 있습니다.

$$
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) \scriptstyle{\text{ how much better } a_t is}
$$


다시 셋을 정리하면 아래와 같습니다.

***

$$
\begin{aligned}
&
\color{red}{Q^{\pi}(s_t,a_t)} = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
\color{blue}{V^{\pi} (s_t)} = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [ \color{red}{Q^{\pi}(s_t,a_t)} ]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = \color{red}{Q^{\pi}(s_t,a_t)} - \color{blue}{V^{\pi} (s_t)}
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$

***


***

이 셋은 RL에서 굉장히 중요한 `Quantity` 이므로 직관적으로 감이 안오신다면 이쯤에서 잠시 끊고 깊게 생각해 본 뒤  넘어가셔도 좋을 것 같습니다.

***

하지만 당연히 실제 알고리즘이 동작할 때 우리는 Advantage Function의 `정확한 값 (correct value)`를 알 수 없습니다. 
그래서 우리는 이 함수를 근사해서 사용하게 될텐데요, 이 근사 함수가 원본에 가까울수록 variance가 낮아집니다.


이제 아래의 Anatomy를 다시 보시면,

![additive3](/assets/images/CS285/lec-6/additive3.png){: width="70%"}
*Fig. Anatomy of Deep RL*

녹색박스가 좀 더 디테일해졌음을 알 수 있습니다. 
이 녹색부분인 `Fitting Value Function` 부분을 좀 더 살펴보도록 하겠습니다.

![slide6](/assets/images/CS285/lec-6/slide6.png)
*Slide. 6.*

```
fit what to what?
```

$$Q,V,A$$ 각각을 어떤 타겟에 피팅시켜야할까요?


다시 세 가지를 적어 두고요,

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$


여기서 우선 Q는 $$s_t,a_t$$가 랜덤 변수가 아니기 때문에 아래와 같이 수식을 다시 쓸 수 있다고 하는데요, (같은 수식입니다, 한스텝 전개했을 뿐)

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \sum_{t'= \color{red}{t+1} }^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

\end{aligned}
$$

여기서 우변의 두 번째 항은 아래와 바꿀 수 있습니다.. 


$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \underbrace{\sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]}_{ \color{red}{V^{\pi}(s_{t+1})} }
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})
& \\

\end{aligned}
$$

즉 Q 함수를 `(현재 보상) current reward` + `(다음 state로 갔을 때의 가치에 대한 기대값) expected value of the reward of the value function of the next time step`로 나타낼 수 있는겁니다.

하지만 여기에는 Vanilla Policy Gradient 에는 필요 없었던 `Transition Probability`가 포함되어 있습니다.


여기서 우리가 원하는 진짜 값인 $$\mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})$$ 를 구해내기는 어려우니, 여기에 근사 (approximation) 가 한번 들어갑니다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + V^{\pi}(s_{t+1})
& \\

\end{aligned}
$$

위의 수식이 의미하는 바는 next-time step의 state에 대한 분포를 이전에 해왔던 것 처럼 `a single sample estimator`로 딱 한번 샘플링 하는것으로 근사하는겁니다. 
하지만 주의해야 할 점은 딱 그 한스텝만 한번 샘플링하는것이고 그 뒤로는 여전히 모든 값을 고려하는 Expectation을 계산한다는 겁니다.

물론 이런 근사를 한 결과값은 당연히 정확한 값과는 거리가 있겠고, variance도 조금 더 높습니다.


하지만 이렇게 하는데에는 이유가 있는데요,

바로 Q함수를 근사한 덕분에 `Advantage Function`을 아래와 같이 표현할 수 있기 때문입니다.


$$
\begin{aligned}

&
\color{red}{Q^{\pi}(s_t,a_t)} = r(s_t,a_t) + V^{\pi}(s_{t+1})
& \\

&
A^{\pi}(s_t,a_t) = \color{red}{Q^{\pi}(s_t,a_t)} - V^{\pi}(s_{t})
& \\ 

&
A^{\pi}(s_t,a_t) = \color{red}{r(s_t,a_t) + V^{\pi}(s_{t+1})} - V^{\pi}(s_{t})
& \\ 

\end{aligned}
$$

이렇게 얻은 수식이 가져다주는 이점은 바로 수식이 $$V^{\pi}$$에 전적으로 의존하게 된다는 건데요,
이렇게 V에만 의존하게 된 수식에서 우리가 할 것은 V를 학습하는 겁니다. 이렇게 하는 이유는 V를 학습하는 것이 Q와 A를 근사하는 것보다 쉽기 때문인데요, 왜냐하면 Q와 A는 $$s_t,a_t$$ 둘 다 필요하지만, `V는 state만 있으면, 즉 어떤 함수의 입력으로 state만 받으면` 되기 때문입니다.


즉, 근사함수가 의존하는 factor가 하나 더 적기 때문에 더 적은 샘플로 다음 state를 예측하는 V함수를 찾는건 할만 하다는 겁니다.

(물론 이 강의를 좀 더 진행하고난 뒤에는 $$Q^{\pi}$$를 직접 근사하는 방법론에 대해서도 살펴볼 것이라고 합니다.)


한편, 이 분야에서 `Universal Function Approximator`로 잘 쓰는 것이 있는데요, 그것은 바로

![additive4](/assets/images/CS285/lec-6/additive4.png){: width="70%"}
*Fig. Neural Netowrk as Universal Function Approximator*

`뉴럴 네트워크 (Neural Network, NN)` 입니다.
그리고 이 네트워크는 $$\phi$$로 파라메터화 되어있습니다.
(주의 : 정책은 $$\pi$$로 파라메터화 되어있음)


이제 $$V^{\pi}(s)$$ 를 근사시켜봅시다.

![slide7](/assets/images/CS285/lec-6/slide7.png)
*Slide. 7.*

이 $$V^{\pi}(s)$$를 피팅하는 과정은 일반적으로 `Policy Evaluation`이라고도 하는데요,
그 이유는 $$V^{\pi}(s)$$가 매 state마다의 policy의 가치 (얼마나 좋은가?) 를 계산하기 때문입니다.


그리고 지난 강의에서 초기 상태인 $$s_1$$의 가치 함수의 기대값이 바로 Objective와 동일하고, 이를 최대로 하는 것이 곧 강화학습의 목표라고도 할 수 있다고 했었습니다.

$$
J(\theta) = \mathbb{E}_{s_1 \sim p(s_1)} [ V^{\pi} (s_1) ] 
$$

그렇다면 어떻게 Policy를 평가할 수 있을까요?


그 중 하나는 바로 `Monte Carlo Policy Evaluation` 입니다.
이는 정책 경사 알고리즘이 작용하는 방식과도 같은 것으로, 여러번 샘플링해서 이를 바탕으로 평가를 하는 방법이었습니다. (기대값은 모든 상황을 다 고려해야하지만 이는 사실상 불가능하기때문에)

$$
V^{\pi} (s_t) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t'=t}^T r(s_{t'},a_{t'})
$$
 
![slide8](/assets/images/CS285/lec-6/slide8.png)
*Slide. 8.*

몬테 카를로 방법을 사용해서 몇개의 trajectory를 샘플링 했을 때 연속적인 state space에서 똑같은 곳을 다시 방문할 수는 없지만 아주 근사한 상태를 방문할 수는 있을겁니다. 그리고 뉴럴 네트워크라는 근사함수는 이러한 비슷한것을 한데 묶어서 생각할 수 있는, 그러니까 `Generalization`을 잘 하는 함수죠 (오버피팅 하겠지만, 좋은 네트워크는 잘 해야 합니다).


그러므로 우리가 가치 함수를 학습시킬 데이터는 현재 가지고 있는 정책으로 `simulation`을 돌려보고 (`roll out`) 얻은 경로를 정답으로 하는겁니다. (이를 데이터 삼아 사용함)

$$
V^{\pi} (s_t) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t'=t}^T r(s_{t'},a_{t'})
$$

$$
\text{training data : } (s_{i,t}, \underbrace{ \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) }_{ y_{i,t} } )
$$

그리고 이를 기반으로 `지도학습 (Supervised Learning)`을 하면,
Value Function은 출력이 (지금은 벡터가 아니라 `scalar`) 보상값이고 이 값은 연속적이기 때문에 가치 함수를 fitting하는 것은 출력 분포를 가우시안 분포로 모델링하여 얻은 아래의 `MSE loss`를 줄이는, 이른 바 `회귀 (Regression)` 문제를 푸는 것이 됩니다.

$$
L(\phi) = \frac{1}{2} \sum_i \parallel \hat{V}_{\phi}^{\pi}(s_i) - y_i \parallel^2
$$

여기서 이 `가치 함수 네트워크가 심하게 오버피팅을 하면` $$y_{i,t}$$를 직접적으로 가져다가 사용하는 Vanilla Policy Gradient와 비교해서 성능이 아주 안좋을 수 있지만, 일반화에 성공한다면 우리는 더 낮은 variance를 얻을 수 있습니다.



왜냐하면 직접적으로 $$y_{i_t}$$를 사용해 이를 평균내서 weighted-sum 하는 기존의 정책 경사 알고리즘은 비슷한 state에서의 비슷하지 않은 label들을 평균내서 사용하기 때문입니다. (?)








![slide9](/assets/images/CS285/lec-6/slide9.png)
*Slide. 9.*

하지만 아직도 부족합니다. 우리가 어떤 상태에서 직접 시뮬레이션을 끝까지 돌려서 얻은 보상값을 타겟으로 하지 말고 `True expected value of rewards`를 사용할 순 없을까요?

$$
\begin{aligned}

& 
y_{i,t} = \sum_{t=t'}^T \mathbb{E}_{\pi_{\theta}}[r(s_{t'},a_{t'}) \vert s_{i,t}]
& \scriptstyle{ \text{ideal target} } \\

&
y_{i,t} = \sum_{t=t'}^T r(s_{i,t'},a_{i,t'})
& \scriptstyle{ \text{Monte Carlo (MC) Traget (one sample)}} \\

\end{aligned}
$$


우리가 이전에 정의했던 것 처럼 큐 함수는 아래와 같이 current reward + 다음 스텝에서의 $$V^{\pi}$$로 표현할 수 있었습니다.

$$
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \underbrace{\sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]}_{V^{\pi}(s_{t+1})}
$$

이러한 사실을 이용해서 `idea target`을 아래와 같이 바꿀 수 있습니다.  

$$
\begin{aligned}
& 
y_{i,t} = \sum_{t=t'}^T \mathbb{E}_{\pi_{\theta}}[r(s_{t'},a_{t'}) \vert s_{i,t}]
& \scriptstyle{ \text{ideal target} } \\

&
\approx r(s_{i,t},a_{i,t}) + \sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{i,t'},a_{i,t'} \vert s_{i,t}]
& \\

&
\approx r(s_{i,t},a_{i,t}) + V^{\pi} (s_{i,t+1})
& \\

\end{aligned}
$$

이러한 타겟을 사용하는게 매 번 한번씩만 샘플링하는 몬테카를로 방법보다는 더 낮은 variance의 솔루션을 얻을 수 있게 해줄 수 있다고 합니다.

여기서 우리는 $$V^{\pi}$$를 모르기 때문에 이를 근사한 $$\hat{V}_{\phi}^{\pi}(s_{i,t+1})$$을 사용하게 됩니다.

$$
\begin{aligned}

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + V^{\pi} (s_{i,t+1})
& \\

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + \hat{V}_{\phi}^{\pi}(s_{i,t+1})
& \\

\end{aligned}
$$

이렇게 근사한 네트워크의 값을 쓰는게 당연히 베스트이진 않지만 없는거보다야 낫다고 합니다.
이를 `bootstrap estimator`라고 부르기도 한다고 합니다.


즉 이렇게 해서 가치 함수의 근사 네트워크를 학습하는 트레이닝 데이터는 아래와 같이 바뀝니다.

$$
\begin{aligned}

&
\text{training data : } (s_{i,t}, \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) )
& \\

&
\text{training data : } (s_{i,t}, r(s_{i,t},a_{i,t}) + \hat{V}_{\phi}^{\pi} (s_{i,t+1}) )
& \\

\end{aligned}
$$

우리는 $$\hat{V}^{\pi}_{\phi}$$ 네트워크가 부정확히지만 학습을 거듭할수록 실제 값 (Oracle Value) 에 가까워 지길 바랄 뿐입니다.
이제 마찬가지로 위의 트레이닝 데이터를 타겟으로 지도 학습을 하면 됩니다.


이러한 부트스트랩 기법(bootstrap estimator)은 한번 샘플링한것을 타겟으로 하는 것 보다 lower variance를 가지기 때문에 좋지만 이 근사 함수가 뱉는 값이 부정확할 것이기 때문에 bias는 클 수 있다고 합니다. (trade off)



![slide10](/assets/images/CS285/lec-6/slide10.png)
*Slide. 10.*

*Slide. 10.*는 몇 가지 예제와 함께 '대체 Policy Evaluation이 의미하는게 정확히 뭐야?' 에 대한 얘기를 합니다.


(TD-Gammon 과 Alphago 예시)












## <mark style='background-color: #fff5b1'> From Evaluation to Actor Critic </mark>

![slide12](/assets/images/CS285/lec-6/slide12.png)
*Slide. 12.*

Actor-Critic 알고리즘의 기본적인 `batch`는 위와 같이 구성되어 있는데요, 
근본적으로 앞서 배운 정책 경사 알고리즘인 `REINFORCE`와 크게 다르지 않습니다.

- rullout 해서 trajectory 샘플을 만든다
- 샘플들을 통해 얻은 reward로 $$\hat{V}^{\pi}_{\phi}(s)$$를 fit한다. (단순히 보상들을 다 합치는거에서 발전함)
- 샘플들을 통해서 Approximate Adavantage, $$\hat{A}^{\pi}(s_i,a_i)$$를 평가한다.
- 얻은 $$\hat{A}^{\pi}(s_i,a_i)$$를 통해서 Objective의 gradient를 구한다.
- 업데이트 한다.

여기서 우리가 앞서 논했던 `Policy Evaluation`는 2번째 스텝에 해당합니다.


```
이를 Actor-Critic 알고리즘이라고 부르는 이유는
어떤 상태에서 어떤 행동을 할 지를 결정하는 정책이 행동하는 모듈인 Actor 이라 하고,
과연 그 상태에서 현재 정책을 따르면 어떤 결과가 나오는지?
즉, 정책이 얼마나 좋은지를 평가하는 모듈이 Critic 이라 하기 때문입니다. 
```






### <mark style='background-color: #dcffe4'> Discount Factor </mark>

![slide13](/assets/images/CS285/lec-6/slide13.png)
*Slide. 13.*

그 다음으로 알아볼것은 `Discount Factor`인데요, 이것은 우리가 bootstrap estimator를 사용해서 가치 함수 네트워크를 근사할 때,
태스크가 `infinite horizon case`일 경우 어떻게 할 것이냐? 에 대한 솔루션 입니다.


만약 $$T \rightarrow \infty$$라면 가치함수가 굉장히 올라갈 겁니다.
물론 episodic case는 별로 문제가 되지 않을 수 있지만요.


가장 단순한 해결책은 바로 $$\gamma \in [0,1]$$인 factor를 도입하는 겁니다.


$$
\begin{aligned}

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + \hat{V}_{\phi}^{\pi}(s_{i,t+1})
& \\

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + \gamma \hat{V}_{\phi}^{\pi}(s_{i,t+1})
& \\

\end{aligned}
$$

사실 강화학습에서 discount factor에 대한 얘기는 많은 서적에서 제일 앞부분에 등장해서 강화학습 책을 한번이라도 접해보신 분들에게는 익숙하실텐데요, 이는 강화학습이 미래의 보상들 까지 다 고려해서 행동을 한다고 하지만, 그래도 현재 받는 보상에 가장 높은 weight을 부여해서 `미래를 고려하긴 하지만 그래도 현재에 집중해서 행동을 선택`하게 한다는 의미를 가지고 있습니다.


그리고 이러한 discount factor를 도입하는 것은 MDP의 결과를 바꾸기도 합니다. episode 횟수가 정해져있고 실제로 MDP를 따라서 얻는 보상의 값이 discount factor를 넣었을 때와 아닐 때가 다릅니다 (수학적으로). (다른 자료를 찾아보시길 바랍니다)


그리고 당연히 이 discount factor를 몬테 카를로 방법을 사용하는 일반적인 정책 경사 알고리즘에도 사용할 수 있는데요,
이는 *Slide. 14.*의 수식을 따라가시다보면 쉽게 확인하실 수 있습니다.

![slide14](/assets/images/CS285/lec-6/slide14.png)
*Slide. 14.*

*Slide. 14.*에서 보시면 option이 2가지가 있는데요. $$t=1 \sim T$$까지와 $$t=t' \sim T$$까지인 버전이 두개있습니다.

그리고 option2는 다시 `causality`를 적용한 뒤 $$\gamma$$를 $$\gamma^{t'-t},\gamma^{t-1}$$로 나누어 log probability와 reward에 각각 분배해줬습니다.

option2는 log probability에도 감가율이 적용되어 `에피소드 초기에 좋은 선택을 하는것이 미래에 좋은 선택을 하는것보다 더 영향력있고 중요하다` 는 의미를 내포합니다.


![slide15](/assets/images/CS285/lec-6/slide15.png)
*Slide. 15.*

여기서 어떤 버전이 더 나은지는 위의 슬라이드에 답으로 나와있습니다.
*Slide. 15.*에는 또한 사람 모양의 로봇이 최대한 멀리 걸어가는게 목표인 강화학습 task가 있는데요,
이러한 `cyclic task`에서 적합하지 않기 때문입니다.


이렇게 discount factor를 추가하는 것도 마찬가지로 알고리즘의 variance를 줄여줍니다.



![slide16](/assets/images/CS285/lec-6/slide16.png)
*Slide. 16.*

이제 마지막으로 `discount factor`까지 적용한 Actor-Critic Algorithm의 batch에 대해서 얘기하고 서브섹션을 마칠겁니다.
Discount factor가 적용된 부분은 3번 스텝이 되겠습니다.


그리고 *Slide. 16.*에는 `Online Actor-Critic Algorithm`이 있는데요, 이는 `Infinite horizon case`에 대해서 적용하기 위한 알고리즘이라고 생각하시면 됩니다.

이는 trajectory를 다 뽑아두고 학습하는 `Eposodic case`와 다르기 때문에 1,2번이 다른 걸 알 수 있습니다.


하지만 이렇게 이론적으로 잘 정의된 것 같은 알고리즘도 실제에서는 적용하기 어려운점이 많습니다.



## <mark style='background-color: #fff5b1'> Actor-Critic Design Decisions </mark>

### <mark style='background-color: #dcffe4'> NN Architecture decision for Actor-Critic </mark>

이제 Actor-Critic 알고리즘을 사용하기 위한 조금 practical한 테크닉들을 알아보도록 하겠습니다.

![slide18](/assets/images/CS285/lec-6/slide18.png)
*Slide. 18.*

우선 Vanilla Policy Gradient 방법론은 뉴럴 네트워크 하나가 policy를 모델링하기 때문에 굉장히 명확했는데요,
Actor-Critic은 지금 `policy와 Value function 두 가지`를 다 모델링 해야 하는 상황입니다.

가장 심플한 방법은 *Slide. 18.*의 좌하단 그림처럼 네트워크를 두개 두는거죠. (각각 파라메터 $$\phi,\pi$$로 구성됨)
하지만 이렇게 할 경우 두 네트워크가 서로 feature (representation)을 공유하는 일이 없다는 단점이 있습니다.


그렇기 때문에 또 다른 방법으로는 `shared network`를 두고 최종 출력 직전의 디코더만 두 개로 구성하는 전략이 있습니다.
이렇게 되면 `internal representation`을 공유하기 때문에 학습하기 더욱 용이할 것입니다. 

뭐 자율주행 같은 컴퓨터비전 task라면 쉐어링 되는 backbone network를 `ResNet` 같은 인코더를 사용하면 되겠네요.


하지만 이렇게 이론적으로 작용할것 처럼 보이는 두 번째 방법론도, 학습이 unstable하게 되는 문제가 있다고 하는데요, 왜냐하면 이는 소위 말하는 `Multi-task Learning`과 유사한 방법론인데, 이런 Multi-task Learning 에서는 두 task의 난이도가 다를 경우 적절하게 weight를 주지 않고 학습하게 되면 문제가 생기기 마련이기 때문입니다. (heuristic하게 하이퍼 파라메터를 정해야겠죠)





### <mark style='background-color: #dcffe4'> Parallel Actor-Critic </mark>

그 다음으로 Sergey교수님이 얘기하는 practical implementation은 *Slide. 19.*에 나와있는데요,

![slide19](/assets/images/CS285/lec-6/slide19.png)
*Slide. 19.*

바로 `batch size`에 관한 것입니다.
앞서 소개했던 온라인 batch방법은 1번 샘플링해서 가치 함수와 정책을 바로 업데이트 하는 데 사용하는 1 batch 방법론이었는데요, 이는 알고리즘 내에 Value Function을 넣는 등의 방법으로 Variance를 효과적으로 줄이긴 했으나 여전히 high variance를 가지고 있기 때문에 Deep RL 알고리즘들에서 좋지 않은 성능을 보인다고 합니다.


`Online Gradient Descent (Stochastic GD, SGD)`로 최적화를 하는건 일반적인 지도학습 방법론을 사용하는 딥러닝 네트워크들에서도 좋지 않는 성능을 보이기 때문에 Deep RL에서는 더한것이죠.



그래서 지금 논할 방법은 심플하게 매 번 policy와 value function을 업데이트하지 말고 모아서 업데이트 하는 겁니다. 
하지만 이러한 방법도 마냥 좋지는 않은데요, 왜냐하면 이렇게 여러개 샘플링해서 업데이트를 하려고 해도 이런 샘플들이 다 비슷할거기 때문이죠 (highly correlated). 그니까 지금 본 state가 다음 state랑 굉장히 비슷한겁니다.


이를 해결하기 위한 좋은 방법으로 Sergey 교수님은 `Parallel Worker`를 사용하는 방법을 이야기합니다.
이는 `Synchronized Parallel Actor-Critic Algorithm`을 디자인 하는것으로 아래의 그림을 보시면 이해가 빠른데요,

![asynchronous1](/assets/images/CS285/lec-6/asynchronous1.png){: width="70%"}

위의 그림을 보시면 worker가 4개이고 4개는 각기 다른 랜덤 시드를 가질것이기 때문에 매우 다른 상황에 놓여 있을 겁니다.
이런 샘플들을 모아서 하나의 큰 배치로 생각하고 모든 업데이트가 끝날 때 까지 기다린 다음 다시 또 진행하고 모으고 가치,정책 업데이트하고 ...를 반복하는 겁니다.


또 다른 방법은 우리가 굉장히 많은 worker를 가지고 있을 때 더 효율적으로 쓸 수 있는 `Asynchronized` 알고리즘을 디자인 하는 겁니다.

![asynchronous2](/assets/images/CS285/lec-6/asynchronous2.png){: width="70%"}

이 알고리즘이 위와 다른 것은 central parameter가 worker가 샘플링한 데이터들을 기반으로 업데이트가 되고, worker들이 기다리지 않으면서 계속 샘플을 하며 전진한다는 겁니다. 과연 어떻게 그게 가능할까요? 네트워크 입장에서는 현재 최신의 정책을 이용해 샘플링한게 아닌 데이터를 받아들일 때도 있는 문제 등이 수있습니다. 우리가 지금까지 논한 정책 경사 기반 알고리즘들이 다 `on-policy`인 것을 가정하고 생각한건데 이치에 맞지 않습니다. 이는 사실 worker나 네트워크 입장에서 그다지 오래된 정책이나 데이터가 쓰이는게 아니기 때문에 큰 문제 없이 실제에서는 꽤 잘 작용하나 이러한 이슈들 때문에 학습이 unstable하다고 합니다.



***

이렇게 비동기식 (asynchronous ; 동시에 발생하지 않는) 방식으로 작동하는 알고리즘은 2016년에 제안된 Deepmind의 논문 [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783) 에서 제안되었으며 `Asynchronous Advantage Actor-Critic, A3C`라고 불리고, 이 A3C의 synchronous version으로 먼저 말씀드린 방법을 `Advantage Actor-Critic, A2C` 라고 합니다.

더 궁금하신 분들은 논문을 참조해주세요.

(그나저나 정말 어디에나 계시는군요 [Alex Graves](https://www.cs.toronto.edu/~graves/)...)

***







## <mark style='background-color: #fff5b1'> Improved Actor-Critic </mark>

### <mark style='background-color: #dcffe4'> Critics as Baselines </mark>

이번에는 Critic을 다른 방식으로 쓸 수 있는 방법에 대해서 알아보게 됩니다.


![slide21](/assets/images/CS285/lec-6/slide21.png)
*Slide. 21.*

*Slide. 21.* 에는 그동안 알아봤던 `Actor-Critic Algorithm`과 `Vanilla Policy Gradient Algorithm`이 있습니다. 
사실 두 알고리즘은 크게 다르지 않은데요, 어떤 time step에서의 log probabilitiy 에 (정책을 따라 한번 끝까지 가봤을때의 보상 - baseline(단순 평균이죠?) ) 을 곱하는 것이 단순 Policy Gradient 였다면, log prob에 곱해지는 값을 (Q-V) 라는 정책을 평가하는 Critic을 도입한게 Actor-Critic입니다.

Critic을 도입함으로써의 장점은 어떤 time step에서의 gradient of log probability를 계산하는 것에 한번 끝까지 가보는 것이 아니라 expectation을 통해 가능한 정확하게 측정한 가치를 곱해주기 때문에 variance가 훨씬 낮다는 건데요, 단점으로는 `더이상 이 알고리즘이 unbiased 하지 않다`는 겁니다. 이는 실제 Value Function을 근사한 뉴럴 네트워크가 구릴 수 있기 때문입니다. (즉 실제 가치 함수와 유사하지 않을 수 있는거죠.) 반대로 일반적인 정책 경사 알고리즘은 Critic 없이 Objective가 전개되기 때문에 variance가 높기 때문에 샘플이 더 많이 필요하거나 learning rate가 작아야 하는 단점이 있는 반면 bias는 적다는 장점이 있습니다. (머신러닝의 일반적인 Bias-Variance Trade-off 인가..?)


여기서 key question은 바로 `Critic을 다른 방법으로 써서 unbiased하면서도 variance는 좀 낮은 좋은 성능의 알고리즘을 만들 순 없을까?`입니다.


우리가 앞서 6장에서 알 수 있었던 것은 `baseline`을 빼는 연산이 estimator를 계속 unbiased하게 두는데 이 baseline이 어떤 scalar가 아닐 때에 action이 아닌 state에만 관련된 함수일 경우도 마찬가지로 bias를 형성하지 않는다는 것이었습니다. 

```
Variance 직접 계산해보라고 하네요
```

*Slide. 21.*의 맨 아래 수식을 보시면 간단하게 Critic을 사용해 variance를 조금 낮춘 Objective (Actor-Critic 보단 아니지만) 를 얻을 수 있다는 걸 알 수 있습니다.


하지만 아마 글을 읽으시는 분들께서도 V함수를 action, state가 모두 condition된 Q함수로 대체해보고 싶은 욕구가 들겁니다
(이러면 그냥 Actor-Critic 아닐까요? 하는 생각과 함께...). 
*Slide. 22.*에는 이런 아이디어를 반영해서 baseline으로 사용하는, 조금은 복잡한 방법이 나와있습니다.

![slide22](/assets/images/CS285/lec-6/slide22.png)
*Slide. 22.*

다시 Q,V,A를 써봅시다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
V^{\pi} (s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)]
& \scriptstyle{\text{ total reward from } s_t}  \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t \text{ is}} \\

\end{aligned}
$$

`action, state dependent baseline`을 쓰는 방법은 일반적으로 `control variants`라고 합니다.
Policy Gradient에서 사용하던 approximate advantage는 `state dependent baseline`를 사용할 때 아래와 같았습니다.

$$
A^{\pi} (s_t,a_t) = \sum_{t'=t}^{\infty} \gamma^{t'-t} r(s_{t'},a_{t'}) - V_{\phi}^{\pi}(s_t)
$$

여기서 V함수 대신 Q함수를 근사해서 사용하면 아래와 같이 됩니다.

$$
A^{\pi} (s_t,a_t) = \sum_{t'=t}^{\infty} \gamma^{t'-t} r(s_{t'},a_{t'}) - \color{red}{Q_{\phi}^{\pi}(s_t,a_t)}
$$

이럴 경우 `Critic`이 정확해질 수록 미래에 선택할 행동 (future behavior)들이 랜덤이 아닐 확률이 굉장히 높아지고 이 quantity (??)는 0으로 수렴하게 됩니다.
하지만 불행하게도, 이렇게 정의한 Advantage Estimator를 정책 경사 알고리즘에 끼워넣게 되면 gradient가 더이상 정확해지지 않게 되는데 왜냐하면 error term이 하나 생기기 때문입니다.

$$
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ( \hat{Q}_{i,t} - Q_{\phi}^{\pi} (s_{i,t},a_{i,t}) ) \\
+ \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} \mathbb{E}_{a \sim \pi_{\theta} (a_t \vert s_{i,t} )} [ Q_{\phi}^{\pi} (s_{i,t},a_t) ]
$$

즉 만약 우리가 baseline으로 state only dependent한 term이 아니라 actin, state에 모두 dependent한 term을 사용하면 위의 수식에서 두 번째 항이 더이상 0가 아니게 됩니다.

위의 수식에서 첫 번째 term은 그저 baseline을 사용한 Policy Gradient이며, 두 번째 term은 현재 정책 하의 Value에 대한 기대값의 gradient로 baseline에 대한 미분값을 나타냅니다. 

첫 번째 term은 학습을 할수록 수렴할테니 0에 가까워질 것이지만 두 번째 term은 꽤 커보일 수 있지만 대부분의 경우에 기대값을 꽤 정확하게 구해낼 수 있으며 이 term이 가지는 variance는 0이거나 0에 가깝게 된다고 합니다. 즉 첫 번째 term은 $$\hat{Q} - Q$$로 굉장히 작아질것이고, 두 번째 term도 굉장히 variance가 작은 어떤 값이 되므로 (continuous action space (가우시안 같은 분포) 이거나 discrete action space일 경우 모두 문제가 안됨) 우리는 action, space 두 가지에 dependent한 baseline을 쓰고도 variance는 줄이면서 bias도 없는 Objective를 얻은 것입니다.

***
더 궁금하신 분들은 [Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic](https://arxiv.org/pdf/1611.02247)을 참조하시면 좋을 것 같습니다.
***











### <mark style='background-color: #dcffe4'> All at Once : Policy Optimization RL Algorithms </mark>

여태까지 Vanilla Policy Gradient부터 배운것을 잠깐 정리하고 넘어가도록 하겠습니다.  

***

$$
\begin{aligned}

&
\rightarrow \text{ Vanilla Policy Gradient }  
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \sum_i (\sum_{t=1}^T) \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (\sum_{t'=t}^T r(s_{i,t},a_{i,t}))
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \sum_i (\sum_{t=1}^T) \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t})   \color{red}{Q(s_{i,t},a_{i,t}))}
& \\


&
\rightarrow \text{ Vanilla Policy Gradient with Baseline and Discount Factor}  
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ( (\sum_{t=t'}^T \gamma^{t'-t} r(s_{i,t'},a_{i,t'})) - \color{red}{b} )
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \color{red}{ \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t}) } )
& \\


&
\rightarrow \text{ Actor-Critic}  
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \color{red}{ \hat{A}^{\pi} (s_{i,t},a_{i,t}) } )
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \color{red}{ ( r(s_{i,t},a_{i,t}) + V_{\phi}^{\pi}(s_{i,t}') - V_{\phi}^{\pi}(s_{i,t}) ) }
& \\

&
\rightarrow \color{blue}{V \space Function} \text{ as Baseline}  
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ( (\sum_{t=t'}^T \gamma^{t'-t} r(s_{i,t'},a_{i,t'})) - \color{red}{ \hat{V}_{\phi}^{\pi} (s_{i,t}) } )
&  \\

&
\rightarrow \color{blue}{Q \space Function} \text{ as Baseline}  
& \\
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ( (\sum_{t=t'}^T \gamma^{t'-t} r(s_{i,t'},a_{i,t'})) - \color{red}{ \hat{Q}_{\phi}^{\pi} (s_{i,t},a_{i,t}) } )
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) ( \hat{Q}_{i,t} - Q_{\phi}^{\pi} (s_{i,t},a_{i,t}) ) + \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} \mathbb{E}_{a \sim \pi_{\theta} (a_t \vert s_{i,t} )} [ Q_{\phi}^{\pi} (s_{i,t},a_t) ]
& \\

\end{aligned}

$$

***






### <mark style='background-color: #dcffe4'> N-Step Returns </mark>

그 다음으로 살펴볼 것은 `Eligibility Traces`와 `N-Step Returns`인데요,


![slide23](/assets/images/CS285/lec-6/slide23.png)
*Slide. 23.*

먼저 Actor-Critic에 사용되는 Advantage Estimator를 아래와 같이 정의합니다.

$$
\hat{A}_{\color{red}{C}}^{\pi} (s_t,a_t) = r(s_t,a_t) + \gamma \hat{V}_{\phi}^{\pi} (s_{t+1}) - \hat{V}_{\phi}^{\pi} (s_{t}) 
$$

$$\hat{A}_C^{\pi}$$ 는 현재 정책 $$\pi$$를 따랐을 때 현재 보상 $$r(s_t,a_t)$$에 다음 상태의 가치와 현재 상태의 가치를 뺀 값을 더한 것으로 정의되었죠.
여러번 말하지만, Actor-Critic은 Variance가 적은 대신에 Value가 조금이라도 틀리면 완전히 틀어져버릴 수 있는 High Bias인 방법론 이었습니다.

Monte Carlo (MC) 방법을 사용하고 Baseline은 Value Function으로 사용한 경우는 

$$
\hat{A}_{\color{red}{MC}}^{\pi} (s_t,a_t) = \sum_{t=t'}^{\infty} \gamma^{t'-t} r(s_{t'},a_{t'}) - \hat{V}_{\phi}^{\pi} (s_{t}) 
$$

위와 같은데요, 이는 샘플링을 한번만 해서 사용하기 때문에 Variance는 크지만 Low Bias인 방법론 이었습니다. 

이 두가지를 다시 쓴 이유는 바로 `bias/variance tradeoff를 컨트롤하기 위해서 두 가지를 적절히 섞을 수 있는 방법은 없을까?`에 대해서 생각해보기 위해서 입니다.

Key Idea는 "시간이 흐를수록 variance가 커질 가능성이 높으니 가까운 시점, 예를들어 $$t$$ 부터 $$t+5$$ 까지는 $$A_{MC}$$로, 그 뒤부터는 variance가 크니 $$A_C$$로" 하자는 겁니다. 

```
Sergey :
"여러분 제가 5분 뒤에 어디에 있을지 생각해보세요, 아마 버클리에 있을 확률이 높겠죠? 이거는 몇번 샘플링 해봐도 위치에 큰 차이가 없을거에요,
하지만 20년 뒤에 어디있을지를 샘플링 해보면 이야기가 다릅니다."
```

그렇게 제안된 `N-Step Returns Advantage Estimator`는 아래와 같이 쓸 수 있습니다.

$$
\hat{A}_{\color{red}{n}}^{\pi} (s_t,a_t) = \sum_{t=t'}^{t+n} \gamma^{t'-t} r(s_{t'},a_{t'}) - \hat{V}_{\phi}^{\pi} (s_{t}) 
$$

$$A_{MC}$$와 굉장히 비슷해보이지만 $$\infty$$까지 해보는 것이 아니라 n번만 해본다는 차이가 있습니다. 여기서 $$n \sim \infty$$는 앞서 말한 것 처럼 Value Function으로 대체하게 되므로, 나머지 term을 더해주면 아래와 같이 됩니다.

$$
\hat{A}_{\color{red}{n}}^{\pi} (s_t,a_t) = \sum_{t=t'}^{t+n} \gamma^{t'-t} r(s_{t'},a_{t'}) - \hat{V}_{\phi}^{\pi} (s_{t}) + \color{blue}{ \gamma^n \hat{V}_{\phi}^{\pi} (s_{t+n}) }
$$

여기서 n은 1보다 큰 값으로 정하는게 좋고 일반적으로 n이 커질수록 마지막 term인 Value Function앞에 곱해지는 $$\gamma^n$$ 덕분에 마지막 term값이 작아지면서 bias가 작아지지만 variance가 커지고 n이 작을 때는 그 반대가 됩니다.

(`첫 번째 텀이 variance를 컨트롤하고, 마지막 term이 bias를 컨트롤 하는 셈이 되는거죠`)

![slide24](/assets/images/CS285/lec-6/slide24.png)
*Slide. 24.*

마지막으로 살펴볼 것은 2015년에 제안된 방법론인 `Generalize Advantage Estimation (GAE)` 으로 앞서 말한 N-Step 방법을 일반화 한 것입니다.

(논문 : $$\rightarrow$$ [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438))

간단하게 얘기해서 N-Step Return이 좋은 알고리즘이긴 하나 어떻게 n을 설정할지가 까다롭기 때문에 $$n=1 \sim \infty$$ 까지 weighted-sum 하자는 아이디어 입니다.

$$
\begin{aligned}

&
\hat{A}_{n}^{\pi} (s_t,a_t) = \sum_{t=t'}^{t+n} \gamma^{t'-t} r(s_{t'},a_{t'}) - \hat{V}_{\phi}^{\pi} (s_{t}) + \gamma^n \hat{V}_{\phi}^{\pi} (s_{t+n})
& \\

&
\hat{A}_{\color{red}{GAE}}^{\pi} (s_t,a_t) = \sum_{n=1}^{\infty} w_n \hat{A}_{n}^{\pi} (s_t,a_t)
& \\

\end{aligned}
$$

최종적인 수식은 아래와 같이 됩니다.

$$
\begin{aligned}
&
\hat{A}_{\color{red}{GAE}}^{\pi} (s_t,a_t) = \sum_{n=1}^{\infty} (\gamma \lambda)^{t'-t} \color{blue}{\delta_{t'}}
& \\

&
\text{where } \color{blue}{\delta_{t'}} = r(s_{t'},a_{t'}) + \gamma \hat{V}_{\phi}^{\pi}(s_{t'+1}) - \hat{V}_{\phi}^{\pi} (s_t)
& \\

\end{aligned}
$$








## <mark style='background-color: #fff5b1'> Review, Examples, and Additional Readings </mark>

마지막 서브섹션은 역시나 리뷰와 데모 비디오를 동반한 몇가지 예제들인데요, (비디오는 강의 참조)

![slide26](/assets/images/CS285/lec-6/slide26.png)
*Slide. 26.*

Actor-Critic Algorithm은 기본적으로 policy를 `Actor`, 그리고 policy가 얼마나좋은가?를 나타내는 value function을 `Critic`으로 하죠. 그리고 이렇게 Critic을 도입함으로써 Vanilla PG 대비 variance를 많이 줄였습니다.

그리고 Critic으로 여러가지를 사용했던 것과, `discount factor`를 사용해서 variance를 더욱 줄였었고,

네트워크를 Critic, Actor를 위해 아예 독립적으로 두개 구성하거나, `shared network`를 둬서 internal representation
을 공유하는 방식으로 네트워크를 구성할 수 있었고, 마지막으로 trajectory를 미리 샘플링해두고 이를 기반으로 학습한는 `batch-mode`와 `online mode`에 대해서 알아봤으며 온라인 모드를 특히 `parallel`하게 구성하기도 하였습니다.

그리고 Critic을 baseline으로 사용하는 방법과 n-step return  `GAE` 까지 알아봤네요.







### <mark style='background-color: #dcffe4'> Examples </mark>

이하 *Slide. 27.*~*Slide. 28.*는 비디오 데모가 포함되어 있기 때문에 강의를 참조하시길 추천드리며, 마지막 슬라이드는 추천할만한 Actor-Critic Based 논문들 입니다.

![slide27](/assets/images/CS285/lec-6/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-6/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-6/slide29.png)
*Slide. 29.*



***

이제 아래의 `Deep RL Taxonomy`에서 Policy Gradient based Algorithm은 거의 커버를 한 것 같네요, 

![rl_taxonomy_intellabs_for_lec6](/assets/images/CS285/lec-6/rl_taxonomy_intellabs_for_lec6.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec6](/assets/images/CS285/lec-6/rl_taxonomy_openai_for_lec6.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))

여기서 TRPO, PPO는 제대로 Cover하지 않았는데요 (TRPO는 5장에서 짧게 설명했지만), 두 방법론은 모두 "PG를 통해 Policy를 업데이트하고 싶은데, 너무 크지 않게 한스텝 업데이트 할 수는 없을까?"라는 생각에서 제안된 정책 경사 알고리즘들이며, 상당히 실전에서 잘 작동하는 알고리즘이라고 합니다. 앞으로의 강의에서 다룰지 모르겠으니 따로 관심 있으신 분들은 원본 논문을 찾아보시는게 좋을 것 같습니다.

- TRPO : [2015, Trust Region Policy Optimization](https://arxiv.org/pdf/1502.05477)
- PPO : [2017, Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)


***


Lecture 7부터는 `Value Function Methods`와 2013년 Atari게임을 정복한 딥마인드의 `Deep Q Learning (DQN)`에 대해서 얘기하게 될 것 같습니다.


끝까지 봐주셔서 감사합니다.



## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [강화학습 알아보기(4) - Actor-Critic, A2C, A3C from 김환희님](https://greentec.github.io/reinforcement-learning-fourth/)













