---
title: Lecture 6 - Actor-Critic Algorithms

categories: CS285
tag: [RL]

toc: true
toc_sticky: true

comments: true
---


이 글은 UC Berkeley 의 심층 강화 학습 (Deep Reinforcement Learning) 강의인 [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)를 듣고 기록하기 위해 작성한 글 입니다. 
강의 자료가 잘 구성되어 있으며, 강화학습 분야의 세계적인 석학인 [Sergey Levine](http://people.eecs.berkeley.edu/~svlevine/)의 강의 흐름을 그대로 따라가는게 낫겠다고 생각하여 슬라이드들을 그대로 사용해서 글을 전개하려고 합니다. (강의를 들으면서 가능하다면 이해를 돕기 위해 추가 자료를 중간 중간 첨부할 예정입니다.)


Lecture 6의 강의 영상과 자료는 아래에서 확인하실 수 있습니다. 
- [Lecture Video Link (Youtube)](https://www.youtube.com/watch?v=wr00ef_TY6Q&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=23)
- [Lecture Slide Link](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)


---
< 목차 >
{: class="table-of-content"}
* TOC
{:toc}
---


![slide1](/assets/images/CS285/lec-6/slide1.png)
*Slide. 1.*

Lecture 5 에서는 정책 경사 알고리즘 (Policy Gradient Algorithm, PG)에 대해서 알아봤습니다.
이번에는 PG에 기반한 `Actor-Critic Algorithm`에 대해서 알아보도록 할 것입니다.


시작하기에 앞서 아래의 `Deep Reinforcement Learning Taxonomy`을 보시면 도움이 될 것 같습니다.

![rl_taxonomy_intellabs_for_lec5](/assets/images/CS285/lec-6/rl_taxonomy_intellabs_for_lec5.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec5](/assets/images/CS285/lec-6/rl_taxonomy_openai_for_lec5.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))


Taxonomy에 보시면 Actor-Critic 알고리즘에는 이를 발전시킨 `Advantage Actor-Critic, A2C`, 하나의 에이전트가 아닌 여러 에이전트로 부터 학습하는 비동기식 알고리즘인 `Asynchronous Advantage Actor-Critic, A3C` 등도 있습니다.




## <mark style='background-color: #fff5b1'> Recap </mark>

지난 Lecture 5에서 우리는 `정책 경사 알고리즘 (Policy Gradient Algorithm, PG)`에 대해 배웠습니다.

![slide2](/assets/images/CS285/lec-6/slide2.png)
*Slide. 2.*

PG는 `Trajectory`를 여러번 샘플링한 뒤 이를 통해 환경과 상호작용하면서 policy를 `directly 업데이트`하죠.
(각 Trajectory마다 매 time-step에서의 정책의 log-probability와 그 시점부터 끝날시점 T까지의 보상의 합을 다 더한 것을 weighted-sum 한 수식이었죠.)


지난 강의에서 `보상 (reward to go)`을 의도적으로 $$Q^{\pi} (x_t,u_t)$$ 라고 정의했었던 것도 기억이 나실 겁니다.


그리고 PG의 직관적인 이해는 곧 `많은 보상을 초래할 것 같은 행동은 더욱 잘 일어나게끔, 그 반대는 덜 발생하게끔` 하는 것이었습니다.

![slide3](/assets/images/CS285/lec-6/slide3.png)
*Slide. 3.*

$$Q^{\pi} (x_t,u_t)$$에 대해서 조금 더 얘기해보자면 이는 `어떤 상태에서 어떤 행동을 했을 때 이 행동과 policy를 따라서 끝까지 가 봤을때 얻을 보상`이라고 할 수 있습니다.

이 보상함수를 평가할 수 있는 더 좋은 방법이 있을까요?

![additive1](/assets/images/CS285/lec-6/additive1.png){: width="60%"}

그것은 바로 어떤 시점 $$t'$$ 에서의 액션을 평가할 때 현재 가지고있는 정책을 가지고 여러번 끝까지 가보고 (샘플링) 이를 보상값으로 쓰는겁니다. 

![additive2](/assets/images/CS285/lec-6/additive2.png){: width="60%"}

이는 정책과 MDP에는 랜덤성 (randomness)를 가지고 있기 때문에 그렇습니다. 

$$ 
\begin{aligned}
& \hat{Q_{i,t}} \approx \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) & \\
& \hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ] & \\
\end{aligned}
$$ 

이러한 문제점은 앞서 Lecture 5에서 살펴봤던 것들처럼 정책 경사 알고리즘의 문제점인 `High Variance` 와도 직접적으로 관련이 있는데요, 여러번 샘플링한 것을 reward to go로 사용할수록 Variance는 줄어들고 한번 사용하면 Variance는 굉장히 큽니다. 
그리고 당연히 Full Expectation을 계산할 수 있으면 Variance는 엄청나게 줄어들겁니다.


그러니까 우리가 만약

$$
\hat{Q_{i,t}} \approx \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$

같은 근사 Q-Function이 아니라 진짜 (`True Q-Function`)에 

$$
Q(s_t, a_t) = \sum_{t'=1}^T \mathbb{E}_{\pi_{\theta}} [ r(s_{t'},a_{t'}) \vert s_t,a_t ]
$$

수 있다면 좋지 않을까요? ($$\approx$$ 가 아니라 $$=$$ 인 것에 주의)

그렇다면 Objective는 아래처럼 다시 쓸 수 있게 됩니다.

$$
\begin{aligned}
& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \hat{Q_{i,t}} & \\

& \bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i-1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) Q(s_{i,t},a_{i,t}) & \\

\end{aligned}
$$

즉 더이상 우변의 reward to go가 샘플링을 통해서 얻은 값이 아닌 그 해당 $$s_t$$에서의 가능한 수를 모두 탐색해본 `Full Expectation`이 되는 겁니다.


그렇다면 지난 Lecture 5에서 배운 것 처럼 baseline을 `True Q-Function`에도 적용할 수 있을까요?



![slide4](/assets/images/CS285/lec-6/slide4.png)
*Slide. 4.*

답은 '당연하다' 입니다.


(6장은 오피셜 pdf가 굉장히 수식이 겹쳐서 안보이는 경우가 많네요, 최대한 latex으로 다시 쓰겠습니다.) 


5장에서 `baseline` $$b$$를 `average reward` 로 설정하는게 꽤 괜찮다고 했었으니 그대로 쓰겠습니다.
True Q-Function을 쓰면 아래와 같이 나타낼 수 있겠네요.

$$
b_i = \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t})
$$

그리고 이를 도입한 `Gradient of Objective`는 아래와 같습니다.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - b)
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t}))
& \\
\end{aligned}
$$

이렇게 도입된 baseline을 기준으로 이를 넘는 결과를 가져오는 행동은 독려하고 아닌 행동의 확률은 줄이는 것이 정책 경사 알고리즘의 Variance를 더욱 줄여주게 됩니다.
여기서 baseline은 사실 bias를 야기하는 action $$a_t$$이 아닌 state $$s_t$$에만 의존할 수 있는데요, 
바로 이 state에만 의존하는 (depend) 수식으로 최고의 선택은 `가치 함수 (Value Function)`가 될 수 있겠습니다. (optimal은 아니라고 하네요)

$$
V(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta} (a_t \vert s_t)} [Q(s_t,a_t)]
$$

가치함수는 어떤 상태 $$s_t$$에서 선택 가능한 행동들에 대해서 모두 해보고 그 보상들을 합치는 (정확히는 기대값) 입니다.
즉 가치함수를 baseline으로 고르는 것은 굉장히 현명한 선택인거죠.

$$
\begin{aligned}
&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - \frac{1}{N} \sum_i Q(s_{i,t}, a_{i,t}))
& \\

&
\bigtriangledown_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \bigtriangledown_{\theta} log \pi_{\theta} (a_{i,t} \vert s_{i,t}) (Q(s_{i,t},a_{i,t}) - V(s_{i,t}))
& \\
\end{aligned}
$$

이는 사실 굉장히 중요한 점을 시사하는데요, 위의 수식에서 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$ 텀이 log probability에 곱해짐으로써 `어떤 상태에서, 어떤 행동을 선택 하는 것이 모든 행동을 고려했을 때의 평균 값 보다 얼마나 좋은가?`를 나타내기 때문입니다.


이만큼 중요한 의미를 가지는 $$(Q(s_{i,t},a_{i,t}) - V(s_{i,t}))$$를 우리는 `special term`으로 나타내는데, 이것이 바로 `Advantage Function` 입니다.


자 이제 `State Value Function`과 `State-Action Value Function`에 대해서 조금 더 얘기해보도록 하겠습니다. (State-Action Value Function을 Q라고 할 수도 있으나 완전히 같은 것은 아니라고 하네요 (?))


![slide5](/assets/images/CS285/lec-6/slide5.png)
*Slide. 5.*

앞서 다 얘기한 것이지만 다시 정의하자면, State-Action Value Function은 우리가 $$s_t$$에서 행동 $$a_t$$를 취했을 때 이후 현재 가지고 있는 정책을 통해 에피소드가 끝날때까지 돌려보고 매 step마다의 보상값들을 전부 더한겁니다. 

$$
Q(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]
$$

큐 함수를 아래와 같이 윗첨자에 $$\pi$$를 추가하여 표시하곤 하는데요,

$$
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t}
$$

이는 큐 함수가 정책 $$\pi$$에 의존한다는걸 강조하기 위한 겁니다.
그러니까 가능한 모든 정책이 각각의 다른 큐 함수를 가지고 있다는 거죠.


$$
V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi_{\theta}(a_t \vert s_t)} [Q^{\pi}(s_t,a_t)] \scriptstyle{\text{ total reward from } s_t}
$$

가치 함수는 이러한 큐함수에 기대값을 취해서 어떤 상태 $$s_t$$에서 가능한 행동 옵션들을 취했을 때의 큐함수를 전부 더한겁니다 (정확히는 그 행동을 취할 확률까지 곱한 기대값). 


마지막으로 우리는 Advantage Function을 아래와 같이 정의할 수 있었습니다.

$$
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) \scriptstyle{\text{ how much better } a_t is}
$$


다시 셋을 정리하면 아래와 같습니다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)
& \scriptstyle{\text{ how much better } a_t is} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t is} \\

\end{aligned}
$$

***

이 셋은 RL에서 굉장히 중요한 `Quantity` 이므로 직관적으로 감이 안오신다면 이쯤에서 잠시 끊고 깊게 생각해 본 뒤  넘어가셔도 좋을 것 같습니다.

***

하지만 당연히 실제 알고리즘이 동작할 때 우리는 Advantage Function의 `정확한 값 (correct value)`를 알 수 없습니다. 
그래서 우리는 이 함수를 근사해서 사용하게 될텐데요, 이 근사 함수가 원본에 가까울수록 variance가 낮아집니다.


이제 아래의 Anatomy를 다시 보시면,

![additive3](/assets/images/CS285/lec-6/additive3.png){: width="70%"}
*Fig. Anatomy of Deep RL *

녹색박스가 좀 더 디테일해졌음을 알 수 있습니다. 
이 녹색부분인 `Fitting Value Function` 부분을 좀 더 살펴보도록 하겠습니다.

![slide6](/assets/images/CS285/lec-6/slide6.png)
*Slide. 6.*

```
fit what to what?
```

$$Q,V,A$$ 각각을 어떤 타겟에 피팅시켜야할까요?


다시 세 가지를 적고요

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \scriptstyle{\text{ total reward from taking } a_t \text{ in } s_t} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)
& \scriptstyle{\text{ how much better } a_t is} \\

&
A^{\pi} (s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t) 
& \scriptstyle{\text{ how much better } a_t is} \\

\end{aligned}
$$


여기서 우선 Q는 $$s_t,a_t$$가 랜덤 변수가 아니기 때문에 아래와 같이 수식을 다시 쓸 수 있다고 하는데요, (같은 수식입니다, 한스텝 전개했을 뿐)

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = \sum_{t'=t}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

\end{aligned}
$$

여기서 우변의 두 번째 항은 아래와 바꿀 수 있습니다.. 


$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t] 
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \underbrace{\sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]}_{V^{\pi}(s_{t+1})}
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})
& \\

\end{aligned}
$$

즉 Q 함수를 `current reward` + `expected value of the reward of the value function of the next time step`로 나타낼 수 있는겁니다.

하지만 여기에는 Vanilla Policy Gradient 에는 필요 없었던 `Transition Probability`가 포함되어 있습니다.


여기서 우리가 원하는 진짜 값인 $$\mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})$$ 를 구해내기는 어려우니, 여기에 근사 (approximation) 가 한번 들어갑니다.

$$
\begin{aligned}
&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)}V^{\pi}(s_{t+1})
& \\

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + V^{\pi}(s_{t+1})
& \\

\end{aligned}
$$

위의 수식이 의미하는 바는 next-time step의 state에 대한 분포를 이전에 해왔던 것 처럼 `a single sample estimator`로 딱 한번 샘플링 하는것으로 근사하는겁니다. 
하지만 주의해야 할 점은 딱 그 한스텝만 한번 샘플링하는것이고 그 뒤로는 여전히 모든 값을 고려하는 Expectation을 계산한다는 겁니다.

물론 이런 근사를 한 결과값은 당연히 정확한 값과는 거리가 있겠고, variance도 조금 더 높습니다.


하지만 이렇게 하는데에는 이유가 있는데요,

바로 Q함수를 근사한 덕분에 `Advantage Function`을 아래와 같이 표현할 수 있기 때문입니다.


$$
\begin{aligned}

&
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + V^{\pi}(s_{t+1})
& \\

&
A^{\pi}(s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_{t})
& \\ 

&
A^{\pi}(s_t,a_t) = r(s_t,a_t) + V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})
& \\ 

\end{aligned}
$$

이렇게 얻은 수식이 가져다주는 이점은 바로 수식이 $$V^{\pi}$$에 전적으로 의존하게 된다는 건데요,
이렇게 V를 학습하는 것이 Q와 A를 근사하는 것보다 쉽다고 합니다. 
왜냐하면 Q와 A는 $$s_t,a_t$$ 둘 다 필요하지만, `V는 state만 있으면` 되기 때문입니다.


즉, 근사함수가 의존하는 factor가 하나 더 적기 때문에 더 적은 샘플로 다음 state를 예측하는 V함수를 찾는건 할만 하다는 겁니다.

(물론 이 강의를 좀 더 진행하고난 뒤에는 $$Q^{\pi}$$를 직접 근사하는 방법론에 대해서도 살펴볼 것이라고 합니다.)


한편, 이 분야에서 `Universal Function Approximator`로 잘 쓰는 것이 있는데요, 그것은 바로

![additive4](/assets/images/CS285/lec-6/additive4.png){: width="70%"}
*Fig. Neural Netowrk as Universal Function Approximator*

`뉴럴 네트워크 (Neural Network, NN)` 입니다.
그리고 이 네트워크는 $$\phi$$로 파라메터화 되어있습니다.
(주의 : 정책은 $$\pi$$로 파라메터화 되어있음)


이제 $$V^{\pi}(s)$$ 를 근사시켜봅시다.

![slide7](/assets/images/CS285/lec-6/slide7.png)
*Slide. 7.*

이 $$V^{\pi}(s)$$를 피팅하는 과정은 일반적으로 `Policy Evaluation`이라고도 하는데요,
그 이유는 $$V^{\pi}(s)$$가 매 state마다의 policy의 가치 (얼마나 좋은가?) 를 계산하기 때문입니다.


그리고 지난 강의에서 초기 상태인 $$s_1$$의 가치 함수의 기대값이 바로 Objective와 동일하고, 이를 최대로 하는 것이 곧 강화학습의 목표라고도 할 수 있다고 했었습니다.

$$
J(\theta) = \mathbb{E}_{s_1 \sim p(s_1)} [ V^{\pi} (s_1) ] 
$$

그렇다면 어떻게 Policy를 평가할 수 있을까요?


그 중 하나는 바로 `Monte Carlo Policy Evaluation` 입니다.
이는 정책 경사 알고리즘이 작용하는 방식과도 같은 것으로, 여러번 샘플링해서 이를 바탕으로 평가를 하는 방법이었습니다. (기대값은 모든 상황을 다 고려해야하지만 이는 사실상 불가능하기때문에)

$$
V^{\pi} (s_t) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t'=t}^T r(s_{t'},a_{t'})
$$
 
![slide8](/assets/images/CS285/lec-6/slide8.png)
*Slide. 8.*

몬테 카를로 방법을 사용해서 몇개의 trajectory를 샘플링 했을 때 연속적인 state space에서 똑같은 곳을 다시 방문할 수는 없지만 아주 근사한 상태를 방문할 수는 있을겁니다. 그리고 뉴럴 네트워크라는 근사함수는 이러한 비슷한것을 한데 묶어서 생각할 수 있는, 그러니까 `Generalization`을 잘 하는 함수죠 (오버피팅 하겠지만, 좋은 네트워크는 잘 해야 합니다).


그러므로 우리가 가치 함수를 학습시킬 데이터는 현재 가지고 있는 정책으로 `simulation`을 돌려보고 (`roll out`) 얻은 경로를 정답으로 하는겁니다. (이를 데이터 삼아 사용함)

$$
V^{\pi} (s_t) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t'=t}^T r(s_{t'},a_{t'})
$$

$$
\text{training data : } (s_{i,t}, \underbrace{ \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) }_{ y_{i,t} } )
$$

그리고 이를 기반으로 `지도학습 (Supervised Learning)`을 하면,
Value Function은 출력이 (지금은 벡터가 아니라 `scalar`) 보상값이고 이 값은 연속적이기 때문에 가치 함수를 fitting하는 것은 출력 분포를 가우시안 분포로 모델링하여 얻은 아래의 `MSE loss`를 줄이는, 이른 바 `회귀 (Regression)` 문제를 푸는 것이 됩니다.

$$
L(\phi) = \frac{1}{2} \sum_i \parallel \hat{V}_{\phi}^{\pi}(s_i) - y_i \parallel^2
$$

여기서 이 `가치 함수 네트워크가 심하게 오버피팅을 하면` $$y_{i,t}$$를 직접적으로 가져다가 사용하는 Vanilla Policy Gradient와 비교해서 성능이 아주 안좋을 수 있지만, 일반화에 성공한다면 우리는 더 낮은 variance를 얻을 수 있습니다.



왜냐하면 직접적으로 $$y_{i_t}$$를 사용해 이를 평균내서 weighted-sum 하는 기존의 정책 경사 알고리즘은 비슷한 state에서의 비슷하지 않은 label들을 평균내서 사용하기 때문입니다. (?)








![slide9](/assets/images/CS285/lec-6/slide9.png)
*Slide. 9.*

하지만 아직도 부족합니다. 우리가 어떤 상태에서 직접 시뮬레이션을 끝까지 돌려서 얻은 보상값을 타겟으로 하지 말고 `True expecte value of rewards`를 사용할 순 없을까요?

$$
\begin{aligned}

& 
y_{i,t} = \sum_{t=t'}^T \mathbb{E}_{\pi_{\theta}}[r(s_{t'},a_{t'}) \vert s_{i,t}]
& \scriptstyle{ideal target} \\

&
y_{i,t} = \sum_{t=t'}^T r(s_{i,t'},a_{i,t'})
& \scriptstyle{Monte Carlo (MC) Traget (one sample)} \\

\end{aligned}
$$


우리가 이전에 정의했던 것 처럼 큐 함수는 아래와 같이 current reward + 다음 스텝에서의 $$V^{\pi}$$로 표현할 수 있었습니다.

$$
Q^{\pi}(s_t,a_t) = r(s_t,a_t) + \underbrace{\sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{t'},a_{t'}) \vert s_t,a_t]}_{V^{\pi}(s_{t+1})}
$$

이러한 사실을 이용해서 `idea target`을 아래와 같이 바꿀 수 있습니다.  

$$
\begin{aligned}
& 
y_{i,t} = \sum_{t=t'}^T \mathbb{E}_{\pi_{\theta}}[r(s_{t'},a_{t'}) \vert s_{i,t}]
& \scriptstyle{ideal target} \\

&
\approx r(s_{i,t},a_{i,t}) + \sum_{t'=t+1}^T \mathbb{E}_{\pi_{\theta}} [r(s_{i,t'},a_{i,t'} \vert s_{i,t}]
& \\

&
\approx r(s_{i,t},a_{i,t}) + V^{\pi} (s_{i,t+1})
& \\

\end{aligned}
$$

이러한 타겟을 사용하는게 매 번 한번씩만 샘플링하는 몬테카를로 방법보다는 더 낮은 variance의 솔루션을 얻을 수 있게 해줄 수 있다고 합니다.

여기서 우리는 $$V^{\pi}$$를 모르기 때문에 이를 근사한 $$\hat{V}_{\phi}^{\pi}(s_{i,t+1})$$을 사용하게 됩니다.

$$
\begin{aligned}

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + V^{\pi} (s_{i,t+1})
& \\

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + \hat{V}_{\phi}^{\pi}(s_{i,t+1})
& \\

\end{aligned}
$$

이렇게 근사한 네트워크의 값을 쓰는게 당연히 베스트이진 않지만 없는거보다야 낫다고 합니다.
이를 `bootstrap estimator`라고 부르기도 한다고 합니다.


즉 이렇게 해서 가치 함수의 근사 네트워크를 학습하는 트레이닝 데이터는 아래와 같이 바뀝니다.

$$
\begin{aligned}

&
\text{training data : } (s_{i,t}, \sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) )
& \\

&
\text{training data : } (s_{i,t}, r(s_{i,t},a_{i,t}) + \hat{V}_{\phi}^{\pi} (s_{i,t+1}) )
& \\

\end{aligned}
$$

우리는 $$\hat{V}^{\pi}_{\phi}$$ 네트워크가 부정확히지만 학습을 거듭할수록 실제 값 (Oracle Value) 에 가까워 지길 바랄 뿐입니다.
이제 마찬가지로 위의 트레이닝 데이터를 타겟으로 지도 학습을 하면 됩니다.


이러한 부트스트랩 기법(bootstrap estimator)은 한번 샘플링한것을 타겟으로 하는 것 보다 lower variance를 가지기 때문에 좋지만 이 근사 함수가 뱉는 값이 부정확할 것이기 때문에 bias는 클 수 있다고 합니다. (trade off)



![slide10](/assets/images/CS285/lec-6/slide10.png)
*Slide. 10.*

*Slide. 10.*는 몇 가지 예제와 함께 '대체 Policy Evaluation이 의미하는게 정확히 뭐야?' 에 대한 얘기를 합니다.


(TD-Gammon 과 Alphago 예시)












## <mark style='background-color: #fff5b1'> From Evaluation to Actor Critic </mark>

![slide12](/assets/images/CS285/lec-6/slide12.png)
*Slide. 12.*

Actor-Critic 알고리즘의 기본적인 `batch`는 위와 같이 구성되어 있는데요, 
근본적으로 앞서 배운 정책 경사 알고리즘인 `REINFORCE`와 크게 다르지 않습니다.

- rullout 해서 trajectory 샘플을 만든다
- 샘플들을 통해 얻은 reward로 $$\hat{V}^{\pi}_{\phi}(s)$$를 fit한다. (단순히 보상들을 다 합치는거에서 발전함)
- 샘플들을 통해서 Approximate Adavantage, $$\hat{A}^{\pi}(s_i,a_i)$$를 평가한다.
- 얻은 $$\hat{A}^{\pi}(s_i,a_i)$$를 통해서 Objective의 gradient를 구한다.
- 업데이트 한다.

여기서 우리가 앞서 논했던 `Policy Evaluation`는 2번째 스텝에 해당합니다.


```
이를 Actor-Critic 알고리즘이라고 부르는 이유는
어떤 상태에서 어떤 행동을 할 지를 결정하는 정책이 행동하는 모듈인 Actor 이라 하고,
과연 그 상태에서 현재 정책을 따르면 어떤 결과가 나오는지?
즉, 정책이 얼마나 좋은지를 평가하는 모듈이 Critic 이라 하기 때문입니다. 
```


![slide13](/assets/images/CS285/lec-6/slide13.png)
*Slide. 13.*

그 다음으로 알아볼것은 `Discount Factor`인데요, 이것은 우리가 bootstrap estimator를 사용해서 가치 함수 네트워크를 근사할 때,
태스크가 `infinite horizon case`일 경우 어떻게 할 것이냐? 에 대한 솔루션 입니다.


만약 $$T \rightarrow \infty$$라면 가치함수가 굉장히 올라갈 겁니다.
물론 episodic case는 별로 문제가 되지 않을 수 있지만요.


가장 단순한 해결책은 바로 $$\gamma \in [0,1]$$인 factor를 도입하는 겁니다.


$$
\begin{aligned}

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + \hat{V}_{\phi}^{\pi}(s_{i,t+1})
& \\

&
y_{i,t} \approx r(s_{i,t},a_{i,t}) + \gamma \hat{V}_{\phi}^{\pi}(s_{i,t+1})
& \\

\end{aligned}
$$

사실 강화학습에서 discount factor에 대한 얘기는 많은 서적에서 제일 앞부분에 등장해서 강화학습 책을 한번이라도 접해보신 분들에게는 익숙하실텐데요, 이는 강화학습이 미래의 보상들 까지 다 고려해서 행동을 한다고 하지만, 그래도 현재 받는 보상에 가장 높은 weight을 부여해서 `미래를 고려하긴 하지만 그래도 현재에 집중해서 행동을 선택`하게 한다는 의미를 가지고 있습니다.


그리고 이러한 discount factor를 도입하는 것은 MDP의 결과를 바꾸기도 합니다. episode 횟수가 정해져있고 실제로 MDP를 따라서 얻는 보상의 값이 discount factor를 넣었을 때와 아닐 때가 다릅니다 (수학적으로). (다른 자료를 찾아보시길 바랍니다)


그리고 당연히 이 discount factor를 몬테 카를로 방법을 사용하는 일반적인 정책 경사 알고리즘에도 사용할 수 있는데요,
이는 *Slide. 14.*의 수식을 따라가시다보면 쉽게 확인하실 수 있습니다.

![slide14](/assets/images/CS285/lec-6/slide14.png)
*Slide. 14.*

*Slide. 14.*에서 보시면 option이 2가지가 있는데요. $$t=1 \sim T$$까지와 $$t=t' \sim T$$까지인 버전이 두개있습니다.

그리고 option2는 다시 `causality`를 적용한 뒤 $$\gamma$$를 $$\gamma^{t'-t},\gamma^{t-1}$$로 나누어 log probability와 reward에 각각 분배해줬습니다.

option2는 log probability에도 감가율이 적용되어 `에피소드 초기에 좋은 선택을 하는것이 미래에 좋은 선택을 하는것보다 더 영향력있고 중요하다` 는 의미를 내포합니다.


![slide15](/assets/images/CS285/lec-6/slide15.png)
*Slide. 15.*

여기서 어떤 버전이 더 나은지는 위의 슬라이드에 답으로 나와있습니다.
*Slide. 15.*에는 또한 사람 모양의 로봇이 최대한 멀리 걸어가는게 목표인 강화학습 task가 있는데요,
이러한 `cyclic task`에서 적합하지 않기 때문입니다.


이렇게 discount factor를 추가하는 것도 마찬가지로 알고리즘의 variance를 줄여줍니다.



![slide16](/assets/images/CS285/lec-6/slide16.png)
*Slide. 16.*

이제 마지막으로 `discount factor`까지 적용한 Actor-Critic Algorithm의 batch에 대해서 얘기하고 서브섹션을 마칠겁니다.
Discount factor가 적용된 부분은 3번 스텝이 되겠습니다.


그리고 *Slide. 16.*에는 `Online Actor-Critic Algorithm`이 있는데요, 이는 `Infinite horizon case`에 대해서 적용하기 위한 알고리즘이라고 생각하시면 됩니다.

이는 trajectory를 다 뽑아두고 학습하는 `Eposodic case`와 다르기 때문에 1,2번이 다른 걸 알 수 있습니다.


하지만 이렇게 이론적으로 잘 정의된 것 같은 알고리즘도 실제에서는 적용하기 어려운점이 많습니다.



## <mark style='background-color: #fff5b1'> Actor-Critic Design Decisions </mark>

이제 Actor-Critic 알고리즘을 사용하기 위한 조금 practical한 테크닉들을 알아보도록 하겠습니다.

![slide18](/assets/images/CS285/lec-6/slide18.png)
*Slide. 18.*

우선 Vanilla Policy Gradient 방법론은 뉴럴 네트워크 하나가 policy를 모델링하기 때문에 굉장히 명확했는데요,
Actor-Critic은 지금 `policy와 Value function 두 가지`를 다 모델링 해야 하는 상황입니다.

가장 심플한 방법은 *Slide. 18.*의 좌하단 그림처럼 네트워크를 두개 두는거죠. (각각 파라메터 $$\phi,\pi$$로 구성됨)
하지만 이렇게 할 경우 두 네트워크가 서로 feature (representation)을 공유하는 일이 없다는 단점이 있습니다.


그렇기 때문에 또 다른 방법으로는 `shared network`를 두고 최종 출력 직전의 디코더만 두 개로 구성하는 전략이 있습니다.
이렇게 되면 `internal representation`을 공유하기 때문에 학습하기 더욱 용이할 것입니다. 

뭐 자율주행 같은 컴퓨터비전 task라면 쉐어링 되는 backbone network를 `ResNet` 같은 인코더를 사용하면 되겠네요.


하지만 이렇게 이론적으로 작용할것 처럼 보이는 두 번째 방법론도, 학습이 unstable하게 되는 문제가 있다고 하는데요, 왜냐하면 이는 소위 말하는 `Multi-task Learning`과 유사한 방법론인데, 이런 Multi-task Learning 에서는 두 task의 난이도가 다를 경우 적절하게 weight를 주지 않고 학습하게 되면 문제가 생기기 마련이기 때문입니다. (heuristic하게 하이퍼 파라메터를 정해야겠죠)


그리고 그 다음으로 Sergey교수님이 얘기하는 practical implementation은 *Slide. 19.*에 나와있는데요,

![slide19](/assets/images/CS285/lec-6/slide19.png)
*Slide. 19.*

바로 `batch size`에 관한 것입니다.
앞서 소개했던 온라인 batch방법은 1번 샘플링해서 사용하는 1 batch 방법론이었는데요, 이는 알고리즘 내에 Advantage Value를 넣는 등 Variance를 효과적으로 줄이긴 했으나 여전히 high variance를 가지고 있습니다.

그래서 매 번 policy와 value function을 업데이트하지 말고 




![asynchronous1](/assets/images/CS285/lec-6/asynchronous1.png)

![asynchronous2](/assets/images/CS285/lec-6/asynchronous2.png)













## <mark style='background-color: #fff5b1'> Critics as Baselines </mark>

![slide21](/assets/images/CS285/lec-6/slide21.png)
*Slide. 21.*

![slide22](/assets/images/CS285/lec-6/slide22.png)
*Slide. 22.*

![slide23](/assets/images/CS285/lec-6/slide23.png)
*Slide. 23.*

![slide24](/assets/images/CS285/lec-6/slide24.png)
*Slide. 24.*













## <mark style='background-color: #fff5b1'> Review, Examples, and Additional Readings </mark>

마지막 서브섹션은 역시나 리뷰와 데모 비디오를 동반한 몇가지 예제들인데요, (비디오는 강의 참조)

![slide26](/assets/images/CS285/lec-6/slide26.png)
*Slide. 26.*

Actor-Critic Algorithm은 기본적으로 policy를 `Actor`, 그리고 policy가 얼마나좋은가?를 나타내는 value function을 `Critic`으로 하죠. 그리고 이렇게 `Critic`을 도입함으로써 Vanilla PG 대비 variance를 많이 줄였습니다.

그리고 Critic으로 여러가지를 사용했던 것과, `discount factor`를 사용해서 variance를 더욱 줄였었고,

네트워크를 Critic, Actor를 위해 아예 독립적으로 두개 구성하거나, `shared network`를 둬서 `internal representation`
을 공유하는 방식으로 네트워크를 구성할 수 있었고, 마지막으로 trajectory를 미리 샘플링해두고 이를 기반으로 학습한는 `batch-mode`와 `online mode`에 대해서 알아봤으며 온라인 모드를 특히 `parallel`하게 구성하기도 하였습니다.

그리고 critic을 다르게 사용하는 방법과 `n-step return`이나 `GAE` 까지 알아봤네요.



이하 *Slide. 27.*~*Slide. 28.*는 비디오 데모가 포함되어 있기 때문에 강의를 참조하시길 추천드리며, 마지막 슬라이드는 추천할만한 Actor-Critic Based 논문들 입니다.

![slide27](/assets/images/CS285/lec-6/slide27.png)
*Slide. 27.*

![slide28](/assets/images/CS285/lec-6/slide28.png)
*Slide. 28.*

![slide29](/assets/images/CS285/lec-6/slide29.png)
*Slide. 29.*



수고하셨습니다.

***

이제 아래의 `Deep RL Taxonomy`에서 Policy Gradient based Algorithm은 거의 커버를 한 것 같네요

![rl_taxonomy_intellabs_for_lec6](/assets/images/CS285/lec-6/rl_taxonomy_intellabs_for_lec6.png)
(이미지 출처 : [Reinforcement Learning Coach from Intel Lab](https://intellabs.github.io/coach/index.html))

![rl_taxonomy_openai_for_lec6](/assets/images/CS285/lec-6/rl_taxonomy_openai_for_lec6.png)
(이미지 출처 : [OpenAI Spinning Up form OpenAI](https://spinningup.openai.com/en/latest/index.html))


Lecture 7부터는 `Value Function Methods`와 2013년 Atari게임을 정복한 딥마인드의 `Deep Q Learning (DQN)`에 대해서 얘기하게 될 것 같습니다.

***


## <mark style='background-color: #fff5b1'> Reference </mark>

- [CS 285 at UC Berkeley : Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)

- [강화학습 알아보기(4) - Actor-Critic, A2C, A3C from 김환희님](https://greentec.github.io/reinforcement-learning-fourth/)













